{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langgraph in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (0.4.7)\n",
      "Requirement already satisfied: langchain in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (0.3.25)\n",
      "Requirement already satisfied: openai in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (1.78.0)\n",
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.63-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting alpha-vantage\n",
      "  Downloading alpha_vantage-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting newsapi-python\n",
      "  Downloading newsapi_python-0.2.7-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pandas in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (1.3.2)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: requests in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (4.13.4)\n",
      "Requirement already satisfied: langchain-core>=0.1 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langgraph) (0.3.59)\n",
      "Requirement already satisfied: langgraph-checkpoint>=2.0.26 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langgraph) (2.0.26)\n",
      "Requirement already satisfied: langgraph-prebuilt>=0.2.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langgraph) (0.2.2)\n",
      "Requirement already satisfied: langgraph-sdk>=0.1.42 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langgraph) (0.1.66)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langgraph) (2.11.4)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langgraph) (3.5.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langchain) (0.3.42)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from openai) (4.13.2)\n",
      "Collecting multitasking>=0.0.7 (from yfinance)\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from yfinance) (4.3.7)\n",
      "Requirement already satisfied: pytz>=2022.5 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from yfinance) (2025.2)\n",
      "Collecting frozendict>=2.3.4 (from yfinance)\n",
      "  Downloading frozendict-2.4.6-py311-none-any.whl.metadata (23 kB)\n",
      "Collecting peewee>=3.16.2 (from yfinance)\n",
      "  Downloading peewee-3.18.1.tar.gz (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting curl_cffi>=0.7 (from yfinance)\n",
      "  Downloading curl_cffi-0.11.3-cp39-abi3-macosx_11_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from yfinance) (6.30.2)\n",
      "Requirement already satisfied: websockets>=13.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: aiohttp in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from alpha-vantage) (3.11.18)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting nltk>=3.9 (from textblob)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: cffi>=1.12.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langchain-core>=0.1->langgraph) (24.2)\n",
      "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langgraph-checkpoint>=2.0.26->langgraph) (1.9.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: click in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from pydantic>=2.7.4->langgraph) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from aiohttp->alpha-vantage) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from aiohttp->alpha-vantage) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from aiohttp->alpha-vantage) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from aiohttp->alpha-vantage) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from aiohttp->alpha-vantage) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from aiohttp->alpha-vantage) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from aiohttp->alpha-vantage) (1.20.0)\n",
      "Requirement already satisfied: pycparser in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/rajeshthakur/miniconda3/envs/ai_experiment/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Downloading yfinance-0.2.63-py2.py3-none-any.whl (118 kB)\n",
      "Downloading alpha_vantage-3.0.0-py3-none-any.whl (35 kB)\n",
      "Downloading newsapi_python-0.2.7-py2.py3-none-any.whl (7.9 kB)\n",
      "Using cached textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "Downloading curl_cffi-0.11.3-cp39-abi3-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
      "Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for peewee: filename=peewee-3.18.1-cp311-cp311-macosx_11_0_arm64.whl size=271362 sha256=63a517a94198323cc3f734573544d03fd000aba84603db1043a3aece3f8243d6\n",
      "  Stored in directory: /Users/rajeshthakur/Library/Caches/pip/wheels/25/cb/79/a133a0d1d75f318a96614ed7fb97bdf2f35a7b6c4d4e426e3f\n",
      "Successfully built peewee\n",
      "Installing collected packages: peewee, multitasking, nltk, frozendict, textblob, newsapi-python, curl_cffi, yfinance, alpha-vantage\n",
      "Successfully installed alpha-vantage-3.0.0 curl_cffi-0.11.3 frozendict-2.4.6 multitasking-0.0.11 newsapi-python-0.2.7 nltk-3.9.1 peewee-3.18.1 textblob-0.19.0 yfinance-0.2.63\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install langgraph langchain openai yfinance alpha-vantage newsapi-python pandas numpy scikit-learn textblob requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Define State Structure\n",
    "Create a shared state class that all agents can read from and write to:\n",
    "Stock symbol and basic info\n",
    "Market data (prices, volumes, indicators)\n",
    "Sentiment scores and news data\n",
    "Technical analysis results\n",
    "Risk metrics\n",
    "Final recommendation\n",
    "Step 3: Create Individual Agent Functions\n",
    "3.1 Data Analysis Agent\n",
    "Function to fetch real-time stock data (yfinance, Alpha Vantage)\n",
    "Function to get historical price data\n",
    "Function to fetch financial statements\n",
    "Function to get market indices data\n",
    "Function to collect economic indicators\n",
    "3.2 Sentiment Analysis Agent\n",
    "Function to fetch recent news articles\n",
    "Function to analyze news sentiment using NLP\n",
    "Function to get social media sentiment (Twitter API/Reddit)\n",
    "Function to collect analyst recommendations\n",
    "Function to calculate overall sentiment score\n",
    "3.3 Technical Analysis Agent\n",
    "Function to calculate moving averages (SMA, EMA)\n",
    "Function to compute momentum indicators (RSI, MACD, Stochastic)\n",
    "Function to identify support/resistance levels\n",
    "Function to detect chart patterns\n",
    "Function to analyze volume trends\n",
    "3.4 Risk Assessment Agent\n",
    "Function to calculate Value at Risk (VaR)\n",
    "Function to compute Beta and correlation\n",
    "Function to assess volatility metrics\n",
    "Function to analyze portfolio impact\n",
    "Function to determine risk score\n",
    "3.5 Decision Making Agent\n",
    "Function to synthesize all analyses\n",
    "Function to apply decision criteria\n",
    "Function to generate recommendations\n",
    "Function to set price targets and stop losses\n",
    "Function to calculate confidence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell 1-2: Environment Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install required packages (run this first)\n",
    "\"\"\"\n",
    "pip install langgraph langchain-openai yfinance alpha-vantage newsapi-python \n",
    "pip install pandas numpy scikit-learn textblob requests beautifulsoup4 \n",
    "pip install ta plotly matplotlib seaborn fredapi tweepy praw\n",
    "\"\"\"\n",
    "\n",
    "# Cell 2: Import all required libraries\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, TypedDict, Annotated, Any\n",
    "from dataclasses import dataclass, field\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import ta\n",
    "\n",
    "# Financial data sources\n",
    "import yfinance as yf\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "from alpha_vantage.fundamentaldata import FundamentalData\n",
    "import fredapi\n",
    "\n",
    "# News and sentiment analysis\n",
    "import requests\n",
    "from newsapi import NewsApiClient\n",
    "import tweepy\n",
    "import praw\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# LangGraph and LangChain\n",
    "from langgraph.graph import Graph, StateGraph, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3-4: Define State Classes and Data Structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Define the main state structure\n",
    "class StockAnalysisState(TypedDict):\n",
    "    # Input parameters\n",
    "    symbol: str\n",
    "    company_name: str\n",
    "    analysis_date: datetime\n",
    "    \n",
    "    # Market data\n",
    "    current_price: float\n",
    "    price_change: float\n",
    "    price_change_percent: float\n",
    "    volume: int\n",
    "    market_cap: float\n",
    "    \n",
    "    # Historical data\n",
    "    historical_data: pd.DataFrame\n",
    "    financial_data: Dict[str, Any]\n",
    "    \n",
    "    # Sentiment analysis results\n",
    "    news_sentiment: float\n",
    "    social_sentiment: float\n",
    "    analyst_consensus: float\n",
    "    overall_sentiment: float\n",
    "    sentiment_details: Dict[str, Any]\n",
    "    \n",
    "    # Technical analysis results\n",
    "    technical_indicators: Dict[str, float]\n",
    "    support_resistance: Dict[str, float]\n",
    "    trend_direction: str\n",
    "    chart_patterns: List[str]\n",
    "    \n",
    "    # Risk assessment results\n",
    "    risk_score: float\n",
    "    volatility: float\n",
    "    beta: float\n",
    "    var_score: float\n",
    "    risk_metrics: Dict[str, float]\n",
    "    \n",
    "    # Final recommendation\n",
    "    recommendation: str\n",
    "    confidence_score: float\n",
    "    price_target: float\n",
    "    stop_loss: float\n",
    "    rationale: str\n",
    "    \n",
    "    # Execution tracking\n",
    "    errors: List[str]\n",
    "    warnings: List[str]\n",
    "    execution_time: Dict[str, float]\n",
    "\n",
    "# Cell 4: Define configuration and helper classes\n",
    "@dataclass\n",
    "class APIConfig:\n",
    "    \"\"\"Configuration for various APIs\"\"\"\n",
    "    alpha_vantage_key: str = \"\"\n",
    "    news_api_key: str = \"\"\n",
    "    fred_api_key: str = \"\"\n",
    "    twitter_bearer_token: str = \"\"\n",
    "    reddit_client_id: str = \"\"\n",
    "    reddit_client_secret: str = \"\"\n",
    "    openai_api_key: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class AnalysisConfig:\n",
    "    \"\"\"Configuration for analysis parameters\"\"\"\n",
    "    lookback_days: int = 252  # 1 year of trading days\n",
    "    short_ma_period: int = 20\n",
    "    long_ma_period: int = 50\n",
    "    rsi_period: int = 14\n",
    "    bollinger_period: int = 20\n",
    "    confidence_threshold: float = 0.6\n",
    "    risk_free_rate: float = 0.02\n",
    "    \n",
    "@dataclass\n",
    "class TechnicalIndicators:\n",
    "    \"\"\"Structure for technical indicators\"\"\"\n",
    "    sma_20: float = 0.0\n",
    "    sma_50: float = 0.0\n",
    "    ema_12: float = 0.0\n",
    "    ema_26: float = 0.0\n",
    "    rsi: float = 0.0\n",
    "    macd: float = 0.0\n",
    "    macd_signal: float = 0.0\n",
    "    bollinger_upper: float = 0.0\n",
    "    bollinger_lower: float = 0.0\n",
    "    bollinger_middle: float = 0.0\n",
    "    atr: float = 0.0\n",
    "    volume_sma: float = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5-9: Implement Data Analysis Agent Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Core data fetching functions\n",
    "class DataAnalysisAgent:\n",
    "    def __init__(self, config: APIConfig):\n",
    "        self.config = config\n",
    "        self.alpha_vantage = TimeSeries(key=config.alpha_vantage_key) if config.alpha_vantage_key else None\n",
    "        self.fundamental_data = FundamentalData(key=config.alpha_vantage_key) if config.alpha_vantage_key else None\n",
    "        self.fred = fredapi.Fred(api_key=config.fred_api_key) if config.fred_api_key else None\n",
    "    \n",
    "    def fetch_stock_data(self, symbol: str, period: str = \"1y\") -> Dict[str, Any]:\n",
    "        \"\"\"Fetch current stock data and basic info\"\"\"\n",
    "        try:\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            info = ticker.info\n",
    "            hist = ticker.history(period=period)\n",
    "            \n",
    "            current_price = info.get('currentPrice', hist['Close'].iloc[-1])\n",
    "            previous_close = info.get('previousClose', hist['Close'].iloc[-2])\n",
    "            \n",
    "            return {\n",
    "                'symbol': symbol,\n",
    "                'company_name': info.get('longName', symbol),\n",
    "                'current_price': current_price,\n",
    "                'previous_close': previous_close,\n",
    "                'price_change': current_price - previous_close,\n",
    "                'price_change_percent': ((current_price - previous_close) / previous_close) * 100,\n",
    "                'volume': info.get('volume', hist['Volume'].iloc[-1]),\n",
    "                'market_cap': info.get('marketCap', 0),\n",
    "                'historical_data': hist,\n",
    "                'info': info\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching stock data for {symbol}: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Cell 6: Historical data analysis\n",
    "    def get_historical_analysis(self, historical_data: pd.DataFrame, days: int = 252) -> Dict[str, float]:\n",
    "        \"\"\"Analyze historical performance metrics\"\"\"\n",
    "        try:\n",
    "            if len(historical_data) < days:\n",
    "                days = len(historical_data)\n",
    "            \n",
    "            recent_data = historical_data.tail(days)\n",
    "            returns = recent_data['Close'].pct_change().dropna()\n",
    "            \n",
    "            return {\n",
    "                'avg_return': returns.mean() * 252,  # Annualized\n",
    "                'volatility': returns.std() * np.sqrt(252),  # Annualized\n",
    "                'sharpe_ratio': (returns.mean() * 252) / (returns.std() * np.sqrt(252)) if returns.std() > 0 else 0,\n",
    "                'max_drawdown': self._calculate_max_drawdown(recent_data['Close']),\n",
    "                'avg_volume': recent_data['Volume'].mean(),\n",
    "                'price_range_52w': {\n",
    "                    'high': recent_data['High'].max(),\n",
    "                    'low': recent_data['Low'].min()\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in historical analysis: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_max_drawdown(self, prices: pd.Series) -> float:\n",
    "        \"\"\"Calculate maximum drawdown\"\"\"\n",
    "        peak = prices.expanding().max()\n",
    "        drawdown = (prices - peak) / peak\n",
    "        return drawdown.min()\n",
    "\n",
    "# Cell 7: Financial statements analysis\n",
    "    def get_financial_data(self, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Fetch and analyze financial statements\"\"\"\n",
    "        try:\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            \n",
    "            # Get financial statements\n",
    "            income_stmt = ticker.financials\n",
    "            balance_sheet = ticker.balance_sheet\n",
    "            cash_flow = ticker.cashflow\n",
    "            \n",
    "            # Key financial ratios\n",
    "            info = ticker.info\n",
    "            financial_ratios = {\n",
    "                'pe_ratio': info.get('trailingPE', 0),\n",
    "                'forward_pe': info.get('forwardPE', 0),\n",
    "                'peg_ratio': info.get('pegRatio', 0),\n",
    "                'price_to_book': info.get('priceToBook', 0),\n",
    "                'debt_to_equity': info.get('debtToEquity', 0),\n",
    "                'roe': info.get('returnOnEquity', 0),\n",
    "                'roa': info.get('returnOnAssets', 0),\n",
    "                'profit_margin': info.get('profitMargins', 0),\n",
    "                'operating_margin': info.get('operatingMargins', 0),\n",
    "                'current_ratio': info.get('currentRatio', 0),\n",
    "                'quick_ratio': info.get('quickRatio', 0)\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'income_statement': income_stmt.to_dict() if not income_stmt.empty else {},\n",
    "                'balance_sheet': balance_sheet.to_dict() if not balance_sheet.empty else {},\n",
    "                'cash_flow': cash_flow.to_dict() if not cash_flow.empty else {},\n",
    "                'financial_ratios': financial_ratios,\n",
    "                'analyst_info': {\n",
    "                    'target_high_price': info.get('targetHighPrice', 0),\n",
    "                    'target_low_price': info.get('targetLowPrice', 0),\n",
    "                    'target_mean_price': info.get('targetMeanPrice', 0),\n",
    "                    'recommendation_mean': info.get('recommendationMean', 0)\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching financial data: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Cell 8: Market indices and sector analysis\n",
    "    def get_market_context(self, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get market and sector context\"\"\"\n",
    "        try:\n",
    "            # Major indices\n",
    "            indices = {\n",
    "                '^GSPC': 'S&P 500',\n",
    "                '^DJI': 'Dow Jones',\n",
    "                '^IXIC': 'NASDAQ',\n",
    "                '^VIX': 'VIX'\n",
    "            }\n",
    "            \n",
    "            market_data = {}\n",
    "            for idx_symbol, name in indices.items():\n",
    "                try:\n",
    "                    idx_ticker = yf.Ticker(idx_symbol)\n",
    "                    idx_hist = idx_ticker.history(period=\"5d\")\n",
    "                    if not idx_hist.empty:\n",
    "                        current = idx_hist['Close'].iloc[-1]\n",
    "                        previous = idx_hist['Close'].iloc[-2]\n",
    "                        change_pct = ((current - previous) / previous) * 100\n",
    "                        market_data[name] = {\n",
    "                            'current': current,\n",
    "                            'change_percent': change_pct\n",
    "                        }\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Get sector information\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            info = ticker.info\n",
    "            sector_info = {\n",
    "                'sector': info.get('sector', 'Unknown'),\n",
    "                'industry': info.get('industry', 'Unknown')\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'market_indices': market_data,\n",
    "                'sector_info': sector_info\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting market context: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Cell 9: Economic indicators\n",
    "    def get_economic_indicators(self) -> Dict[str, float]:\n",
    "        \"\"\"Fetch key economic indicators\"\"\"\n",
    "        try:\n",
    "            if not self.fred:\n",
    "                return {}\n",
    "            \n",
    "            indicators = {\n",
    "                'GDP': 'GDP',\n",
    "                'UNRATE': 'Unemployment Rate',\n",
    "                'CPIAUCSL': 'CPI',\n",
    "                'FEDFUNDS': 'Federal Funds Rate',\n",
    "                'DGS10': '10-Year Treasury Rate'\n",
    "            }\n",
    "            \n",
    "            economic_data = {}\n",
    "            for code, name in indicators.items():\n",
    "                try:\n",
    "                    data = self.fred.get_series(code, limit=1)\n",
    "                    if not data.empty:\n",
    "                        economic_data[name] = data.iloc[-1]\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return economic_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching economic indicators: {e}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10-14: Implement Sentiment Analysis Agent Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: News sentiment analysis\n",
    "class SentimentAnalysisAgent:\n",
    "    def __init__(self, config: APIConfig):\n",
    "        self.config = config\n",
    "        self.news_api = NewsApiClient(api_key=config.news_api_key) if config.news_api_key else None\n",
    "        self.setup_social_media_clients()\n",
    "    \n",
    "    def setup_social_media_clients(self):\n",
    "        \"\"\"Setup social media API clients\"\"\"\n",
    "        # Twitter client\n",
    "        if self.config.twitter_bearer_token:\n",
    "            self.twitter_client = tweepy.Client(bearer_token=self.config.twitter_bearer_token)\n",
    "        else:\n",
    "            self.twitter_client = None\n",
    "        \n",
    "        # Reddit client\n",
    "        if self.config.reddit_client_id and self.config.reddit_client_secret:\n",
    "            self.reddit_client = praw.Reddit(\n",
    "                client_id=self.config.reddit_client_id,\n",
    "                client_secret=self.config.reddit_client_secret,\n",
    "                user_agent=\"StockAnalyzer\"\n",
    "            )\n",
    "        else:\n",
    "            self.reddit_client = None\n",
    "    \n",
    "    def analyze_news_sentiment(self, symbol: str, days_back: int = 7) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze news sentiment for a stock\"\"\"\n",
    "        try:\n",
    "            company_name = yf.Ticker(symbol).info.get('longName', symbol)\n",
    "            \n",
    "            # Fetch news articles\n",
    "            articles = self._fetch_news_articles(symbol, company_name, days_back)\n",
    "            \n",
    "            if not articles:\n",
    "                return {'sentiment_score': 0.0, 'article_count': 0, 'articles': []}\n",
    "            \n",
    "            # Analyze sentiment of each article\n",
    "            sentiments = []\n",
    "            analyzed_articles = []\n",
    "            \n",
    "            for article in articles:\n",
    "                title_sentiment = TextBlob(article['title']).sentiment.polarity\n",
    "                description_sentiment = TextBlob(article.get('description', '')).sentiment.polarity\n",
    "                \n",
    "                # Weight title more heavily\n",
    "                combined_sentiment = (title_sentiment * 0.7) + (description_sentiment * 0.3)\n",
    "                sentiments.append(combined_sentiment)\n",
    "                \n",
    "                analyzed_articles.append({\n",
    "                    'title': article['title'],\n",
    "                    'sentiment': combined_sentiment,\n",
    "                    'published_at': article['publishedAt'],\n",
    "                    'source': article['source']['name']\n",
    "                })\n",
    "            \n",
    "            # Calculate overall sentiment\n",
    "            overall_sentiment = np.mean(sentiments) if sentiments else 0.0\n",
    "            \n",
    "            return {\n",
    "                'sentiment_score': overall_sentiment,\n",
    "                'article_count': len(articles),\n",
    "                'articles': analyzed_articles[:10],  # Top 10 articles\n",
    "                'sentiment_distribution': {\n",
    "                    'positive': len([s for s in sentiments if s > 0.1]),\n",
    "                    'neutral': len([s for s in sentiments if -0.1 <= s <= 0.1]),\n",
    "                    'negative': len([s for s in sentiments if s < -0.1])\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing news sentiment: {e}\")\n",
    "            return {'sentiment_score': 0.0, 'article_count': 0, 'articles': []}\n",
    "\n",
    "# Cell 11: News fetching helper\n",
    "    def _fetch_news_articles(self, symbol: str, company_name: str, days_back: int) -> List[Dict]:\n",
    "        \"\"\"Fetch news articles from various sources\"\"\"\n",
    "        articles = []\n",
    "        \n",
    "        # NewsAPI\n",
    "        if self.news_api:\n",
    "            try:\n",
    "                from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "                \n",
    "                # Search by symbol and company name\n",
    "                queries = [symbol, company_name]\n",
    "                \n",
    "                for query in queries:\n",
    "                    news_response = self.news_api.get_everything(\n",
    "                        q=query,\n",
    "                        from_param=from_date,\n",
    "                        language='en',\n",
    "                        sort_by='relevancy',\n",
    "                        page_size=50\n",
    "                    )\n",
    "                    \n",
    "                    if news_response['status'] == 'ok':\n",
    "                        articles.extend(news_response['articles'])\n",
    "            except Exception as e:\n",
    "                logger.error(f\"NewsAPI error: {e}\")\n",
    "        \n",
    "        # Yahoo Finance news (backup)\n",
    "        try:\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            yahoo_news = ticker.news\n",
    "            for item in yahoo_news[:20]:  # Limit to 20 articles\n",
    "                articles.append({\n",
    "                    'title': item.get('title', ''),\n",
    "                    'description': item.get('summary', ''),\n",
    "                    'publishedAt': datetime.fromtimestamp(item.get('providerPublishTime', 0)).isoformat(),\n",
    "                    'source': {'name': item.get('publisher', 'Yahoo Finance')}\n",
    "                })\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Yahoo Finance news error: {e}\")\n",
    "        \n",
    "        # Remove duplicates and sort by date\n",
    "        unique_articles = {article['title']: article for article in articles}.values()\n",
    "        return sorted(unique_articles, key=lambda x: x['publishedAt'], reverse=True)\n",
    "\n",
    "# Cell 12: Social media sentiment\n",
    "    def analyze_social_sentiment(self, symbol: str, days_back: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze social media sentiment\"\"\"\n",
    "        twitter_sentiment = self._get_twitter_sentiment(symbol, days_back)\n",
    "        reddit_sentiment = self._get_reddit_sentiment(symbol, days_back)\n",
    "        \n",
    "        # Combine sentiments\n",
    "        sentiments = []\n",
    "        sources = []\n",
    "        \n",
    "        if twitter_sentiment['sentiment_score'] != 0:\n",
    "            sentiments.append(twitter_sentiment['sentiment_score'])\n",
    "            sources.append('Twitter')\n",
    "        \n",
    "        if reddit_sentiment['sentiment_score'] != 0:\n",
    "            sentiments.append(reddit_sentiment['sentiment_score'])\n",
    "            sources.append('Reddit')\n",
    "        \n",
    "        overall_sentiment = np.mean(sentiments) if sentiments else 0.0\n",
    "        \n",
    "        return {\n",
    "            'sentiment_score': overall_sentiment,\n",
    "            'twitter': twitter_sentiment,\n",
    "            'reddit': reddit_sentiment,\n",
    "            'sources_analyzed': sources,\n",
    "            'total_mentions': twitter_sentiment.get('tweet_count', 0) + reddit_sentiment.get('post_count', 0)\n",
    "        }\n",
    "    \n",
    "    def _get_twitter_sentiment(self, symbol: str, days_back: int) -> Dict[str, Any]:\n",
    "        \"\"\"Get Twitter sentiment for a stock\"\"\"\n",
    "        try:\n",
    "            if not self.twitter_client:\n",
    "                return {'sentiment_score': 0.0, 'tweet_count': 0}\n",
    "            \n",
    "            # Search for tweets\n",
    "            query = f\"${symbol} OR {symbol} -is:retweet lang:en\"\n",
    "            tweets = tweepy.Paginator(\n",
    "                self.twitter_client.search_recent_tweets,\n",
    "                query=query,\n",
    "                max_results=100,\n",
    "                tweet_fields=['created_at', 'public_metrics']\n",
    "            ).flatten(limit=200)\n",
    "            \n",
    "            sentiments = []\n",
    "            for tweet in tweets:\n",
    "                sentiment = TextBlob(tweet.text).sentiment.polarity\n",
    "                sentiments.append(sentiment)\n",
    "            \n",
    "            return {\n",
    "                'sentiment_score': np.mean(sentiments) if sentiments else 0.0,\n",
    "                'tweet_count': len(sentiments)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Twitter sentiment error: {e}\")\n",
    "            return {'sentiment_score': 0.0, 'tweet_count': 0}\n",
    "    \n",
    "    def _get_reddit_sentiment(self, symbol: str, days_back: int) -> Dict[str, Any]:\n",
    "        \"\"\"Get Reddit sentiment for a stock\"\"\"\n",
    "        try:\n",
    "            if not self.reddit_client:\n",
    "                return {'sentiment_score': 0.0, 'post_count': 0}\n",
    "            \n",
    "            # Search in relevant subreddits\n",
    "            subreddits = ['stocks', 'investing', 'SecurityAnalysis', 'StockMarket']\n",
    "            all_sentiments = []\n",
    "            \n",
    "            for subreddit_name in subreddits:\n",
    "                try:\n",
    "                    subreddit = self.reddit_client.subreddit(subreddit_name)\n",
    "                    for post in subreddit.search(symbol, limit=50):\n",
    "                        # Analyze post title and content\n",
    "                        text = f\"{post.title} {post.selftext}\"\n",
    "                        sentiment = TextBlob(text).sentiment.polarity\n",
    "                        all_sentiments.append(sentiment)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return {\n",
    "                'sentiment_score': np.mean(all_sentiments) if all_sentiments else 0.0,\n",
    "                'post_count': len(all_sentiments)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Reddit sentiment error: {e}\")\n",
    "            return {'sentiment_score': 0.0, 'post_count': 0}\n",
    "\n",
    "# Cell 13: Analyst sentiment\n",
    "    def get_analyst_sentiment(self, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get analyst recommendations and sentiment\"\"\"\n",
    "        try:\n",
    "            ticker = yf.Ticker(symbol)\n",
    "            info = ticker.info\n",
    "            recommendations = ticker.recommendations\n",
    "            \n",
    "            # Get recommendation summary\n",
    "            recommendation_mean = info.get('recommendationMean', 3.0)  # 1=Strong Buy, 5=Strong Sell\n",
    "            \n",
    "            # Convert to sentiment score (-1 to 1)\n",
    "            # 1-2: Strong positive, 2-3: Positive, 3-4: Negative, 4-5: Strong negative\n",
    "            sentiment_score = (5 - recommendation_mean) / 2 - 1  # Convert to -1 to 1 scale\n",
    "            \n",
    "            analyst_data = {\n",
    "                'sentiment_score': max(-1, min(1, sentiment_score)),\n",
    "                'recommendation_mean': recommendation_mean,\n",
    "                'target_price': info.get('targetMeanPrice', 0),\n",
    "                'number_of_analysts': info.get('numberOfAnalystOpinions', 0)\n",
    "            }\n",
    "            \n",
    "            # Get recent recommendations if available\n",
    "            if recommendations is not None and not recommendations.empty:\n",
    "                recent_recs = recommendations.head(10)\n",
    "                analyst_data['recent_recommendations'] = recent_recs.to_dict('records')\n",
    "            \n",
    "            return analyst_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting analyst sentiment: {e}\")\n",
    "            return {'sentiment_score': 0.0, 'recommendation_mean': 3.0}\n",
    "\n",
    "# Cell 14: Overall sentiment calculation\n",
    "    def calculate_overall_sentiment(self, news_sentiment: float, social_sentiment: float, \n",
    "                                  analyst_sentiment: float) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate weighted overall sentiment\"\"\"\n",
    "        try:\n",
    "            # Weights for different sentiment sources\n",
    "            weights = {\n",
    "                'news': 0.4,\n",
    "                'social': 0.3,\n",
    "                'analyst': 0.3\n",
    "            }\n",
    "            \n",
    "            # Calculate weighted average\n",
    "            overall_sentiment = (\n",
    "                news_sentiment * weights['news'] +\n",
    "                social_sentiment * weights['social'] +\n",
    "                analyst_sentiment * weights['analyst']\n",
    "            )\n",
    "            \n",
    "            # Classify sentiment\n",
    "            if overall_sentiment > 0.3:\n",
    "                sentiment_label = \"Bullish\"\n",
    "            elif overall_sentiment > 0.1:\n",
    "                sentiment_label = \"Slightly Bullish\"\n",
    "            elif overall_sentiment > -0.1:\n",
    "                sentiment_label = \"Neutral\"\n",
    "            elif overall_sentiment > -0.3:\n",
    "                sentiment_label = \"Slightly Bearish\"\n",
    "            else:\n",
    "                sentiment_label = \"Bearish\"\n",
    "            \n",
    "            return {\n",
    "                'overall_sentiment': overall_sentiment,\n",
    "                'sentiment_label': sentiment_label,\n",
    "                'component_scores': {\n",
    "                    'news': news_sentiment,\n",
    "                    'social': social_sentiment,\n",
    "                    'analyst': analyst_sentiment\n",
    "                },\n",
    "                'weights_used': weights\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating overall sentiment: {e}\")\n",
    "            return {\n",
    "                'overall_sentiment': 0.0,\n",
    "                'sentiment_label': \"Neutral\",\n",
    "                'component_scores': {},\n",
    "                'weights_used': {}\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 15-19: Implement Technical Analysis Agent Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Technical Analysis Agent class\n",
    "class TechnicalAnalysisAgent:\n",
    "    def __init__(self, config: AnalysisConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def calculate_all_indicators(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate all technical indicators\"\"\"\n",
    "        try:\n",
    "            if df.empty or len(df) < self.config.long_ma_period:\n",
    "                return {}\n",
    "            \n",
    "            indicators = {}\n",
    "            \n",
    "            # Moving Averages\n",
    "            indicators.update(self._calculate_moving_averages(df))\n",
    "            \n",
    "            # Momentum Indicators\n",
    "            indicators.update(self._calculate_momentum_indicators(df))\n",
    "            \n",
    "            # Volatility Indicators\n",
    "            indicators.update(self._calculate_volatility_indicators(df))\n",
    "            \n",
    "            # Volume Indicators\n",
    "            indicators.update(self._calculate_volume_indicators(df))\n",
    "            \n",
    "            # Trend Indicators\n",
    "            indicators.update(self._calculate_trend_indicators(df))\n",
    "            \n",
    "            return indicators\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating technical indicators: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_moving_averages(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate various moving averages\"\"\"\n",
    "        try:\n",
    "            close = df['Close']\n",
    "            \n",
    "            return {\n",
    "                'sma_20': ta.trend.sma_indicator(close, window=20).iloc[-1],\n",
    "                'sma_50': ta.trend.sma_indicator(close, window=50).iloc[-1],\n",
    "                'sma_200': ta.trend.sma_indicator(close, window=200).iloc[-1] if len(df) >= 200 else 0,\n",
    "                'ema_12': ta.trend.ema_indicator(close, window=12).iloc[-1],\n",
    "                'ema_26': ta.trend.ema_indicator(close, window=26).iloc[-1],\n",
    "                'ema_50': ta.trend.ema_indicator(close, window=50).iloc[-1],\n",
    "                'wma_20': ta.trend.wma_indicator(close, window=20).iloc[-1]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating moving averages: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Cell 16: Momentum indicators\n",
    "    def _calculate_momentum_indicators(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate momentum-based indicators\"\"\"\n",
    "        try:\n",
    "            high, low, close, volume = df['High'], df['Low'], df['Close'], df['Volume']\n",
    "            \n",
    "            # RSI\n",
    "            rsi = ta.momentum.rsi(close, window=self.config.rsi_period)\n",
    "            \n",
    "            # MACD\n",
    "            macd_line = ta.trend.macd(close)\n",
    "            macd_signal = ta.trend.macd_signal(close)\n",
    "            macd_histogram = ta.trend.macd_diff(close)\n",
    "            \n",
    "            # Stochastic\n",
    "            stoch_k = ta.momentum.stoch(high, low, close)\n",
    "            stoch_d = ta.momentum.stoch_signal(high, low, close)\n",
    "            \n",
    "            # Williams %R\n",
    "            williams_r = ta.momentum.williams_r(high, low, close)\n",
    "            \n",
    "            # Commodity Channel Index\n",
    "            cci = ta.trend.cci(high, low, close)\n",
    "            \n",
    "            return {\n",
    "                'rsi': rsi.iloc[-1] if not rsi.empty else 50,\n",
    "                'macd': macd_line.iloc[-1] if not macd_line.empty else 0,\n",
    "                'macd_signal': macd_signal.iloc[-1] if not macd_signal.empty else 0,\n",
    "                'macd_histogram': macd_histogram.iloc[-1] if not macd_histogram.empty else 0,\n",
    "                'stoch_k': stoch_k.iloc[-1] if not stoch_k.empty else 50,\n",
    "                'stoch_d': stoch_d.iloc[-1] if not stoch_d.empty else 50,\n",
    "                'williams_r': williams_r.iloc[-1] if not williams_r.empty else -50,\n",
    "                'cci': cci.iloc[-1] if not cci.empty else 0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating momentum indicators: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_volatility_indicators(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate volatility-based indicators\"\"\"\n",
    "        try:\n",
    "            high, low, close = df['High'], df['Low'], df['Close']\n",
    "            \n",
    "            # Bollinger Bands\n",
    "            bb_high = ta.volatility.bollinger_hband(close, window=self.config.bollinger_period)\n",
    "            bb_low = ta.volatility.bollinger_lband(close, window=self.config.bollinger_period)\n",
    "            bb_mid = ta.volatility.bollinger_mavg(close, window=self.config.bollinger_period)\n",
    "            bb_width = ta.volatility.bollinger_wband(close, window=self.config.bollinger_period)\n",
    "            \n",
    "            # Average True Range\n",
    "            atr = ta.volatility.average_true_range(high, low, close)\n",
    "            \n",
    "            # Keltner Channel\n",
    "            kc_high = ta.volatility.keltner_channel_hband(high, low, close)\n",
    "            kc_low = ta.volatility.keltner_channel_lband(high, low, close)\n",
    "            \n",
    "            return {\n",
    "                'bollinger_upper': bb_high.iloc[-1] if not bb_high.empty else close.iloc[-1],\n",
    "                'bollinger_lower': bb_low.iloc[-1] if not bb_low.empty else close.iloc[-1],\n",
    "                'bollinger_middle': bb_mid.iloc[-1] if not bb_mid.empty else close.iloc[-1],\n",
    "                'bollinger_width': bb_width.iloc[-1] if not bb_width.empty else 0,\n",
    "                'atr': atr.iloc[-1] if not atr.empty else 0,\n",
    "                'keltner_upper': kc_high.iloc[-1] if not kc_high.empty else close.iloc[-1],\n",
    "                'keltner_lower': kc_low.iloc[-1] if not kc_low.empty else close.iloc[-1]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating volatility indicators: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Cell 17: Volume and trend indicators\n",
    "    def _calculate_volume_indicators(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate volume-based indicators\"\"\"\n",
    "        try:\n",
    "            high, low, close, volume = df['High'], df['Low'], df['Close'], df['Volume']\n",
    "            \n",
    "            # On Balance Volume\n",
    "            obv = ta.volume.on_balance_volume(close, volume)\n",
    "            \n",
    "            # Volume Weighted Average Price\n",
    "            vwap = ta.volume.volume_weighted_average_price(high, low, close, volume)\n",
    "            \n",
    "            # Chaikin Money Flow\n",
    "            cmf = ta.volume.chaikin_money_flow(high, low, close, volume)\n",
    "            \n",
    "            # Volume SMA\n",
    "            volume_sma = ta.trend.sma_indicator(volume, window=20)\n",
    "            \n",
    "            return {\n",
    "                'obv': obv.iloc[-1] if not obv.empty else 0,\n",
    "                'vwap': vwap.iloc[-1] if not vwap.empty else close.iloc[-1],\n",
    "                'cmf': cmf.iloc[-1] if not cmf.empty else 0,\n",
    "                'volume_sma': volume_sma.iloc[-1] if not volume_sma.empty else volume.iloc[-1],\n",
    "                'volume_ratio': volume.iloc[-1] / volume_sma.iloc[-1] if not volume_sma.empty and volume_sma.iloc[-1] > 0 else 1\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating volume indicators: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_trend_indicators(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate trend-based indicators\"\"\"\n",
    "        try:\n",
    "            high, low, close = df['High'], df['Low'], df['Close']\n",
    "            \n",
    "            # ADX (Average Directional Index)\n",
    "            adx = ta.trend.adx(high, low, close)\n",
    "            adx_pos = ta.trend.adx_pos(high, low, close)\n",
    "            adx_neg = ta.trend.adx_neg(high, low, close)\n",
    "            \n",
    "            # Parabolic SAR\n",
    "            psar = ta.trend.psar_down(high, low, close)\n",
    "            \n",
    "            # Aroon\n",
    "            aroon_up = ta.trend.aroon_up(close)\n",
    "            aroon_down = ta.trend.aroon_down(close)\n",
    "            \n",
    "            return {\n",
    "                'adx': adx.iloc[-1] if not adx.empty else 25,\n",
    "                'adx_pos': adx_pos.iloc[-1] if not adx_pos.empty else 25,\n",
    "                'adx_neg': adx_neg.iloc[-1] if not adx_neg.empty else 25,\n",
    "                'psar': psar.iloc[-1] if not psar.empty else close.iloc[-1],\n",
    "                'aroon_up': aroon_up.iloc[-1] if not aroon_up.empty else 50,\n",
    "                'aroon_down': aroon_down.iloc[-1] if not aroon_down.empty else 50\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating trend indicators: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Cell 18: Support and resistance analysis\n",
    "    def identify_support_resistance(self, df: pd.DataFrame, window: int = 20) -> Dict[str, float]:\n",
    "        \"\"\"Identify support and resistance levels\"\"\"\n",
    "        try:\n",
    "            high, low, close = df['High'], df['Low'], df['Close']\n",
    "            current_price = close.iloc[-1]\n",
    "            \n",
    "            # Calculate pivot points\n",
    "            recent_data = df.tail(window * 3)  # Use more data for better accuracy\n",
    "            \n",
    "            # Find local maxima and minima\n",
    "            highs = recent_data['High'].rolling(window=window, center=True).max()\n",
    "            lows = recent_data['Low'].rolling(window=window, center=True).min()\n",
    "            \n",
    "            # Identify resistance levels (local maxima)\n",
    "            resistance_candidates = []\n",
    "            for i in range(window, len(recent_data) - window):\n",
    "                if recent_data['High'].iloc[i] == highs.iloc[i]:\n",
    "                    resistance_candidates.append(recent_data['High'].iloc[i])\n",
    "            \n",
    "            # Identify support levels (local minima)\n",
    "            support_candidates = []\n",
    "            for i in range(window, len(recent_data) - window):\n",
    "                if recent_data['Low'].iloc[i] == lows.iloc[i]:\n",
    "                    support_candidates.append(recent_data['Low'].iloc[i])\n",
    "            \n",
    "            # Find the closest support and resistance levels\n",
    "            resistance_above = [r for r in resistance_candidates if r > current_price]\n",
    "            support_below = [s for s in support_candidates if s < current_price]\n",
    "            \n",
    "            nearest_resistance = min(resistance_above) if resistance_above else current_price * 1.05\n",
    "            nearest_support = max(support_below) if support_below else current_price * 0.95\n",
    "            \n",
    "            # Calculate additional levels using different methods\n",
    "            # Fibonacci retracements\n",
    "            recent_high = df['High'].tail(252).max()  # 1 year high\n",
    "            recent_low = df['Low'].tail(252).min()    # 1 year low\n",
    "            \n",
    "            fib_levels = self._calculate_fibonacci_levels(recent_high, recent_low)\n",
    "            \n",
    "            return {\n",
    "                'nearest_support': nearest_support,\n",
    "                'nearest_resistance': nearest_resistance,\n",
    "                'support_strength': len([s for s in support_candidates if abs(s - nearest_support) < current_price * 0.02]),\n",
    "                'resistance_strength': len([r for r in resistance_candidates if abs(r - nearest_resistance) < current_price * 0.02]),\n",
    "                'fibonacci_levels': fib_levels,\n",
    "                'pivot_point': (recent_data['High'].iloc[-1] + recent_data['Low'].iloc[-1] + recent_data['Close'].iloc[-1]) / 3\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error identifying support/resistance: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_fibonacci_levels(self, high: float, low: float) -> Dict[str, float]:\n",
    "        \"\"\"Calculate Fibonacci retracement levels\"\"\"\n",
    "        diff = high - low\n",
    "        return {\n",
    "            'fib_0': high,\n",
    "            'fib_23.6': high - (diff * 0.236),\n",
    "            'fib_38.2': high - (diff * 0.382),\n",
    "            'fib_50': high - (diff * 0.5),\n",
    "            'fib_61.8': high - (diff * 0.618),\n",
    "            'fib_100': low\n",
    "        }\n",
    "\n",
    "# Cell 19: Chart pattern recognition and trend analysis\n",
    "    def analyze_trend_and_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze trend direction and identify chart patterns\"\"\"\n",
    "        try:\n",
    "            close = df['Close']\n",
    "            current_price = close.iloc[-1]\n",
    "            \n",
    "            # Trend analysis using multiple timeframes\n",
    "            trends = {\n",
    "                'short_term': self._determine_trend(df.tail(20)),  # 20 days\n",
    "                'medium_term': self._determine_trend(df.tail(50)), # 50 days\n",
    "                'long_term': self._determine_trend(df.tail(200)) if len(df) >= 200 else 'Neutral'  # 200 days\n",
    "            }\n",
    "            \n",
    "            # Overall trend (weighted)\n",
    "            trend_scores = {\n",
    "                'Upward': 1,\n",
    "                'Sideways': 0,\n",
    "                'Downward': -1,\n",
    "                'Neutral': 0\n",
    "            }\n",
    "            \n",
    "            weighted_trend = (\n",
    "                trend_scores.get(trends['short_term'], 0) * 0.5 +\n",
    "                trend_scores.get(trends['medium_term'], 0) * 0.3 +\n",
    "                trend_scores.get(trends['long_term'], 0) * 0.2\n",
    "            )\n",
    "            \n",
    "            if weighted_trend > 0.3:\n",
    "                overall_trend = 'Upward'\n",
    "            elif weighted_trend < -0.3:\n",
    "                overall_trend = 'Downward'\n",
    "            else:\n",
    "                overall_trend = 'Sideways'\n",
    "            \n",
    "            # Pattern recognition\n",
    "            patterns = self._identify_chart_patterns(df)\n",
    "            \n",
    "            # Momentum analysis\n",
    "            momentum = self._analyze_momentum(df)\n",
    "            \n",
    "            return {\n",
    "                'trends': trends,\n",
    "                'overall_trend': overall_trend,\n",
    "                'trend_strength': abs(weighted_trend),\n",
    "                'patterns': patterns,\n",
    "                'momentum': momentum,\n",
    "                'trend_score': weighted_trend\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in trend and pattern analysis: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _determine_trend(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Determine trend direction for given timeframe\"\"\"\n",
    "        try:\n",
    "            if len(df) < 10:\n",
    "                return 'Neutral'\n",
    "            \n",
    "            close = df['Close']\n",
    "            sma_short = close.rolling(window=min(10, len(df)//2)).mean()\n",
    "            sma_long = close.rolling(window=min(20, len(df))).mean()\n",
    "            \n",
    "            current_price = close.iloc[-1]\n",
    "            sma_short_current = sma_short.iloc[-1]\n",
    "            sma_long_current = sma_long.iloc[-1]\n",
    "            \n",
    "            # Price vs moving averages\n",
    "            if current_price > sma_short_current > sma_long_current:\n",
    "                return 'Upward'\n",
    "            elif current_price < sma_short_current < sma_long_current:\n",
    "                return 'Downward'\n",
    "            else:\n",
    "                # Check slope of moving averages\n",
    "                if len(sma_long) >= 5:\n",
    "                    sma_slope = (sma_long.iloc[-1] - sma_long.iloc[-5]) / sma_long.iloc[-5]\n",
    "                    if sma_slope > 0.02:\n",
    "                        return 'Upward'\n",
    "                    elif sma_slope < -0.02:\n",
    "                        return 'Downward'\n",
    "                \n",
    "                return 'Sideways'\n",
    "        except:\n",
    "            return 'Neutral'\n",
    "    \n",
    "    def _identify_chart_patterns(self, df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"Identify basic chart patterns\"\"\"\n",
    "        patterns = []\n",
    "        \n",
    "        try:\n",
    "            if len(df) < 50:\n",
    "                return patterns\n",
    "            \n",
    "            close = df['Close'].tail(50)\n",
    "            high = df['High'].tail(50)\n",
    "            low = df['Low'].tail(50)\n",
    "            \n",
    "            # Double top/bottom patterns\n",
    "            if self._is_double_top(high):\n",
    "                patterns.append('Double Top')\n",
    "            \n",
    "            if self._is_double_bottom(low):\n",
    "                patterns.append('Double Bottom')\n",
    "            \n",
    "            # Head and shoulders\n",
    "            if self._is_head_and_shoulders(high):\n",
    "                patterns.append('Head and Shoulders')\n",
    "            \n",
    "            # Triangle patterns\n",
    "            triangle_pattern = self._identify_triangle(high, low)\n",
    "            if triangle_pattern:\n",
    "                patterns.append(triangle_pattern)\n",
    "            \n",
    "            # Flag and pennant\n",
    "            flag_pattern = self._identify_flag_pennant(close, high, low)\n",
    "            if flag_pattern:\n",
    "                patterns.append(flag_pattern)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error identifying patterns: {e}\")\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _is_double_top(self, high: pd.Series) -> bool:\n",
    "        \"\"\"Identify double top pattern\"\"\"\n",
    "        try:\n",
    "            peaks = []\n",
    "            for i in range(5, len(high) - 5):\n",
    "                if high.iloc[i] == high.iloc[i-5:i+6].max():\n",
    "                    peaks.append((i, high.iloc[i]))\n",
    "            \n",
    "            if len(peaks) >= 2:\n",
    "                # Check if two highest peaks are similar in height\n",
    "                peaks_sorted = sorted(peaks, key=lambda x: x[1], reverse=True)\n",
    "                peak1, peak2 = peaks_sorted[0], peaks_sorted[1]\n",
    "                height_diff = abs(peak1[1] - peak2[1]) / peak1[1]\n",
    "                return height_diff < 0.03  # Within 3%\n",
    "            \n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _is_double_bottom(self, low: pd.Series) -> bool:\n",
    "        \"\"\"Identify double bottom pattern\"\"\"\n",
    "        try:\n",
    "            troughs = []\n",
    "            for i in range(5, len(low) - 5):\n",
    "                if low.iloc[i] == low.iloc[i-5:i+6].min():\n",
    "                    troughs.append((i, low.iloc[i]))\n",
    "            \n",
    "            if len(troughs) >= 2:\n",
    "                # Check if two lowest troughs are similar in depth\n",
    "                troughs_sorted = sorted(troughs, key=lambda x: x[1])\n",
    "                trough1, trough2 = troughs_sorted[0], troughs_sorted[1]\n",
    "                depth_diff = abs(trough1[1] - trough2[1]) / trough1[1]\n",
    "                return depth_diff < 0.03  # Within 3%\n",
    "            \n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _is_head_and_shoulders(self, high: pd.Series) -> bool:\n",
    "        \"\"\"Identify head and shoulders pattern\"\"\"\n",
    "        try:\n",
    "            peaks = []\n",
    "            for i in range(10, len(high) - 10):\n",
    "                if high.iloc[i] == high.iloc[i-10:i+11].max():\n",
    "                    peaks.append((i, high.iloc[i]))\n",
    "            \n",
    "            if len(peaks) >= 3:\n",
    "                # Sort by height to find head (highest) and shoulders\n",
    "                peaks_sorted = sorted(peaks, key=lambda x: x[1], reverse=True)\n",
    "                head = peaks_sorted[0]\n",
    "                potential_shoulders = peaks_sorted[1:3]\n",
    "                \n",
    "                # Check if shoulders are similar height and head is significantly higher\n",
    "                if len(potential_shoulders) >= 2:\n",
    "                    shoulder_diff = abs(potential_shoulders[0][1] - potential_shoulders[1][1]) / potential_shoulders[0][1]\n",
    "                    head_shoulder_diff = (head[1] - potential_shoulders[0][1]) / potential_shoulders[0][1]\n",
    "                    \n",
    "                    return shoulder_diff < 0.05 and head_shoulder_diff > 0.1\n",
    "            \n",
    "            return False\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "   def _identify_triangle(self, high: pd.Series, low: pd.Series) -> Optional[str]:\n",
    "        \"\"\"Identify triangle patterns\"\"\"\n",
    "        try:\n",
    "            # Simple triangle identification based on converging trend lines\n",
    "            recent_highs = high.tail(20)\n",
    "            recent_lows = low.tail(20)\n",
    "            \n",
    "            # Calculate trend lines\n",
    "            high_slope = np.polyfit(range(len(recent_highs)), recent_highs, 1)[0]\n",
    "            low_slope = np.polyfit(range(len(recent_lows)), recent_lows, 1)[0]\n",
    "            \n",
    "            # Ascending triangle: horizontal resistance, rising support\n",
    "            if abs(high_slope) < 0.01 and low_slope > 0.01:\n",
    "                return 'Ascending Triangle'\n",
    "            \n",
    "            # Descending triangle: falling resistance, horizontal support\n",
    "            elif high_slope < -0.01 and abs(low_slope) < 0.01:\n",
    "                return 'Descending Triangle'\n",
    "            \n",
    "            # Symmetrical triangle: converging lines\n",
    "            elif high_slope < -0.01 and low_slope > 0.01:\n",
    "                return 'Symmetrical Triangle'\n",
    "            \n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _identify_flag_pennant(self, close: pd.Series, high: pd.Series, low: pd.Series) -> Optional[str]:\n",
    "        \"\"\"Identify flag and pennant patterns\"\"\"\n",
    "        try:\n",
    "            # Look for strong move followed by consolidation\n",
    "            recent_data = close.tail(30)\n",
    "            \n",
    "            # Check for strong initial move (first 10 periods)\n",
    "            initial_move = recent_data.iloc[:10]\n",
    "            consolidation = recent_data.iloc[10:]\n",
    "            \n",
    "            initial_change = (initial_move.iloc[-1] - initial_move.iloc[0]) / initial_move.iloc[0]\n",
    "            consolidation_volatility = consolidation.std() / consolidation.mean()\n",
    "            \n",
    "            # Strong move (>5%) followed by low volatility consolidation\n",
    "            if abs(initial_change) > 0.05 and consolidation_volatility < 0.02:\n",
    "                if initial_change > 0:\n",
    "                    return 'Bull Flag'\n",
    "                else:\n",
    "                    return 'Bear Flag'\n",
    "            \n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def _analyze_momentum(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze price momentum\"\"\"\n",
    "        try:\n",
    "            close = df['Close']\n",
    "            \n",
    "            # Rate of change over different periods\n",
    "            roc_5 = ((close.iloc[-1] - close.iloc[-6]) / close.iloc[-6]) * 100 if len(close) > 5 else 0\n",
    "            roc_10 = ((close.iloc[-1] - close.iloc[-11]) / close.iloc[-11]) * 100 if len(close) > 10 else 0\n",
    "            roc_20 = ((close.iloc[-1] - close.iloc[-21]) / close.iloc[-21]) * 100 if len(close) > 20 else 0\n",
    "            \n",
    "            # Price momentum score\n",
    "            momentum_scores = [roc_5 * 0.5, roc_10 * 0.3, roc_20 * 0.2]\n",
    "            momentum_score = sum(momentum_scores)\n",
    "            \n",
    "            # Momentum classification\n",
    "            if momentum_score > 5:\n",
    "                momentum_strength = 'Strong Positive'\n",
    "            elif momentum_score > 2:\n",
    "                momentum_strength = 'Moderate Positive'\n",
    "            elif momentum_score > -2:\n",
    "                momentum_strength = 'Neutral'\n",
    "            elif momentum_score > -5:\n",
    "                momentum_strength = 'Moderate Negative'\n",
    "            else:\n",
    "                momentum_strength = 'Strong Negative'\n",
    "            \n",
    "            return {\n",
    "                'momentum_score': momentum_score,\n",
    "                'momentum_strength': momentum_strength,\n",
    "                'roc_5d': roc_5,\n",
    "                'roc_10d': roc_10,\n",
    "                'roc_20d': roc_20\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error analyzing momentum: {e}\")\n",
    "            return {}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 20-24: Implement Risk Assessment Agent Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Risk Assessment Agent class\n",
    "class RiskAssessmentAgent:\n",
    "    def __init__(self, config: AnalysisConfig):\n",
    "        self.config = config\n",
    "        self.risk_free_rate = config.risk_free_rate\n",
    "    \n",
    "    def calculate_comprehensive_risk(self, df: pd.DataFrame, symbol: str, \n",
    "                                   market_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Calculate comprehensive risk metrics\"\"\"\n",
    "        try:\n",
    "            risk_metrics = {}\n",
    "            \n",
    "            # Price volatility risk\n",
    "            volatility_risk = self._calculate_volatility_risk(df)\n",
    "            risk_metrics.update(volatility_risk)\n",
    "            \n",
    "            # Market risk (Beta)\n",
    "            market_risk = self._calculate_market_risk(df, symbol)\n",
    "            risk_metrics.update(market_risk)\n",
    "            \n",
    "            # Value at Risk\n",
    "            var_metrics = self._calculate_var(df)\n",
    "            risk_metrics.update(var_metrics)\n",
    "            \n",
    "            # Liquidity risk\n",
    "            liquidity_risk = self._calculate_liquidity_risk(df, market_data)\n",
    "            risk_metrics.update(liquidity_risk)\n",
    "            \n",
    "            # Fundamental risk\n",
    "            fundamental_risk = self._calculate_fundamental_risk(market_data)\n",
    "            risk_metrics.update(fundamental_risk)\n",
    "            \n",
    "            # Overall risk score\n",
    "            overall_risk = self._calculate_overall_risk_score(risk_metrics)\n",
    "            risk_metrics['overall_risk_score'] = overall_risk\n",
    "            \n",
    "            return risk_metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating comprehensive risk: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_volatility_risk(self, df: pd.DataFrame) -> Dict[str, float]:\n",
    "        \"\"\"Calculate volatility-based risk metrics\"\"\"\n",
    "        try:\n",
    "            close = df['Close']\n",
    "            returns = close.pct_change().dropna()\n",
    "            \n",
    "            # Historical volatility (annualized)\n",
    "            daily_vol = returns.std()\n",
    "            annual_vol = daily_vol * np.sqrt(252)\n",
    "            \n",
    "            # Volatility percentiles\n",
    "            vol_30d = returns.tail(30).std() * np.sqrt(252)\n",
    "            vol_90d = returns.tail(90).std() * np.sqrt(252) if len(returns) >= 90 else annual_vol\n",
    "            \n",
    "            # Volatility risk score (0-1, where 1 is highest risk)\n",
    "            # Compare to typical stock volatility (15-25%)\n",
    "            vol_risk_score = min(annual_vol / 0.4, 1.0)  # Cap at 40% volatility\n",
    "            \n",
    "            return {\n",
    "                'annual_volatility': annual_vol,\n",
    "                'volatility_30d': vol_30d,\n",
    "                'volatility_90d': vol_90d,\n",
    "                'volatility_risk_score': vol_risk_score,\n",
    "                'volatility_percentile': self._calculate_volatility_percentile(returns)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating volatility risk: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_volatility_percentile(self, returns: pd.Series) -> float:\n",
    "        \"\"\"Calculate current volatility percentile vs historical\"\"\"\n",
    "        try:\n",
    "            if len(returns) < 60:\n",
    "                return 0.5\n",
    "            \n",
    "            # Rolling 30-day volatilities\n",
    "            rolling_vols = returns.rolling(30).std() * np.sqrt(252)\n",
    "            current_vol = rolling_vols.iloc[-1]\n",
    "            \n",
    "            # Percentile of current volatility\n",
    "            percentile = stats.percentileofscore(rolling_vols.dropna(), current_vol) / 100\n",
    "            return percentile\n",
    "        except:\n",
    "            return 0.5\n",
    "\n",
    "# Cell 21: Market risk calculation\n",
    "    def _calculate_market_risk(self, df: pd.DataFrame, symbol: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate market risk metrics including Beta\"\"\"\n",
    "        try:\n",
    "            # Get S&P 500 data for beta calculation\n",
    "            spy = yf.Ticker(\"SPY\")\n",
    "            spy_hist = spy.history(period=\"1y\")\n",
    "            \n",
    "            # Align dates\n",
    "            common_dates = df.index.intersection(spy_hist.index)\n",
    "            if len(common_dates) < 30:  # Need at least 30 data points\n",
    "                return {'beta': 1.0, 'correlation_with_market': 0.0}\n",
    "            \n",
    "            stock_returns = df.loc[common_dates]['Close'].pct_change().dropna()\n",
    "            market_returns = spy_hist.loc[common_dates]['Close'].pct_change().dropna()\n",
    "            \n",
    "            # Align returns\n",
    "            common_dates_returns = stock_returns.index.intersection(market_returns.index)\n",
    "            stock_returns = stock_returns.loc[common_dates_returns]\n",
    "            market_returns = market_returns.loc[common_dates_returns]\n",
    "            \n",
    "            # Calculate beta\n",
    "            covariance = np.cov(stock_returns, market_returns)[0, 1]\n",
    "            market_variance = np.var(market_returns)\n",
    "            beta = covariance / market_variance if market_variance > 0 else 1.0\n",
    "            \n",
    "            # Calculate correlation\n",
    "            correlation = np.corrcoef(stock_returns, market_returns)[0, 1]\n",
    "            \n",
    "            # Calculate alpha (excess return)\n",
    "            stock_mean_return = stock_returns.mean()\n",
    "            market_mean_return = market_returns.mean()\n",
    "            alpha = stock_mean_return - (self.risk_free_rate/252 + beta * (market_mean_return - self.risk_free_rate/252))\n",
    "            \n",
    "            # Market risk score\n",
    "            market_risk_score = min(abs(beta - 1) + (1 - abs(correlation)), 1.0)\n",
    "            \n",
    "            return {\n",
    "                'beta': beta,\n",
    "                'alpha': alpha * 252,  # Annualized\n",
    "                'correlation_with_market': correlation,\n",
    "                'market_risk_score': market_risk_score,\n",
    "                'systematic_risk': beta * market_returns.std() * np.sqrt(252)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating market risk: {e}\")\n",
    "            return {'beta': 1.0, 'correlation_with_market': 0.0, 'market_risk_score': 0.5}\n",
    "\n",
    "# Cell 22: Value at Risk calculation\n",
    "    def _calculate_var(self, df: pd.DataFrame, confidence_levels: List[float] = [0.95, 0.99]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate Value at Risk using multiple methods\"\"\"\n",
    "        try:\n",
    "            close = df['Close']\n",
    "            returns = close.pct_change().dropna()\n",
    "            \n",
    "            if len(returns) < 30:\n",
    "                return {}\n",
    "            \n",
    "            var_metrics = {}\n",
    "            \n",
    "            # Historical VaR\n",
    "            for confidence in confidence_levels:\n",
    "                var_percentile = (1 - confidence) * 100\n",
    "                historical_var = np.percentile(returns, var_percentile)\n",
    "                var_metrics[f'var_historical_{int(confidence*100)}'] = historical_var\n",
    "            \n",
    "            # Parametric VaR (assuming normal distribution)\n",
    "            mean_return = returns.mean()\n",
    "            std_return = returns.std()\n",
    "            \n",
    "            for confidence in confidence_levels:\n",
    "                z_score = stats.norm.ppf(1 - confidence)\n",
    "                parametric_var = mean_return + z_score * std_return\n",
    "                var_metrics[f'var_parametric_{int(confidence*100)}'] = parametric_var\n",
    "            \n",
    "            # Monte Carlo VaR\n",
    "            mc_var = self._monte_carlo_var(returns, confidence_levels)\n",
    "            var_metrics.update(mc_var)\n",
    "            \n",
    "            # Expected Shortfall (CVaR)\n",
    "            for confidence in confidence_levels:\n",
    "                var_threshold = var_metrics.get(f'var_historical_{int(confidence*100)}', 0)\n",
    "                expected_shortfall = returns[returns <= var_threshold].mean()\n",
    "                var_metrics[f'expected_shortfall_{int(confidence*100)}'] = expected_shortfall\n",
    "            \n",
    "            # Maximum Drawdown\n",
    "            var_metrics['max_drawdown'] = self._calculate_max_drawdown(close)\n",
    "            \n",
    "            return var_metrics\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating VaR: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _monte_carlo_var(self, returns: pd.Series, confidence_levels: List[float], \n",
    "                        simulations: int = 10000) -> Dict[str, float]:\n",
    "        \"\"\"Calculate VaR using Monte Carlo simulation\"\"\"\n",
    "        try:\n",
    "            mean_return = returns.mean()\n",
    "            std_return = returns.std()\n",
    "            \n",
    "            # Generate random returns\n",
    "            np.random.seed(42)  # For reproducibility\n",
    "            simulated_returns = np.random.normal(mean_return, std_return, simulations)\n",
    "            \n",
    "            mc_var = {}\n",
    "            for confidence in confidence_levels:\n",
    "                var_percentile = (1 - confidence) * 100\n",
    "                mc_var_value = np.percentile(simulated_returns, var_percentile)\n",
    "                mc_var[f'var_monte_carlo_{int(confidence*100)}'] = mc_var_value\n",
    "            \n",
    "            return mc_var\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in Monte Carlo VaR: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_max_drawdown(self, prices: pd.Series) -> float:\n",
    "        \"\"\"Calculate maximum drawdown\"\"\"\n",
    "        try:\n",
    "            peak = prices.expanding().max()\n",
    "            drawdown = (prices - peak) / peak\n",
    "            return drawdown.min()\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "# Cell 23: Liquidity and fundamental risk\n",
    "    def _calculate_liquidity_risk(self, df: pd.DataFrame, market_data: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate liquidity risk metrics\"\"\"\n",
    "        try:\n",
    "            volume = df['Volume']\n",
    "            close = df['Close']\n",
    "            \n",
    "            # Average daily volume\n",
    "            avg_volume = volume.mean()\n",
    "            \n",
    "            # Volume volatility\n",
    "            volume_volatility = volume.std() / avg_volume if avg_volume > 0 else 1\n",
    "            \n",
    "            # Bid-ask spread proxy (using high-low spread)\n",
    "            spread_proxy = ((df['High'] - df['Low']) / df['Close']).mean()\n",
    "            \n",
    "            # Market cap from market_data\n",
    "            market_cap = market_data.get('market_cap', 0)\n",
    "            \n",
    "            # Liquidity risk score\n",
    "            if market_cap > 10e9:  # Large cap\n",
    "                size_risk = 0.1\n",
    "            elif market_cap > 2e9:  # Mid cap\n",
    "                size_risk = 0.3\n",
    "            else:  # Small cap\n",
    "                size_risk = 0.6\n",
    "            \n",
    "            # Volume risk (lower volume = higher risk)\n",
    "            volume_risk = min(1.0, 1000000 / avg_volume) if avg_volume > 0 else 1.0\n",
    "            \n",
    "            liquidity_risk_score = (size_risk + volume_risk + min(spread_proxy * 10, 1.0)) / 3\n",
    "            \n",
    "            return {\n",
    "                'avg_volume': avg_volume,\n",
    "                'volume_volatility': volume_volatility,\n",
    "                'spread_proxy': spread_proxy,\n",
    "                'liquidity_risk_score': liquidity_risk_score,\n",
    "                'market_cap_risk': size_risk\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating liquidity risk: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _calculate_fundamental_risk(self, market_data: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate fundamental risk based on financial metrics\"\"\"\n",
    "        try:\n",
    "            financial_ratios = market_data.get('financial_data', {}).get('financial_ratios', {})\n",
    "            \n",
    "            if not financial_ratios:\n",
    "                return {'fundamental_risk_score': 0.5}\n",
    "            \n",
    "            risk_factors = {}\n",
    "            \n",
    "            # P/E ratio risk\n",
    "            pe_ratio = financial_ratios.get('pe_ratio', 15)\n",
    "            if pe_ratio > 30 or pe_ratio < 5:\n",
    "                risk_factors['pe_risk'] = 0.8\n",
    "            elif pe_ratio > 25 or pe_ratio < 8:\n",
    "                risk_factors['pe_risk'] = 0.5\n",
    "            else:\n",
    "                risk_factors['pe_risk'] = 0.2\n",
    "            \n",
    "            # Debt-to-equity risk\n",
    "            debt_to_equity = financial_ratios.get('debt_to_equity', 0)\n",
    "            if debt_to_equity > 2:\n",
    "                risk_factors['debt_risk'] = 0.9\n",
    "            elif debt_to_equity > 1:\n",
    "                risk_factors['debt_risk'] = 0.6\n",
    "            else:\n",
    "                risk_factors['debt_risk'] = 0.3\n",
    "            \n",
    "            # Profitability risk\n",
    "            profit_margin = financial_ratios.get('profit_margin', 0)\n",
    "            if profit_margin < 0:\n",
    "                risk_factors['profitability_risk'] = 0.9\n",
    "            elif profit_margin < 0.05:\n",
    "                risk_factors['profitability_risk'] = 0.6\n",
    "            else:\n",
    "                risk_factors['profitability_risk'] = 0.2\n",
    "            \n",
    "            # Current ratio (liquidity)\n",
    "            current_ratio = financial_ratios.get('current_ratio', 1)\n",
    "            if current_ratio < 1:\n",
    "                risk_factors['liquidity_risk'] = 0.8\n",
    "            elif current_ratio < 1.5:\n",
    "                risk_factors['liquidity_risk'] = 0.4\n",
    "            else:\n",
    "                risk_factors['liquidity_risk'] = 0.1\n",
    "            \n",
    "            # Calculate overall fundamental risk\n",
    "            fundamental_risk_score = np.mean(list(risk_factors.values())) if risk_factors else 0.5\n",
    "            \n",
    "            return {\n",
    "                'fundamental_risk_score': fundamental_risk_score,\n",
    "                'risk_factors': risk_factors\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating fundamental risk: {e}\")\n",
    "            return {'fundamental_risk_score': 0.5}\n",
    "\n",
    "# Cell 24: Overall risk score calculation\n",
    "    def _calculate_overall_risk_score(self, risk_metrics: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate overall risk score and classification\"\"\"\n",
    "        try:\n",
    "            # Weight different risk components\n",
    "            weights = {\n",
    "                'volatility': 0.3,\n",
    "                'market': 0.2,\n",
    "                'liquidity': 0.2,\n",
    "                'fundamental': 0.2,\n",
    "                'var': 0.1\n",
    "            }\n",
    "            \n",
    "            # Extract risk scores\n",
    "            volatility_risk = risk_metrics.get('volatility_risk_score', 0.5)\n",
    "            market_risk = risk_metrics.get('market_risk_score', 0.5)\n",
    "            liquidity_risk = risk_metrics.get('liquidity_risk_score', 0.5)\n",
    "            fundamental_risk = risk_metrics.get('fundamental_risk_score', 0.5)\n",
    "            \n",
    "            # VaR risk (convert to 0-1 scale)\n",
    "            var_95 = abs(risk_metrics.get('var_historical_95', -0.02))\n",
    "            var_risk = min(var_95 / 0.1, 1.0)  # Normalize to 10% daily loss as maximum\n",
    "            \n",
    "            # Calculate weighted overall risk\n",
    "            overall_risk = (\n",
    "                volatility_risk * weights['volatility'] +\n",
    "                market_risk * weights['market'] +\n",
    "                liquidity_risk * weights['liquidity'] +\n",
    "                fundamental_risk * weights['fundamental'] +\n",
    "                var_risk * weights['var']\n",
    "            )\n",
    "            \n",
    "            # Risk classification\n",
    "            if overall_risk < 0.3:\n",
    "                risk_category = 'Low'\n",
    "            elif overall_risk < 0.6:\n",
    "                risk_category = 'Medium'\n",
    "            else:\n",
    "                risk_category = 'High'\n",
    "            \n",
    "            return {\n",
    "                'overall_risk_score': overall_risk,\n",
    "                'risk_category': risk_category,\n",
    "                'risk_components': {\n",
    "                    'volatility_risk': volatility_risk,\n",
    "                    'market_risk': market_risk,\n",
    "                    'liquidity_risk': liquidity_risk,\n",
    "                    'fundamental_risk': fundamental_risk,\n",
    "                    'var_risk': var_risk\n",
    "                },\n",
    "                'weights_used': weights\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating overall risk score: {e}\")\n",
    "            return {'overall_risk_score': 0.5, 'risk_category': 'Medium'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 25-29: Implement Decision Making Agent Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25: Decision Making Agent class\n",
    "class DecisionMakingAgent:\n",
    "    def __init__(self, config: AnalysisConfig):\n",
    "        self.config = config\n",
    "        self.confidence_threshold = config.confidence_threshold\n",
    "    \n",
    "    def make_investment_decision(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Make final investment recommendation based on all analyses\"\"\"\n",
    "        try:\n",
    "            # Extract analysis results\n",
    "            sentiment_data = self._extract_sentiment_data(state)\n",
    "            technical_data = self._extract_technical_data(state)\n",
    "            risk_data = self._extract_risk_data(state)\n",
    "            fundamental_data = self._extract_fundamental_data(state)\n",
    "            \n",
    "            # Calculate individual recommendation scores\n",
    "            sentiment_score = self._calculate_sentiment_score(sentiment_data)\n",
    "            technical_score = self._calculate_technical_score(technical_data)\n",
    "            risk_score = self._calculate_risk_score(risk_data)\n",
    "            fundamental_score = self._calculate_fundamental_score(fundamental_data)\n",
    "            \n",
    "            # Weight and combine scores\n",
    "            weighted_score = self._combine_scores(\n",
    "                sentiment_score, technical_score, risk_score, fundamental_score\n",
    "            )\n",
    "            \n",
    "            # Generate recommendation\n",
    "            recommendation = self._generate_recommendation(weighted_score)\n",
    "            \n",
    "            # Calculate confidence\n",
    "            confidence = self._calculate_confidence(\n",
    "                sentiment_score, technical_score, risk_score, fundamental_score, weighted_score\n",
    "            )\n",
    "            \n",
    "            # Set price targets and stop loss\n",
    "            price_targets = self._calculate_price_targets(state, recommendation, technical_data, risk_data)\n",
    "            \n",
    "            # Generate rationale\n",
    "            rationale = self._generate_rationale(\n",
    "                recommendation, sentiment_data, technical_data, risk_data, \n",
    "                fundamental_data, confidence\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'recommendation': recommendation,\n",
    "                'confidence_score': confidence,\n",
    "                'weighted_score': weighted_score,\n",
    "                'component_scores': {\n",
    "                    'sentiment': sentiment_score,\n",
    "                    'technical': technical_score,\n",
    "                    'risk': risk_score,\n",
    "                    'fundamental': fundamental_score\n",
    "                },\n",
    "                'price_target': price_targets['target'],\n",
    "                'stop_loss': price_targets['stop_loss'],\n",
    "                'upside_potential': price_targets['upside_percent'],\n",
    "                'downside_risk': price_targets['downside_percent'],\n",
    "                'risk_reward_ratio': price_targets['risk_reward_ratio'],\n",
    "                'rationale': rationale,\n",
    "                'time_horizon': self._determine_time_horizon(technical_data, recommendation)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error making investment decision: {e}\")\n",
    "            return self._default_decision()\n",
    "    \n",
    "    def _extract_sentiment_data(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract sentiment analysis data from state\"\"\"\n",
    "        return {\n",
    "            'overall_sentiment': state.get('overall_sentiment', 0.0),\n",
    "            'news_sentiment': state.get('news_sentiment', 0.0),\n",
    "            'social_sentiment': state.get('social_sentiment', 0.0),\n",
    "            'analyst_consensus': state.get('analyst_consensus', 0.0),\n",
    "            'sentiment_details': state.get('sentiment_details', {})\n",
    "        }\n",
    "    \n",
    "    def _extract_technical_data(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract technical analysis data from state\"\"\"\n",
    "        return {\n",
    "            'technical_indicators': state.get('technical_indicators', {}),\n",
    "            'trend_direction': state.get('trend_direction', 'Neutral'),\n",
    "            'support_resistance': state.get('support_resistance', {}),\n",
    "            'chart_patterns': state.get('chart_patterns', [])\n",
    "        }\n",
    "    \n",
    "    def _extract_risk_data(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract risk assessment data from state\"\"\"\n",
    "        return {\n",
    "            'risk_score': state.get('risk_score', 0.5),\n",
    "            'volatility': state.get('volatility', 0.2),\n",
    "            'beta': state.get('beta', 1.0),\n",
    "            'var_score': state.get('var_score', -0.02),\n",
    "            'risk_metrics': state.get('risk_metrics', {})\n",
    "        }\n",
    "    \n",
    "    def _extract_fundamental_data(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract fundamental analysis data from state\"\"\"\n",
    "        return {\n",
    "            'financial_data': state.get('financial_data', {}),\n",
    "            'market_cap': state.get('market_cap', 0),\n",
    "            'pe_ratio': state.get('financial_data', {}).get('financial_ratios', {}).get('pe_ratio', 15)\n",
    "        }\n",
    "\n",
    "# Cell 26: Score calculation methods\n",
    "    def _calculate_sentiment_score(self, sentiment_data: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate sentiment-based recommendation score (-1 to 1)\"\"\"\n",
    "        try:\n",
    "            overall_sentiment = sentiment_data.get('overall_sentiment', 0.0)\n",
    "            \n",
    "            # Normalize sentiment to -1 to 1 scale\n",
    "            sentiment_score = max(-1, min(1, overall_sentiment))\n",
    "            \n",
    "            # Apply sentiment strength weighting\n",
    "            news_sentiment = sentiment_data.get('news_sentiment', 0.0)\n",
    "            social_sentiment = sentiment_data.get('social_sentiment', 0.0)\n",
    "            analyst_sentiment = sentiment_data.get('analyst_consensus', 0.0)\n",
    "            \n",
    "            # Check for sentiment agreement (higher weight if all sources agree)\n",
    "            sentiments = [news_sentiment, social_sentiment, analyst_sentiment]\n",
    "            sentiment_std = np.std([s for s in sentiments if s != 0])\n",
    "            \n",
    "            # Lower standard deviation means more agreement\n",
    "            agreement_bonus = max(0, (0.5 - sentiment_std) / 0.5) * 0.2 if sentiment_std > 0 else 0\n",
    "            \n",
    "            final_score = sentiment_score + (agreement_bonus if sentiment_score > 0 else -agreement_bonus)\n",
    "            return max(-1, min(1, final_score))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating sentiment score: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_technical_score(self, technical_data: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate technical analysis score (-1 to 1)\"\"\"\n",
    "        try:\n",
    "            indicators = technical_data.get('technical_indicators', {})\n",
    "            trend = technical_data.get('trend_direction', 'Neutral')\n",
    "            patterns = technical_data.get('chart_patterns', [])\n",
    "            \n",
    "            score_components = []\n",
    "            \n",
    "            # Trend score\n",
    "            trend_scores = {'Upward': 0.6, 'Sideways': 0.0, 'Downward': -0.6, 'Neutral': 0.0}\n",
    "            trend_score = trend_scores.get(trend, 0.0)\n",
    "            score_components.append(trend_score)\n",
    "            \n",
    "            # RSI score\n",
    "            rsi = indicators.get('rsi', 50)\n",
    "            if rsi > 70:\n",
    "                rsi_score = -0.3  # Overbought\n",
    "            elif rsi < 30:\n",
    "                rsi_score = 0.3   # Oversold\n",
    "            else:\n",
    "                rsi_score = (50 - abs(rsi - 50)) / 100  # Closer to 50 is neutral\n",
    "            score_components.append(rsi_score)\n",
    "            \n",
    "            # MACD score\n",
    "            macd = indicators.get('macd', 0)\n",
    "            macd_signal = indicators.get('macd_signal', 0)\n",
    "            macd_score = 0.2 if macd > macd_signal else -0.2\n",
    "            score_components.append(macd_score)\n",
    "            \n",
    "            # Moving average score\n",
    "            sma_20 = indicators.get('sma_20', 0)\n",
    "            sma_50 = indicators.get('sma_50', 0)\n",
    "            current_price = indicators.get('current_price', sma_20)\n",
    "            \n",
    "            ma_score = 0\n",
    "            if current_price > sma_20 > sma_50:\n",
    "                ma_score = 0.4\n",
    "            elif current_price < sma_20 < sma_50:\n",
    "                ma_score = -0.4\n",
    "            score_components.append(ma_score)\n",
    "            \n",
    "            # Pattern score\n",
    "            pattern_score = 0\n",
    "            bullish_patterns = ['Double Bottom', 'Bull Flag', 'Ascending Triangle']\n",
    "            bearish_patterns = ['Double Top', 'Bear Flag', 'Descending Triangle', 'Head and Shoulders']\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                if pattern in bullish_patterns:\n",
    "                    pattern_score += 0.2\n",
    "                elif pattern in bearish_patterns:\n",
    "                    pattern_score -= 0.2\n",
    "            \n",
    "            score_components.append(pattern_score)\n",
    "            \n",
    "            # Calculate weighted average\n",
    "            final_score = np.mean(score_components)\n",
    "            return max(-1, min(1, final_score))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating technical score: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_risk_score(self, risk_data: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate risk-adjusted score (-1 to 1, where negative = high risk)\"\"\"\n",
    "        try:\n",
    "            overall_risk = risk_data.get('risk_score', 0.5)\n",
    "            volatility = risk_data.get('volatility', 0.2)\n",
    "            beta = risk_data.get('beta', 1.0)\n",
    "            \n",
    "            # Convert risk to score (lower risk = higher score)\n",
    "            risk_score = 1 - (overall_risk * 2)  # Convert 0-1 risk to 1 to -1 score\n",
    "            \n",
    "            # Adjust for volatility preference\n",
    "            vol_adjustment = max(-0.3, min(0.1, (0.25 - volatility) / 0.25))\n",
    "            \n",
    "            # Beta adjustment (prefer moderate beta)\n",
    "            beta_adjustment = max(-0.2, min(0.1, (1.5 - abs(beta - 1)) / 1.5))\n",
    "            \n",
    "            final_score = risk_score + vol_adjustment + beta_adjustment\n",
    "            return max(-1, min(1, final_score))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating risk score: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_fundamental_score(self, fundamental_data: Dict[str, Any]) -> float:\n",
    "        \"\"\"Calculate fundamental analysis score (-1 to 1)\"\"\"\n",
    "        try:\n",
    "            financial_data = fundamental_data.get('financial_data', {})\n",
    "            ratios = financial_data.get('financial_ratios', {})\n",
    "            \n",
    "            if not ratios:\n",
    "                return 0.0\n",
    "            \n",
    "            score_components = []\n",
    "            \n",
    "            # P/E ratio score\n",
    "            pe_ratio = ratios.get('pe_ratio', 15)\n",
    "            if 10 <= pe_ratio <= 20:\n",
    "                pe_score = 0.3\n",
    "            elif 8 <= pe_ratio <= 25:\n",
    "                pe_score = 0.1\n",
    "            elif pe_ratio > 30:\n",
    "                pe_score = -0.3\n",
    "            else:\n",
    "                pe_score = 0.0\n",
    "            score_components.append(pe_score)\n",
    "            \n",
    "            # ROE score\n",
    "            roe = ratios.get('roe', 0)\n",
    "            if roe > 0.15:\n",
    "                roe_score = 0.3\n",
    "            elif roe > 0.10:\n",
    "                roe_score = 0.1\n",
    "            elif roe < 0:\n",
    "                roe_score = -0.4\n",
    "            else:\n",
    "                roe_score = 0.0\n",
    "            score_components.append(roe_score)\n",
    "            \n",
    "            # Debt-to-equity score\n",
    "            debt_equity = ratios.get('debt_to_equity', 0)\n",
    "            if debt_equity < 0.5:\n",
    "                debt_score = 0.2\n",
    "            elif debt_equity < 1.0:\n",
    "                debt_score = 0.1\n",
    "            elif debt_equity > 2.0:\n",
    "                debt_score = -0.3\n",
    "            else:\n",
    "                debt_score = -0.1\n",
    "            score_components.append(debt_score)\n",
    "            \n",
    "            # Profit margin score\n",
    "            profit_margin = ratios.get('profit_margin', 0)\n",
    "            if profit_margin > 0.2:\n",
    "                margin_score = 0.3\n",
    "            elif profit_margin > 0.1:\n",
    "                margin_score = 0.2\n",
    "            elif profit_margin > 0.05:\n",
    "                margin_score = 0.1\n",
    "            elif profit_margin < 0:\n",
    "                margin_score = -0.4\n",
    "            else:\n",
    "                margin_score = 0.0\n",
    "            score_components.append(margin_score)\n",
    "            \n",
    "            final_score = np.mean(score_components)\n",
    "            return max(-1, min(1, final_score))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating fundamental score: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "# Cell 27: Score combination and recommendation generation\n",
    "    def _combine_scores(self, sentiment_score: float, technical_score: float, \n",
    "                       risk_score: float, fundamental_score: float) -> float:\n",
    "        \"\"\"Combine individual scores with weights\"\"\"\n",
    "        try:\n",
    "            # Define weights based on investment strategy\n",
    "            weights = {\n",
    "                'sentiment': 0.2,\n",
    "                'technical': 0.3,\n",
    "                'risk': 0.25,\n",
    "                'fundamental': 0.25\n",
    "            }\n",
    "            \n",
    "            weighted_score = (\n",
    "                sentiment_score * weights['sentiment'] +\n",
    "                technical_score * weights['technical'] +\n",
    "                risk_score * weights['risk'] +\n",
    "                fundamental_score * weights['fundamental']\n",
    "            )\n",
    "            \n",
    "            return max(-1, min(1, weighted_score))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error combining scores: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _generate_recommendation(self, weighted_score: float) -> str:\n",
    "        \"\"\"Generate recommendation based on weighted score\"\"\"\n",
    "        try:\n",
    "            if weighted_score > 0.6:\n",
    "                return 'STRONG BUY'\n",
    "            elif weighted_score > 0.2:\n",
    "                return 'BUY'\n",
    "            elif weighted_score > -0.2:\n",
    "                return 'HOLD'\n",
    "            elif weighted_score > -0.6:\n",
    "                return 'SELL'\n",
    "            else:\n",
    "                return 'STRONG SELL'\n",
    "        except:\n",
    "            return 'HOLD'\n",
    "    \n",
    "    def _calculate_confidence(self, sentiment_score: float, technical_score: float,\n",
    "                            risk_score: float, fundamental_score: float, \n",
    "                            weighted_score: float) -> float:\n",
    "        \"\"\"Calculate confidence score based on agreement between analyses\"\"\"\n",
    "        try:\n",
    "            scores = [sentiment_score, technical_score, risk_score, fundamental_score]\n",
    "            \n",
    "            # Calculate agreement (lower standard deviation = higher confidence)\n",
    "            score_std = np.std(scores)\n",
    "            max_std = 2.0  # Maximum possible std for scores from -1 to 1\n",
    "            \n",
    "            # Base confidence from agreement\n",
    "            agreement_confidence = max(0, (max_std - score_std) / max_std)\n",
    "            \n",
    "            # Boost confidence for stronger signals\n",
    "            signal_strength = abs(weighted_score)\n",
    "            strength_confidence = signal_strength\n",
    "            \n",
    "            # Combined confidence\n",
    "            confidence = (agreement_confidence * 0.6 + strength_confidence * 0.4) * 100\n",
    "            \n",
    "            return max(0, min(100, confidence))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating confidence: {e}\")\n",
    "            return 50.0\n",
    "\n",
    "# Cell 28: Price targets and risk-reward calculation\n",
    "    def _calculate_price_targets(self, state: Dict[str, Any], recommendation: str,\n",
    "                               technical_data: Dict[str, Any], risk_data: Dict[str, Any]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate price targets and stop-loss levels\"\"\"\n",
    "        try:\n",
    "            current_price = state.get('current_price', 100.0)\n",
    "            support_resistance = technical_data.get('support_resistance', {})\n",
    "            volatility = risk_data.get('volatility', 0.2)\n",
    "            \n",
    "            # Base target multipliers based on recommendation\n",
    "            target_multipliers = {\n",
    "                'STRONG BUY': 1.15,\n",
    "                'BUY': 1.08,\n",
    "                'HOLD': 1.02,\n",
    "                'SELL': 0.95,\n",
    "                'STRONG SELL': 0.85\n",
    "            }\n",
    "            \n",
    "            base_target = current_price * target_multipliers.get(recommendation, 1.0)\n",
    "            \n",
    "            # Adjust based on technical levels\n",
    "            resistance = support_resistance.get('nearest_resistance', current_price * 1.05)\n",
    "            support = support_resistance.get('nearest_support', current_price * 0.95)\n",
    "            \n",
    "            # For buy recommendations, target near resistance\n",
    "            if recommendation in ['STRONG BUY', 'BUY']:\n",
    "                if resistance > current_price:\n",
    "                    price_target = min(base_target, resistance * 0.98)  # Slightly below resistance\n",
    "                else:\n",
    "                    price_target = base_target\n",
    "                \n",
    "                # Stop loss below support\n",
    "                stop_loss = max(support * 0.98, current_price * 0.92)\n",
    "            \n",
    "            # For sell recommendations, target near support\n",
    "            elif recommendation in ['SELL', 'STRONG SELL']:\n",
    "                if support < current_price:\n",
    "                    price_target = max(base_target, support * 1.02)  # Slightly above support\n",
    "                else:\n",
    "                    price_target = base_target\n",
    "                \n",
    "                # Stop loss above resistance\n",
    "                stop_loss = min(resistance * 1.02, current_price * 1.08)\n",
    "            \n",
    "            # For hold, narrow targets\n",
    "            else:\n",
    "                price_target = current_price * 1.02\n",
    "                stop_loss = current_price * 0.95\n",
    "            \n",
    "            # Calculate percentages\n",
    "            upside_percent = ((price_target - current_price) / current_price) * 100\n",
    "            downside_percent = ((current_price - stop_loss) / current_price) * 100\n",
    "            \n",
    "            # Risk-reward ratio\n",
    "            risk_reward_ratio = abs(upside_percent / downside_percent) if downside_percent != 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'target': round(price_target, 2),\n",
    "                'stop_loss': round(stop_loss, 2),\n",
    "                'upside_percent': round(upside_percent, 2),\n",
    "                'downside_percent': round(downside_percent, 2),\n",
    "                'risk_reward_ratio': round(risk_reward_ratio, 2)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating price targets: {e}\")\n",
    "            return {\n",
    "                'target': state.get('current_price', 100) * 1.05,\n",
    "                'stop_loss': state.get('current_price', 100) * 0.95,\n",
    "                'upside_percent': 5.0,\n",
    "                'downside_percent': 5.0,\n",
    "                'risk_reward_ratio': 1.0\n",
    "            }\n",
    "\n",
    "# Cell 29: Rationale generation and time horizon\n",
    "    def _generate_rationale(self, recommendation: str, sentiment_data: Dict[str, Any],\n",
    "                          technical_data: Dict[str, Any], risk_data: Dict[str, Any],\n",
    "                          fundamental_data: Dict[str, Any], confidence: float) -> str:\n",
    "        \"\"\"Generate detailed rationale for the recommendation\"\"\"\n",
    "        try:\n",
    "            rationale_parts = []\n",
    "            \n",
    "            # Opening statement\n",
    "            rationale_parts.append(f\"Recommendation: {recommendation} (Confidence: {confidence:.0f}%)\")\n",
    "            \n",
    "            # Sentiment analysis\n",
    "            overall_sentiment = sentiment_data.get('overall_sentiment', 0)\n",
    "            if abs(overall_sentiment) > 0.2:\n",
    "                sentiment_desc = \"positive\" if overall_sentiment > 0 else \"negative\"\n",
    "                rationale_parts.append(f\"Market sentiment is {sentiment_desc} ({overall_sentiment:.2f}), indicating {sentiment_desc} investor perception.\")\n",
    "            \n",
    "            # Technical analysis\n",
    "            trend = technical_data.get('trend_direction', 'Neutral')\n",
    "            if trend != 'Neutral':\n",
    "                rationale_parts.append(f\"Technical analysis shows {trend.lower()} trend with supporting indicators.\")\n",
    "            \n",
    "            patterns = technical_data.get('chart_patterns', [])\n",
    "            if patterns:\n",
    "                rationale_parts.append(f\"Identified chart patterns: {', '.join(patterns)}.\")\n",
    "            \n",
    "            # Risk assessment\n",
    "            risk_score = risk_data.get('risk_score', 0.5)\n",
    "            risk_level = \"high\" if risk_score > 0.6 else \"moderate\" if risk_score > 0.3 else \"low\"\n",
    "            rationale_parts.append(f\"Risk assessment indicates {risk_level} risk profile (score: {risk_score:.2f}).\")\n",
    "            \n",
    "            # Fundamental factors\n",
    "            ratios = fundamental_data.get('financial_data', {}).get('financial_ratios', {})\n",
    "            if ratios:\n",
    "                pe_ratio = ratios.get('pe_ratio', 0)\n",
    "                if pe_ratio > 0:\n",
    "                    rationale_parts.append(f\"P/E ratio of {pe_ratio:.1f} suggests {'expensive' if pe_ratio > 25 else 'reasonable' if pe_ratio > 15 else 'attractive'} valuation.\")\n",
    "            \n",
    "            # Risk-reward consideration\n",
    "            if recommendation in ['STRONG BUY', 'BUY']:\n",
    "                rationale_parts.append(\"The positive factors outweigh the risks, presenting a favorable risk-adjusted opportunity.\")\n",
    "            elif recommendation in ['SELL', 'STRONG SELL']:\n",
    "                rationale_parts.append(\"Risk factors and negative indicators suggest limited upside potential and increased downside risk.\")\n",
    "            else:\n",
    "                rationale_parts.append(\"Mixed signals suggest a cautious approach with careful monitoring of key indicators.\")\n",
    "            \n",
    "            return \" \".join(rationale_parts)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating rationale: {e}\")\n",
    "            return f\"Recommendation: {recommendation} based on comprehensive analysis of available data.\"\n",
    "    \n",
    "    def _determine_time_horizon(self, technical_data: Dict[str, Any], recommendation: str) -> str:\n",
    "        \"\"\"Determine recommended time horizon for the investment\"\"\"\n",
    "        try:\n",
    "            trend = technical_data.get('trend_direction', 'Neutral')\n",
    "            patterns = technical_data.get('chart_patterns', [])\n",
    "            \n",
    "            # Strong trends suggest longer holding periods\n",
    "            if recommendation in ['STRONG BUY', 'STRONG SELL']:\n",
    "                if trend in ['Upward', 'Downward']:\n",
    "                    return 'Medium to Long term (3-12 months)'\n",
    "                else:\n",
    "                    return 'Short to Medium term (1-6 months)'\n",
    "            elif recommendation in ['BUY', 'SELL']:\n",
    "                return 'Short to Medium term (1-6 months)'\n",
    "            else:\n",
    "                return 'Short term (1-3 months)'\n",
    "        except:\n",
    "            return 'Medium term (3-6 months)'\n",
    "    \n",
    "    def _default_decision(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return default decision in case of errors\"\"\"\n",
    "        return {\n",
    "            'recommendation': 'HOLD',\n",
    "            'confidence_score': 50.0,\n",
    "            'weighted_score': 0.0,\n",
    "            'component_scores': {},\n",
    "            'price_target': 0.0,\n",
    "            'stop_loss': 0.0,\n",
    "            'upside_potential': 0.0,\n",
    "            'downside_risk': 0.0,\n",
    "            'risk_reward_ratio': 1.0,\n",
    "            'rationale': 'Unable to complete analysis due to insufficient data.',\n",
    "            'time_horizon': 'Medium term'\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 30-34: Set up LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30: LangGraph workflow setup\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import Dict, Any\n",
    "\n",
    "def setup_stock_analysis_workflow(api_config: APIConfig, analysis_config: AnalysisConfig) -> StateGraph:\n",
    "    \"\"\"Set up the LangGraph workflow for stock analysis\"\"\"\n",
    "    \n",
    "    # Initialize agents\n",
    "    data_agent = DataAnalysisAgent(api_config)\n",
    "    sentiment_agent = SentimentAnalysisAgent(api_config)\n",
    "    technical_agent = TechnicalAnalysisAgent(analysis_config)\n",
    "    risk_agent = RiskAssessmentAgent(analysis_config)\n",
    "    decision_agent = DecisionMakingAgent(analysis_config)\n",
    "    \n",
    "    # Define the workflow graph\n",
    "    workflow = StateGraph(StockAnalysisState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"data_analysis\", create_data_analysis_node(data_agent))\n",
    "    workflow.add_node(\"sentiment_analysis\", create_sentiment_analysis_node(sentiment_agent))\n",
    "    workflow.add_node(\"technical_analysis\", create_technical_analysis_node(technical_agent))\n",
    "    workflow.add_node(\"risk_assessment\", create_risk_assessment_node(risk_agent))\n",
    "    workflow.add_node(\"decision_making\", create_decision_making_node(decision_agent))\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"data_analysis\")\n",
    "    \n",
    "    # Add edges (data flow)\n",
    "    workflow.add_edge(\"data_analysis\", \"sentiment_analysis\")\n",
    "    workflow.add_edge(\"data_analysis\", \"technical_analysis\")\n",
    "    workflow.add_edge(\"sentiment_analysis\", \"risk_assessment\")\n",
    "    workflow.add_edge(\"technical_analysis\", \"risk_assessment\")\n",
    "    workflow.add_edge(\"risk_assessment\", \"decision_making\")\n",
    "    workflow.add_edge(\"decision_making\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Cell 31: Node creation functions\n",
    "def create_data_analysis_node(agent: DataAnalysisAgent):\n",
    "    \"\"\"Create data analysis node\"\"\"\n",
    "    def data_analysis_node(state: StockAnalysisState) -> StockAnalysisState:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            symbol = state[\"symbol\"]\n",
    "            \n",
    "            # Fetch stock data\n",
    "            stock_data = agent.fetch_stock_data(symbol)\n",
    "            if not stock_data:\n",
    "                state[\"errors\"].append(\"Failed to fetch stock data\")\n",
    "                return state\n",
    "            \n",
    "            # Update state with basic stock info\n",
    "            state.update({\n",
    "                \"company_name\": stock_data.get(\"company_name\", symbol),\n",
    "                \"current_price\": stock_data.get(\"current_price\", 0),\n",
    "                \"price_change\": stock_data.get(\"price_change\", 0),\n",
    "                \"price_change_percent\": stock_data.get(\"price_change_percent\", 0),\n",
    "                \"volume\": stock_data.get(\"volume\", 0),\n",
    "                \"market_cap\": stock_data.get(\"market_cap\", 0),\n",
    "                \"historical_data\": stock_data.get(\"historical_data\", pd.DataFrame())\n",
    "            })\n",
    "            \n",
    "            # Get financial data\n",
    "            financial_data = agent.get_financial_data(symbol)\n",
    "            state[\"financial_data\"] = financial_data\n",
    "            \n",
    "            # Get market context\n",
    "            market_context = agent.get_market_context(symbol)\n",
    "            state.setdefault(\"market_context\", {}).update(market_context)\n",
    "            \n",
    "            # Get economic indicators\n",
    "            economic_data = agent.get_economic_indicators()\n",
    "            state.setdefault(\"economic_data\", {}).update(economic_data)\n",
    "            \n",
    "            # Record execution time\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            state[\"execution_time\"][\"data_analysis\"] = execution_time\n",
    "            \n",
    "            logger.info(f\"Data analysis completed for {symbol} in {execution_time:.2f}s\")\n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Data analysis error: {str(e)}\"\n",
    "            state[\"errors\"].append(error_msg)\n",
    "            logger.error(error_msg)\n",
    "            return state\n",
    "    \n",
    "    return data_analysis_node\n",
    "\n",
    "\n",
    "def create_sentiment_analysis_node(agent: SentimentAnalysisAgent):\n",
    "    \"\"\"Create sentiment analysis node\"\"\"\n",
    "    def sentiment_analysis_node(state: StockAnalysisState) -> StockAnalysisState:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            symbol = state[\"symbol\"]\n",
    "            \n",
    "            # Analyze news sentiment\n",
    "            news_analysis = agent.analyze_news_sentiment(symbol)\n",
    "            state[\"news_sentiment\"] = news_analysis.get(\"sentiment_score\", 0.0)\n",
    "            \n",
    "            # Analyze social media sentiment\n",
    "            social_analysis = agent.analyze_social_sentiment(symbol)\n",
    "            state[\"social_sentiment\"] = social_analysis.get(\"sentiment_score\", 0.0)\n",
    "            \n",
    "            # Get analyst sentiment\n",
    "            analyst_analysis = agent.get_analyst_sentiment(symbol)\n",
    "            state[\"analyst_consensus\"] = analyst_analysis.get(\"sentiment_score\", 0.0)\n",
    "            \n",
    "            # Calculate overall sentiment\n",
    "            overall_analysis = agent.calculate_overall_sentiment(\n",
    "                state[\"news_sentiment\"],\n",
    "                state[\"social_sentiment\"],\n",
    "                state[\"analyst_consensus\"]\n",
    "            )\n",
    "            state[\"overall_sentiment\"] = overall_analysis.get(\"overall_sentiment\", 0.0)\n",
    "            \n",
    "            # Store detailed sentiment data\n",
    "            state[\"sentiment_details\"] = {\n",
    "                \"news_analysis\": news_analysis,\n",
    "                \"social_analysis\": social_analysis,\n",
    "                \"analyst_analysis\": analyst_analysis,\n",
    "                \"overall_analysis\": overall_analysis\n",
    "            }\n",
    "            \n",
    "            # Record execution time\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            state[\"execution_time\"][\"sentiment_analysis\"] = execution_time\n",
    "            \n",
    "            logger.info(f\"Sentiment analysis completed for {symbol} in {execution_time:.2f}s\")\n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Sentiment analysis error: {str(e)}\"\n",
    "            state[\"errors\"].append(error_msg)\n",
    "            logger.error(error_msg)\n",
    "            return state\n",
    "    \n",
    "    return sentiment_analysis_node\n",
    "\n",
    "\n",
    "def create_technical_analysis_node(agent: TechnicalAnalysisAgent):\n",
    "    \"\"\"Create technical analysis node\"\"\"\n",
    "    def technical_analysis_node(state: StockAnalysisState) -> StockAnalysisState:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            historical_data = state.get(\"historical_data\", pd.DataFrame())\n",
    "            \n",
    "            if historical_data.empty:\n",
    "                state[\"warnings\"].append(\"No historical data available for technical analysis\")\n",
    "                return state\n",
    "            \n",
    "            # Calculate technical indicators\n",
    "            indicators = agent.calculate_all_indicators(historical_data)\n",
    "            state[\"technical_indicators\"] = indicators\n",
    "            \n",
    "            # Identify support and resistance levels\n",
    "            support_resistance = agent.identify_support_resistance(historical_data)\n",
    "            state[\"support_resistance\"] = support_resistance\n",
    "            \n",
    "            # Analyze trends and patterns\n",
    "            trend_analysis = agent.analyze_trend_and_patterns(historical_data)\n",
    "            state[\"trend_direction\"] = trend_analysis.get(\"overall_trend\", \"Neutral\")\n",
    "            state[\"chart_patterns\"] = trend_analysis.get(\"patterns\", [])\n",
    "            \n",
    "            # Store complete technical analysis\n",
    "            state.setdefault(\"technical_analysis_details\", {}).update({\n",
    "                \"indicators\": indicators,\n",
    "                \"support_resistance\": support_resistance,\n",
    "                \"trend_analysis\": trend_analysis\n",
    "            })\n",
    "            \n",
    "            # Record execution time\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            state[\"execution_time\"][\"technical_analysis\"] = execution_time\n",
    "            \n",
    "            logger.info(f\"Technical analysis completed in {execution_time:.2f}s\")\n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Technical analysis error: {str(e)}\"\n",
    "            state[\"errors\"].append(error_msg)\n",
    "            logger.error(error_msg)\n",
    "            return state\n",
    "    \n",
    "    return technical_analysis_node\n",
    "\n",
    "\n",
    "def create_risk_assessment_node(agent: RiskAssessmentAgent):\n",
    "    \"\"\"Create risk assessment node\"\"\"\n",
    "    def risk_assessment_node(state: StockAnalysisState) -> StockAnalysisState:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            symbol = state[\"symbol\"]\n",
    "            historical_data = state.get(\"historical_data\", pd.DataFrame())\n",
    "            \n",
    "            if historical_data.empty:\n",
    "                state[\"warnings\"].append(\"No historical data available for risk assessment\")\n",
    "                return state\n",
    "            \n",
    "            # Calculate comprehensive risk metrics\n",
    "            market_data = {\n",
    "                \"market_cap\": state.get(\"market_cap\", 0),\n",
    "                \"financial_data\": state.get(\"financial_data\", {})\n",
    "            }\n",
    "            \n",
    "            risk_metrics = agent.calculate_comprehensive_risk(historical_data, symbol, market_data)\n",
    "            \n",
    "            # Update state with risk metrics\n",
    "            state[\"risk_score\"] = risk_metrics.get(\"overall_risk_score\", {}).get(\"overall_risk_score\", 0.5)\n",
    "            state[\"volatility\"] = risk_metrics.get(\"annual_volatility\", 0.2)\n",
    "            state[\"beta\"] = risk_metrics.get(\"beta\", 1.0)\n",
    "            state[\"var_score\"] = risk_metrics.get(\"var_historical_95\", -0.02)\n",
    "            state[\"risk_metrics\"] = risk_metrics\n",
    "            \n",
    "            # Record execution time\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            state[\"execution_time\"][\"risk_assessment\"] = execution_time\n",
    "            \n",
    "            logger.info(f\"Risk assessment completed for {symbol} in {execution_time:.2f}s\")\n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Risk assessment error: {str(e)}\"\n",
    "            state[\"errors\"].append(error_msg)\n",
    "            logger.error(error_msg)\n",
    "            return state\n",
    "    \n",
    "    return risk_assessment_node\n",
    "\n",
    "\n",
    "def create_decision_making_node(agent: DecisionMakingAgent):\n",
    "    \"\"\"Create decision making node\"\"\"\n",
    "    def decision_making_node(state: StockAnalysisState) -> StockAnalysisState:\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Make investment decision based on all analyses\n",
    "            decision = agent.make_investment_decision(state)\n",
    "            \n",
    "            # Update state with final recommendation\n",
    "            state.update({\n",
    "                \"recommendation\": decision.get(\"recommendation\", \"HOLD\"),\n",
    "                \"confidence_score\": decision.get(\"confidence_score\", 50.0),\n",
    "                \"price_target\": decision.get(\"price_target\", 0.0),\n",
    "                \"stop_loss\": decision.get(\"stop_loss\", 0.0),\n",
    "                \"rationale\": decision.get(\"rationale\", \"Analysis completed\")\n",
    "            })\n",
    "            \n",
    "            # Store complete decision analysis\n",
    "            state.setdefault(\"decision_details\", {}).update(decision)\n",
    "            \n",
    "            # Record execution time\n",
    "            execution_time = (datetime.now() - start_time).total_seconds()\n",
    "            state[\"execution_time\"][\"decision_making\"] = execution_time\n",
    "            \n",
    "            # Calculate total execution time\n",
    "            total_time = sum(state[\"execution_time\"].values())\n",
    "            state[\"execution_time\"][\"total\"] = total_time\n",
    "            \n",
    "            logger.info(f\"Decision making completed in {execution_time:.2f}s (Total: {total_time:.2f}s)\")\n",
    "            return state\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Decision making error: {str(e)}\"\n",
    "            state[\"errors\"].append(error_msg)\n",
    "            logger.error(error_msg)\n",
    "            return state\n",
    "    \n",
    "    return decision_making_node\n",
    "\n",
    "\n",
    "class StockAnalysisWorkflow:\n",
    "    \"\"\"Main workflow orchestrator for stock analysis\"\"\"\n",
    "    \n",
    "    def __init__(self, api_config: APIConfig, analysis_config: AnalysisConfig):\n",
    "        self.api_config = api_config\n",
    "        self.analysis_config = analysis_config\n",
    "        self.workflow = setup_stock_analysis_workflow(api_config, analysis_config)\n",
    "    \n",
    "    def analyze_stock(self, symbol: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze a single stock\"\"\"\n",
    "        try:\n",
    "            # Initialize state\n",
    "            initial_state = StockAnalysisState(\n",
    "                symbol=symbol.upper(),\n",
    "                company_name=\"\",\n",
    "                analysis_date=datetime.now(),\n",
    "                current_price=0.0,\n",
    "                price_change=0.0,\n",
    "                price_change_percent=0.0,\n",
    "                volume=0,\n",
    "                market_cap=0.0,\n",
    "                historical_data=pd.DataFrame(),\n",
    "                financial_data={},\n",
    "                news_sentiment=0.0,\n",
    "                social_sentiment=0.0,\n",
    "                analyst_consensus=0.0,\n",
    "                overall_sentiment=0.0,\n",
    "                sentiment_details={},\n",
    "                technical_indicators={},\n",
    "                support_resistance={},\n",
    "                trend_direction=\"Neutral\",\n",
    "                chart_patterns=[],\n",
    "                risk_score=0.5,\n",
    "                volatility=0.2,\n",
    "                beta=1.0,\n",
    "                var_score=-0.02,\n",
    "                risk_metrics={},\n",
    "                recommendation=\"HOLD\",\n",
    "                confidence_score=50.0,\n",
    "                price_target=0.0,\n",
    "                stop_loss=0.0,\n",
    "                rationale=\"\",\n",
    "                errors=[],\n",
    "                warnings=[],\n",
    "                execution_time={}\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"Starting analysis for {symbol}\")\n",
    "            \n",
    "            # Execute workflow\n",
    "            result = self.workflow.invoke(initial_state)\n",
    "            \n",
    "            # Log completion\n",
    "            total_time = result.get(\"execution_time\", {}).get(\"total\", 0)\n",
    "            logger.info(f\"Analysis completed for {symbol} in {total_time:.2f}s\")\n",
    "            \n",
    "            # Log any errors or warnings\n",
    "            if result.get(\"errors\"):\n",
    "                logger.warning(f\"Errors during analysis: {result['errors']}\")\n",
    "            if result.get(\"warnings\"):\n",
    "                logger.info(f\"Warnings during analysis: {result['warnings']}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Workflow execution error for {symbol}: {e}\")\n",
    "            return {\n",
    "                \"symbol\": symbol,\n",
    "                \"recommendation\": \"HOLD\",\n",
    "                \"confidence_score\": 0.0,\n",
    "                \"rationale\": f\"Analysis failed: {str(e)}\",\n",
    "                \"errors\": [str(e)]\n",
    "            }\n",
    "    \n",
    "    def analyze_multiple_stocks(self, symbols: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze multiple stocks\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            try:\n",
    "                logger.info(f\"Analyzing {symbol}...\")\n",
    "                result = self.analyze_stock(symbol)\n",
    "                results[symbol] = result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to analyze {symbol}: {e}\")\n",
    "                results[symbol] = {\n",
    "                    \"symbol\": symbol,\n",
    "                    \"recommendation\": \"HOLD\",\n",
    "                    \"confidence_score\": 0.0,\n",
    "                    \"rationale\": f\"Analysis failed: {str(e)}\",\n",
    "                    \"errors\": [str(e)]\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_workflow_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get workflow configuration status\"\"\"\n",
    "        return {\n",
    "            \"api_config_set\": bool(self.api_config.alpha_vantage_key or self.api_config.news_api_key),\n",
    "            \"workflow_ready\": self.workflow is not None,\n",
    "            \"supported_analysis\": [\n",
    "                \"data_analysis\",\n",
    "                \"sentiment_analysis\", \n",
    "                \"technical_analysis\",\n",
    "                \"risk_assessment\",\n",
    "                \"decision_making\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Cell 34: Utility functions for workflow management\n",
    "def create_default_configs() -> tuple[APIConfig, AnalysisConfig]:\n",
    "    \"\"\"Create default configurations\"\"\"\n",
    "    api_config = APIConfig(\n",
    "        alpha_vantage_key=os.getenv(\"ALPHA_VANTAGE_API_KEY\", \"\"),\n",
    "        news_api_key=os.getenv(\"NEWS_API_KEY\", \"\"),\n",
    "        fred_api_key=os.getenv(\"FRED_API_KEY\", \"\"),\n",
    "        twitter_bearer_token=os.getenv(\"TWITTER_BEARER_TOKEN\", \"\"),\n",
    "        reddit_client_id=os.getenv(\"REDDIT_CLIENT_ID\", \"\"),\n",
    "        reddit_client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\", \"\"),\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "    )\n",
    "    \n",
    "    analysis_config = AnalysisConfig(\n",
    "        lookback_days=252,\n",
    "        short_ma_period=20,\n",
    "        long_ma_period=50,\n",
    "        rsi_period=14,\n",
    "        bollinger_period=20,\n",
    "        confidence_threshold=0.6,\n",
    "        risk_free_rate=0.02\n",
    "    )\n",
    "    \n",
    "    return api_config, analysis_config\n",
    "\n",
    "def validate_configuration(api_config: APIConfig) -> Dict[str, bool]:\n",
    "    \"\"\"Validate API configuration\"\"\"\n",
    "    validation_results = {\n",
    "        \"yfinance_available\": True,  # Always available\n",
    "        \"alpha_vantage_configured\": bool(api_config.alpha_vantage_key),\n",
    "        \"news_api_configured\": bool(api_config.news_api_key),\n",
    "        \"fred_api_configured\": bool(api_config.fred_api_key),\n",
    "        \"twitter_api_configured\": bool(api_config.twitter_bearer_token),\n",
    "        \"reddit_api_configured\": bool(api_config.reddit_client_id and api_config.reddit_client_secret),\n",
    "        \"openai_configured\": bool(api_config.openai_api_key)\n",
    "    }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def setup_logging_config(log_level: str = \"INFO\") -> None:\n",
    "    \"\"\"Setup logging configuration\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=getattr(logging, log_level.upper()),\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.StreamHandler(),\n",
    "            logging.FileHandler('stock_analysis.log')\n",
    "        ]\n",
    "    )\n",
    "\n",
    "async def async_analyze_stocks(workflow: StockAnalysisWorkflow, symbols: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Asynchronously analyze multiple stocks (for future enhancement)\"\"\"\n",
    "    # This is a placeholder for future async implementation\n",
    "    # Currently, we'll run synchronously\n",
    "    return workflow.analyze_multiple_stocks(symbols)\n",
    "\n",
    "def create_workflow_visualization() -> str:\n",
    "    \"\"\"Create a text-based visualization of the workflow\"\"\"\n",
    "    return \"\"\"\n",
    "    Stock Analysis Workflow:\n",
    "    \n",
    "    [Input: Stock Symbol]\n",
    "            |\n",
    "    [Data Analysis Agent] ──────┐\n",
    "            |                   |\n",
    "            v                   v\n",
    "    [Sentiment Analysis]   [Technical Analysis]\n",
    "            |                   |\n",
    "            └─────┬─────────────┘\n",
    "                  v\n",
    "        [Risk Assessment Agent]\n",
    "                  |\n",
    "                  v\n",
    "        [Decision Making Agent]\n",
    "                  |\n",
    "                  v\n",
    "        [Output: Recommendation]\n",
    "    \n",
    "    Parallel Processing:\n",
    "    - Sentiment and Technical analysis run in parallel\n",
    "    - Risk assessment waits for both to complete\n",
    "    - Decision making synthesizes all results\n",
    "    \"\"\"\n",
    "\n",
    "# Example usage function\n",
    "def demo_workflow_setup():\n",
    "    \"\"\"Demonstrate workflow setup\"\"\"\n",
    "    print(\"Setting up Stock Analysis Workflow...\")\n",
    "    \n",
    "    # Create configurations\n",
    "    api_config, analysis_config = create_default_configs()\n",
    "    \n",
    "    # Validate configuration\n",
    "    validation = validate_configuration(api_config)\n",
    "    print(\"Configuration Validation:\")\n",
    "    for service, status in validation.items():\n",
    "        print(f\"  {service}: {'✓' if status else '✗'}\")\n",
    "    \n",
    "    # Setup workflow\n",
    "    try:\n",
    "        workflow = StockAnalysisWorkflow(api_config, analysis_config)\n",
    "        print(\"✓ Workflow setup successful\")\n",
    "        \n",
    "        # Show workflow status\n",
    "        status = workflow.get_workflow_status()\n",
    "        print(f\"Workflow ready: {status['workflow_ready']}\")\n",
    "        print(f\"Supported analyses: {', '.join(status['supported_analysis'])}\")\n",
    "        \n",
    "        return workflow\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Workflow setup failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Print workflow visualization\n",
    "print(create_workflow_visualization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
