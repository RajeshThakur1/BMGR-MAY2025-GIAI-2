{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bd42c25",
   "metadata": {},
   "source": [
    "# SQL Agent: Natural Language to SQL Query Generator\n",
    "\n",
    "This notebook will teach you step-by-step how to build a robust SQL agent that converts natural language queries into PostgreSQL queries.\n",
    "\n",
    "## What We'll Build:\n",
    "1. **Database Connection Setup** - Connect to PostgreSQL database\n",
    "2. **Schema Discovery** - Automatically discover and understand database structure\n",
    "3. **Query Context Builder** - Create context for better query generation\n",
    "4. **AI-Powered SQL Generation** - Use LLM to convert natural language to SQL\n",
    "5. **Query Validation** - Validate and optimize generated queries\n",
    "6. **Interactive Interface** - Create a user-friendly interface\n",
    "\n",
    "## Database Information:\n",
    "- **Hostname**: hh-pgsql-public.ebi.ac.uk\n",
    "- **Port**: 5432\n",
    "- **Database**: pfmegrnargs\n",
    "- **User**: reader\n",
    "- **Password**: NWDMCE5xdipIjRrp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7ac6c",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Dependencies\n",
    "\n",
    "First, let's install all the necessary packages for our SQL agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f48371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (2.9.10)\n",
      "Requirement already satisfied: pandas in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: sqlalchemy in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (2.0.38)\n",
      "Requirement already satisfied: openai in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (1.66.3)\n",
      "Requirement already satisfied: anthropic in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (0.58.2)\n",
      "Requirement already satisfied: python-dotenv in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (0.3.20)\n",
      "Requirement already satisfied: langchain-openai in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain-anthropic in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (0.3.17)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from sqlalchemy) (4.13.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain) (0.3.69)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain-openai) (0.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (1.33)\n",
      "Requirement already satisfied: packaging>=23.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.41->langchain) (24.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain) (3.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipywidgets in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipywidgets) (8.32.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /Users/rajeshthakur/miniconda3/envs/ai_experiments/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install psycopg2-binary pandas sqlalchemy openai anthropic python-dotenv langchain langchain-openai langchain-anthropic\n",
    "\n",
    "# Alternative installations if needed\n",
    "# %pip install psycopg2  # for Windows\n",
    "%pip install ipywidgets  # For interactive widgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8769c26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cf8bcc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de0a26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database libraries\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text, inspect\n",
    "\n",
    "# AI libraries\n",
    "import openai\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "# Utility libraries\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956f10b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to PostgreSQL!\n",
      "üìä Database: simplilearn\n",
      "üîß Version: PostgreSQL 17.5 (Debian 17.5-1.pgdg120+1) on aarch...\n"
     ]
    }
   ],
   "source": [
    "class PostgreSQLConnection:\n",
    "    \"\"\"\n",
    "    A robust PostgreSQL connection handler with error handling and connection management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, host, port, database, user, password):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.database = database\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.connection = None\n",
    "        self.engine = None\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to PostgreSQL database\"\"\"\n",
    "        try:\n",
    "            # Create connection string\n",
    "            connection_string = f\"postgresql://{self.user}:{self.password}@{self.host}:{self.port}/{self.database}\"\n",
    "            \n",
    "            # Create SQLAlchemy engine\n",
    "            self.engine = create_engine(connection_string)\n",
    "            \n",
    "            # Test connection\n",
    "            with self.engine.connect() as conn:\n",
    "                result = conn.execute(text(\"SELECT version()\"))\n",
    "                version = result.fetchone()[0]\n",
    "                print(f\"‚úÖ Connected to PostgreSQL!\")\n",
    "                print(f\"üìä Database: {self.database}\")\n",
    "                print(f\"üîß Version: {version[:50]}...\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Connection failed: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def execute_query(self, query, return_df=True):\n",
    "        \"\"\"Execute a SQL query and return results\"\"\"\n",
    "        try:\n",
    "            if return_df:\n",
    "                df = pd.read_sql_query(query, self.engine)\n",
    "                return df\n",
    "            else:\n",
    "                with self.engine.connect() as conn:\n",
    "                    result = conn.execute(text(query))\n",
    "                    return result.fetchall()\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query execution failed: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def get_table_info(self):\n",
    "        \"\"\"Get information about all tables in the database\"\"\"\n",
    "        try:\n",
    "            inspector = inspect(self.engine)\n",
    "            tables_info = {}\n",
    "            \n",
    "            for table_name in inspector.get_table_names():\n",
    "                columns = inspector.get_columns(table_name)\n",
    "                tables_info[table_name] = {\n",
    "                    'columns': [col['name'] for col in columns],\n",
    "                    'column_details': columns\n",
    "                }\n",
    "                \n",
    "            return tables_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to get table info: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "# Initialize database connection\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',  # localhost\n",
    "    'port': 5431,\n",
    "    'database': 'simplilearn',\n",
    "    'user': 'myuser',\n",
    "    'password': 'mypassword'\n",
    "}\n",
    "\n",
    "# Create database connection\n",
    "db = PostgreSQLConnection(**DB_CONFIG)\n",
    "connection_success = db.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b99db6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction_ID</th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Transaction_Amount</th>\n",
       "      <th>Merchant_Category</th>\n",
       "      <th>Transaction_Date</th>\n",
       "      <th>Transaction_Location</th>\n",
       "      <th>Payment_Method</th>\n",
       "      <th>Is_Fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>25795</td>\n",
       "      <td>1266.97</td>\n",
       "      <td>Food</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>Miami</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10860</td>\n",
       "      <td>2602.77</td>\n",
       "      <td>Retail</td>\n",
       "      <td>2023-01-01 01:00:00</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>86820</td>\n",
       "      <td>2612.16</td>\n",
       "      <td>Travel</td>\n",
       "      <td>2023-01-01 02:00:00</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>64886</td>\n",
       "      <td>2946.72</td>\n",
       "      <td>Travel</td>\n",
       "      <td>2023-01-01 03:00:00</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Crypto</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>16265</td>\n",
       "      <td>3759.62</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2023-01-01 04:00:00</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>59500</td>\n",
       "      <td>3796.35</td>\n",
       "      <td>Food</td>\n",
       "      <td>2023-07-28 03:00:00</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Debit Card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>22100</td>\n",
       "      <td>2559.85</td>\n",
       "      <td>Health</td>\n",
       "      <td>2023-07-28 04:00:00</td>\n",
       "      <td>New York</td>\n",
       "      <td>Crypto</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>67021</td>\n",
       "      <td>3221.91</td>\n",
       "      <td>Travel</td>\n",
       "      <td>2023-07-28 05:00:00</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>29800</td>\n",
       "      <td>515.61</td>\n",
       "      <td>Health</td>\n",
       "      <td>2023-07-28 06:00:00</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>73400</td>\n",
       "      <td>4223.39</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>2023-07-28 07:00:00</td>\n",
       "      <td>Miami</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Transaction_ID  Customer_ID  Transaction_Amount Merchant_Category  \\\n",
       "0                  1        25795             1266.97              Food   \n",
       "1                  2        10860             2602.77            Retail   \n",
       "2                  3        86820             2612.16            Travel   \n",
       "3                  4        64886             2946.72            Travel   \n",
       "4                  5        16265             3759.62       Electronics   \n",
       "...              ...          ...                 ...               ...   \n",
       "4995            4996        59500             3796.35              Food   \n",
       "4996            4997        22100             2559.85            Health   \n",
       "4997            4998        67021             3221.91            Travel   \n",
       "4998            4999        29800              515.61            Health   \n",
       "4999            5000        73400             4223.39       Electronics   \n",
       "\n",
       "         Transaction_Date Transaction_Location Payment_Method  Is_Fraud  \n",
       "0     2023-01-01 00:00:00                Miami     Debit Card         0  \n",
       "1     2023-01-01 01:00:00        San Francisco         PayPal         0  \n",
       "2     2023-01-01 02:00:00        San Francisco    Credit Card         0  \n",
       "3     2023-01-01 03:00:00          Los Angeles         Crypto         1  \n",
       "4     2023-01-01 04:00:00        San Francisco    Credit Card         1  \n",
       "...                   ...                  ...            ...       ...  \n",
       "4995  2023-07-28 03:00:00        San Francisco     Debit Card         0  \n",
       "4996  2023-07-28 04:00:00             New York         Crypto         0  \n",
       "4997  2023-07-28 05:00:00          Los Angeles    Credit Card         0  \n",
       "4998  2023-07-28 06:00:00        San Francisco    Credit Card         0  \n",
       "4999  2023-07-28 07:00:00                Miami    Credit Card         0  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.execute_query(\"select * from transections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdfe285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abailable Tablees\n",
      "==================================================\n",
      "  1. transections (BASE TABLE)\n",
      "\n",
      "üî¢ Total tables found: 1\n"
     ]
    }
   ],
   "source": [
    "if connection_success:\n",
    "     tables_query = \"\"\"\n",
    "    SELECT \n",
    "        table_name,\n",
    "        table_schema,\n",
    "        table_type\n",
    "    FROM information_schema.tables \n",
    "    WHERE table_schema = 'public'\n",
    "    ORDER BY table_name;\n",
    "    \"\"\"\n",
    "     tables_df = db.execute_query(tables_query)\n",
    "     print(\"Abailable Tablees\")\n",
    "     print(\"=\"*50)\n",
    "     for idx, row in tables_df.iterrows():\n",
    "        print(f\"  {idx+1}. {row['table_name']} ({row['table_type']})\")\n",
    "     print(f\"\\nüî¢ Total tables found: {len(tables_df)}\")\n",
    "\n",
    "\n",
    "else:\n",
    "     print(\"trouble to connect with database\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2cbe847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Table: transections\n",
      "============================================================\n",
      "üìä Column Structure:\n",
      "  ‚Ä¢ Transaction_ID: bigint (NULL)\n",
      "  ‚Ä¢ Customer_ID: bigint (NULL)\n",
      "  ‚Ä¢ Transaction_Amount: double precision (NULL)\n",
      "  ‚Ä¢ Merchant_Category: text (NULL)\n",
      "  ‚Ä¢ Transaction_Date: text (NULL)\n",
      "  ‚Ä¢ Transaction_Location: text (NULL)\n",
      "  ‚Ä¢ Payment_Method: text (NULL)\n",
      "  ‚Ä¢ Is_Fraud: bigint (NULL)\n",
      "\n",
      "üìù Sample Data (first 5 rows):\n",
      "   Transaction_ID  Customer_ID  Transaction_Amount Merchant_Category     Transaction_Date Transaction_Location Payment_Method  Is_Fraud\n",
      "0               1        25795             1266.97              Food  2023-01-01 00:00:00                Miami     Debit Card         0\n",
      "1               2        10860             2602.77            Retail  2023-01-01 01:00:00        San Francisco         PayPal         0\n",
      "2               3        86820             2612.16            Travel  2023-01-01 02:00:00        San Francisco    Credit Card         0\n",
      "3               4        64886             2946.72            Travel  2023-01-01 03:00:00          Los Angeles         Crypto         1\n",
      "4               5        16265             3759.62       Electronics  2023-01-01 04:00:00        San Francisco    Credit Card         1\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def explore_table_structure(table_name, limit=5):\n",
    "    columns_query = f\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable,\n",
    "        column_default\n",
    "    FROM information_schema.columns \n",
    "    WHERE table_name = '{table_name}'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\"\n",
    "    columns_df = db.execute_query(columns_query)\n",
    "    print(f\"üîç Table: {table_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä Column Structure:\")\n",
    "    for idx, row in columns_df.iterrows():\n",
    "        nullable = \"NULL\" if row['is_nullable'] == 'YES' else \"NOT NULL\"\n",
    "        default = f\" DEFAULT {row['column_default']}\" if row['column_default'] else \"\"\n",
    "        print(f\"  ‚Ä¢ {row['column_name']}: {row['data_type']} ({nullable}){default}\")\n",
    "    # Get sample data\n",
    "    sample_query = f\"SELECT * FROM {table_name} LIMIT {limit};\"\n",
    "    sample_df = db.execute_query(sample_query)\n",
    "    \n",
    "    print(f\"\\nüìù Sample Data (first {limit} rows):\")\n",
    "    if sample_df is not None and not sample_df.empty:\n",
    "        print(sample_df.to_string())\n",
    "    else:\n",
    "        print(\"  No data found or query failed\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    return columns_df, sample_df\n",
    "\n",
    "\n",
    "if connection_success and not tables_df.empty:\n",
    "    # Take first few tables to explore\n",
    "    tables_to_explore = tables_df['table_name'].head(3).tolist()\n",
    "    \n",
    "    for table in tables_to_explore:\n",
    "        try:\n",
    "            explore_table_structure(table)\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error exploring {table}: {str(e)}\")\n",
    "            print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "450e3f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Schema context builder initialized!\n",
      "üìä Cached schema for 1 tables\n"
     ]
    }
   ],
   "source": [
    "class SchemaContextBuilder:\n",
    "    \"\"\"\n",
    "    Builds context about database schema for AI models to generate accurate SQL queries\n",
    "    \"\"\"\n",
    "    def __init__(self, db_connection):\n",
    "        self.db = db_connection\n",
    "        self.schema_cache = {}\n",
    "        self.build_full_schema_context()\n",
    "\n",
    "    def build_full_schema_context(self):\n",
    "        \"\"\"Build complete schema context for all tables\"\"\"\n",
    "        \n",
    "        # Get all tables\n",
    "        tables_query = \"\"\"\n",
    "        SELECT table_name, table_schema \n",
    "        FROM information_schema.tables \n",
    "        WHERE table_schema = 'public'\n",
    "        ORDER BY table_name;\n",
    "        \"\"\"\n",
    "        tables_df = self.db.execute_query(tables_query)\n",
    "        if tables_df is None:\n",
    "            return\n",
    "        for _, row in tables_df.iterrows():\n",
    "            table_name = row['table_name']\n",
    "            self.schema_cache[table_name] = self.get_table_schema(table_name)\n",
    "\n",
    "    def get_table_schema(self, table_name):\n",
    "        \"\"\"Get detailed schema for a specific table\"\"\"\n",
    "\n",
    "        columns_query = f\"\"\"\n",
    "        SELECT \n",
    "            column_name,\n",
    "            data_type,\n",
    "            is_nullable,\n",
    "            column_default,\n",
    "            character_maximum_length\n",
    "        FROM information_schema.columns \n",
    "        WHERE table_name = '{table_name}'\n",
    "        ORDER BY ordinal_position;\n",
    "        \"\"\"\n",
    "        columns_df = self.db.execute_query(columns_query)\n",
    "\n",
    "        if columns_df is None:\n",
    "            return None\n",
    "        \n",
    "        # Get foreign key relationships\n",
    "        fk_query = f\"\"\"\n",
    "        SELECT\n",
    "            kcu.column_name,\n",
    "            ccu.table_name AS foreign_table_name,\n",
    "            ccu.column_name AS foreign_column_name\n",
    "        FROM information_schema.table_constraints AS tc\n",
    "        JOIN information_schema.key_column_usage AS kcu\n",
    "            ON tc.constraint_name = kcu.constraint_name\n",
    "        JOIN information_schema.constraint_column_usage AS ccu\n",
    "            ON ccu.constraint_name = tc.constraint_name\n",
    "        WHERE tc.constraint_type = 'FOREIGN KEY'\n",
    "            AND tc.table_name = '{table_name}';\n",
    "        \"\"\"\n",
    "\n",
    "        fk_df = self.db.execute_query(fk_query)\n",
    "\n",
    "        # Build schema info\n",
    "        schema_info = {\n",
    "            'table_name': table_name,\n",
    "            'columns': [],\n",
    "            'foreign_keys': []\n",
    "        }\n",
    "\n",
    "        for _, col in columns_df.iterrows():\n",
    "            col_info = {\n",
    "                'name': col['column_name'],\n",
    "                'type': col['data_type'],\n",
    "                'nullable': col['is_nullable'] == 'YES',\n",
    "                'default': col['column_default'],\n",
    "                'max_length': col['character_maximum_length']\n",
    "            }\n",
    "            schema_info['columns'].append(col_info)\n",
    "\n",
    "        if fk_df is not None and not fk_df.empty:\n",
    "            for _, fk in fk_df.iterrows():\n",
    "                fk_info = {\n",
    "                    'column': fk['column_name'],\n",
    "                    'references_table': fk['foreign_table_name'],\n",
    "                    'references_column': fk['foreign_column_name']\n",
    "                }\n",
    "                schema_info['foreign_keys'].append(fk_info)\n",
    "        \n",
    "        return schema_info\n",
    "    \n",
    "\n",
    "    def get_relevant_tables(self, query_text):\n",
    "        \"\"\"Identify tables that might be relevant to the query\"\"\"\n",
    "        query_lower = query_text.lower()\n",
    "        relevant_tables = []\n",
    "        \n",
    "        for table_name in self.schema_cache.keys():\n",
    "            # Check if table name appears in query\n",
    "            if table_name.lower() in query_lower:\n",
    "                relevant_tables.append(table_name)\n",
    "                continue\n",
    "                \n",
    "            # Check if any column names appear in query\n",
    "            schema = self.schema_cache[table_name]\n",
    "            if schema:\n",
    "                for col in schema['columns']:\n",
    "                    if col['name'].lower() in query_lower:\n",
    "                        relevant_tables.append(table_name)\n",
    "                        break\n",
    "        \n",
    "        # If no specific tables found, return first few tables\n",
    "        if not relevant_tables:\n",
    "            relevant_tables = list(self.schema_cache.keys())[:5]\n",
    "            \n",
    "        return relevant_tables\n",
    "    \n",
    "    def build_context_for_query(self, query_text):\n",
    "        \"\"\"Build focused context for a specific query\"\"\"\n",
    "        relevant_tables = self.get_relevant_tables(query_text)\n",
    "        \n",
    "        context = f\"\"\"\n",
    "DATABASE SCHEMA INFORMATION:\n",
    "Database: {self.db.database}\n",
    "Relevant Tables for Query: \"{query_text}\"\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        for table_name in relevant_tables:\n",
    "            schema = self.schema_cache.get(table_name)\n",
    "            if not schema:\n",
    "                continue\n",
    "                \n",
    "            context += f\"TABLE: {table_name}\\n\"\n",
    "            context += \"Columns:\\n\"\n",
    "            \n",
    "            for col in schema['columns']:\n",
    "                nullable = \"NULL\" if col['nullable'] else \"NOT NULL\"\n",
    "                context += f\"  - {col['name']}: {col['type']} ({nullable})\\n\"\n",
    "            \n",
    "            if schema['foreign_keys']:\n",
    "                context += \"Foreign Keys:\\n\"\n",
    "                for fk in schema['foreign_keys']:\n",
    "                    context += f\"  - {fk['column']} -> {fk['references_table']}.{fk['references_column']}\\n\"\n",
    "            \n",
    "            context += \"\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "\n",
    "# Initialize schema builder\n",
    "if connection_success:\n",
    "    schema_builder = SchemaContextBuilder(db)\n",
    "    print(\"‚úÖ Schema context builder initialized!\")\n",
    "    print(f\"üìä Cached schema for {len(schema_builder.schema_cache)} tables\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize schema builder - no database connection\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# sc = SchemaContextBuilder(db)\n",
    "# sc.get_table_schema(\"film\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4858675c",
   "metadata": {},
   "source": [
    "### Agent Building Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0e10bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea8b4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI GPT-4o-mini available\n",
      "\n",
      "üìä Available models: ['openai']\n"
     ]
    }
   ],
   "source": [
    "# AI Configuration\n",
    "# You'll need to set your API keys here\n",
    "# Option 1: Set as environment variables\n",
    "# export OPENAI_API_KEY=\"your-openai-key\"\n",
    "# export ANTHROPIC_API_KEY=\"your-anthropic-key\"\n",
    "\n",
    "# Option 2: Set directly in code (less secure)\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n",
    "\n",
    "def get_available_models():\n",
    "    \"\"\"Check which AI models are available based on API keys\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Check OpenAI\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        try:\n",
    "            models[\"openai\"] = ChatOpenAI(\n",
    "                model=\"gpt-4o-mini\",  # Cost-effective but powerful\n",
    "                temperature=0.1,      # Low temperature for consistent SQL generation\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            print(\"‚úÖ OpenAI GPT-4o-mini available\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå OpenAI setup failed: {str(e)}\")\n",
    "    \n",
    "    # Check Anthropic\n",
    "    if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "        try:\n",
    "            models[\"anthropic\"] = ChatAnthropic(\n",
    "                model=\"claude-3-haiku-20240307\",  # Fast and cost-effective\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            print(\"‚úÖ Anthropic Claude available\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Anthropic setup failed: {str(e)}\")\n",
    "    \n",
    "    if not models:\n",
    "        print(\"‚ö†Ô∏è  No AI models available. Please set your API keys.\")\n",
    "        print(\"   You can use OpenAI, Anthropic, or other compatible models.\")\n",
    "        print(\"   For this tutorial, we'll create a mock model for demonstration.\")\n",
    "        \n",
    "        # Create a mock model for demonstration\n",
    "        class MockModel:\n",
    "            def invoke(self, messages):\n",
    "                # Simple pattern matching for demo\n",
    "                user_msg = messages[-1].content.lower()\n",
    "                \n",
    "                if \"count\" in user_msg and \"table\" in user_msg:\n",
    "                    return type('Response', (), {'content': 'SELECT COUNT(*) FROM your_table_name;'})()\n",
    "                elif \"select\" in user_msg or \"show\" in user_msg:\n",
    "                    return type('Response', (), {'content': 'SELECT * FROM your_table_name LIMIT 10;'})()\n",
    "                else:\n",
    "                    return type('Response', (), {'content': 'SELECT * FROM your_table_name WHERE condition = value;'})()\n",
    "        \n",
    "        models[\"mock\"] = MockModel()\n",
    "        print(\"‚úÖ Mock model created for demonstration\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Initialize available models\n",
    "available_models = get_available_models()\n",
    "print(f\"\\nüìä Available models: {list(available_models.keys())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce6f4e3",
   "metadata": {},
   "source": [
    "# SQL AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49c7b9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLAgent:\n",
    "    \"\"\"\n",
    "    A robust SQL Agent that converts natural language queries to SQL using AI\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_connection, schema_builder, ai_model, model_name=\"default\"):\n",
    "        self.db = db_connection\n",
    "        self.schema_builder = schema_builder\n",
    "        self.ai_model = ai_model\n",
    "        self.model_name = model_name\n",
    "        self.query_history = []\n",
    "\n",
    "    def create_system_prompt(self):\n",
    "        \"\"\"Create a comprehensive system prompt for SQL generation\"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"You are an expert PostgreSQL database analyst. Your job is to convert natural language questions into accurate, efficient SQL queries.\n",
    "\n",
    "IMPORTANT GUIDELINES:\n",
    "1. Always use proper PostgreSQL syntax\n",
    "2. Use appropriate table and column names from the provided schema\n",
    "3. Include proper JOINs when querying multiple tables\n",
    "4. Use LIMIT clauses for exploratory queries to avoid large result sets\n",
    "5. Handle NULL values appropriately\n",
    "6. Use proper date/time functions for temporal queries\n",
    "7. Return ONLY the SQL query, no explanations or markdown formatting\n",
    "8. Make queries efficient and avoid unnecessary complexity\n",
    "\n",
    "QUERY STRUCTURE:\n",
    "- Use SELECT statements for data retrieval\n",
    "- Use appropriate WHERE clauses for filtering\n",
    "- Use GROUP BY and aggregation functions when needed\n",
    "- Use ORDER BY for sorting results\n",
    "- Use proper JOIN syntax for multi-table queries\n",
    "\n",
    "COMMON PATTERNS:\n",
    "- For counts: SELECT COUNT(*) FROM table_name WHERE condition\n",
    "- For lists: SELECT column_name FROM table_name WHERE condition LIMIT 10\n",
    "- For aggregations: SELECT column_name, AGG_FUNCTION(column) FROM table_name GROUP BY column_name\n",
    "- For date ranges: WHERE date_column BETWEEN 'start_date' AND 'end_date'\n",
    "\n",
    "Remember: Return only valid PostgreSQL SQL queries that can be executed directly.\"\"\"\n",
    "\n",
    "        return system_prompt\n",
    "    \n",
    "    def generate_sql_query(self, natural_language_query):\n",
    "            \n",
    "            \"\"\"Convert natural language to SQL query\"\"\"\n",
    "            \n",
    "            try:\n",
    "                # Build context for the query\n",
    "                schema_context = self.schema_builder.build_context_for_query(natural_language_query)\n",
    "                \n",
    "                # Create messages for the AI model\n",
    "                system_prompt = self.create_system_prompt()\n",
    "                \n",
    "                user_prompt = f\"\"\"\n",
    "    {schema_context}\n",
    "\n",
    "    Convert this natural language question to a PostgreSQL query:\n",
    "    \"{natural_language_query}\"\n",
    "\n",
    "    Return only the SQL query, nothing else.\n",
    "    \"\"\"\n",
    "                \n",
    "                # Prepare messages\n",
    "                messages = [\n",
    "                    SystemMessage(content=system_prompt),\n",
    "                    HumanMessage(content=user_prompt)\n",
    "                ]\n",
    "                \n",
    "                # Generate SQL using AI model\n",
    "                response = self.ai_model.invoke(messages)\n",
    "                sql_query = response.content.strip()\n",
    "                \n",
    "                # Clean up the response (remove markdown formatting if present)\n",
    "                sql_query = self.clean_sql_response(sql_query)\n",
    "                \n",
    "                # Store in history\n",
    "                self.query_history.append({\n",
    "                    'natural_language': natural_language_query,\n",
    "                    'sql_query': sql_query,\n",
    "                    'timestamp': datetime.now(),\n",
    "                    'model': self.model_name\n",
    "                })\n",
    "                \n",
    "                return sql_query\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error generating SQL: {str(e)}\"\n",
    "                print(f\"‚ùå {error_msg}\")\n",
    "                return None\n",
    "    \n",
    "    def clean_sql_response(self, sql_response):\n",
    "        \"\"\"Clean up SQL response from AI model\"\"\"\n",
    "        \n",
    "        # Remove markdown code blocks\n",
    "        sql_response = re.sub(r'```sql\\n', '', sql_response)\n",
    "        sql_response = re.sub(r'```\\n', '', sql_response)\n",
    "        sql_response = re.sub(r'```', '', sql_response)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        sql_response = sql_response.strip()\n",
    "        \n",
    "        # Ensure it ends with semicolon\n",
    "        if not sql_response.endswith(';'):\n",
    "            sql_response += ';'\n",
    "            \n",
    "        return sql_response\n",
    "    \n",
    "\n",
    "    def validate_sql_query(self, sql_query):\n",
    "\n",
    "        \"\"\"Validate SQL query syntax without executing it\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Use EXPLAIN to validate without executing\n",
    "            explain_query = f\"EXPLAIN {sql_query}\"\n",
    "            with self.db.engine.connect() as conn:\n",
    "                conn.execute(text(explain_query))\n",
    "            return True, \"Query is valid\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return False, f\"Query validation failed: {str(e)}\"\n",
    "        \n",
    "    def execute_query_safely(self, sql_query, max_rows=100):\n",
    "        \"\"\"Execute SQL query with safety limits\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Validate first\n",
    "            is_valid, validation_msg = self.validate_sql_query(sql_query)\n",
    "            \n",
    "            if not is_valid:\n",
    "                return None, validation_msg\n",
    "            \n",
    "            # Add LIMIT if not present for SELECT queries\n",
    "            if sql_query.upper().strip().startswith('SELECT') and 'LIMIT' not in sql_query.upper():\n",
    "                sql_query = sql_query.rstrip(';') + f' LIMIT {max_rows};'\n",
    "            \n",
    "            # Execute query\n",
    "            result_df = self.db.execute_query(sql_query)\n",
    "            \n",
    "            if result_df is not None:\n",
    "                return result_df, f\"Query executed successfully. Returned {len(result_df)} rows.\"\n",
    "            else:\n",
    "                return None, \"Query execution failed\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            return None, f\"Execution error: {str(e)}\"\n",
    "    \n",
    "\n",
    "\n",
    "    def query(self, natural_language_query, execute=True, max_rows=100):\n",
    "        \"\"\"\n",
    "        Main method to convert natural language to SQL and optionally execute it\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"ü§î Question: {natural_language_query}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        # Generate SQL\n",
    "        sql_query = self.generate_sql_query(natural_language_query)\n",
    "        \n",
    "        if sql_query is None:\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"üîß Generated SQL:\")\n",
    "        print(sql_query)\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        if execute:\n",
    "            # Execute the query\n",
    "            result_df, message = self.execute_query_safely(sql_query, max_rows)\n",
    "            print(f\"üìä {message}\")\n",
    "            \n",
    "            if result_df is not None and not result_df.empty:\n",
    "                print(\"\\nüìã Results:\")\n",
    "                print(result_df.to_string())\n",
    "            \n",
    "            return sql_query, result_df\n",
    "        else:\n",
    "            return sql_query, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fdecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Question: How many intries are there in transections table?\n",
      "================================================================================\n",
      "üîß Generated SQL:\n",
      "SELECT COUNT(*) FROM transections;\n",
      "----------------------------------------\n",
      "üìä Query executed successfully. Returned 1 rows.\n",
      "\n",
      "üìã Results:\n",
      "   count\n",
      "0   5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SELECT COUNT(*) FROM transections;',\n",
       "    count\n",
       " 0   5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = list(available_models.keys())[0]\n",
    "selected_model = available_models[model_name]\n",
    "\n",
    "sql_agent = SQLAgent(\n",
    "        db_connection=db,\n",
    "        schema_builder=schema_builder,\n",
    "        ai_model=selected_model,\n",
    "        model_name=model_name\n",
    "    )\n",
    "\n",
    "sql_agent.query(\"How many intries are there in transections table?\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "598f38b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§î Question: show me total transection amount on everyday.\n",
      "================================================================================\n",
      "üîß Generated SQL:\n",
      "SELECT Transaction_Date, SUM(Transaction_Amount) AS Total_Transaction_Amount \n",
      "FROM transections \n",
      "GROUP BY Transaction_Date \n",
      "ORDER BY Transaction_Date;\n",
      "----------------------------------------\n",
      "üìä Query validation failed: (psycopg2.errors.UndefinedColumn) column \"transaction_date\" does not exist\n",
      "LINE 1: EXPLAIN SELECT Transaction_Date, SUM(Transaction_Amount) AS ...\n",
      "                       ^\n",
      "HINT:  Perhaps you meant to reference the column \"transections.Transaction_Date\".\n",
      "\n",
      "[SQL: EXPLAIN SELECT Transaction_Date, SUM(Transaction_Amount) AS Total_Transaction_Amount \n",
      "FROM transections \n",
      "GROUP BY Transaction_Date \n",
      "ORDER BY Transaction_Date;]\n",
      "(Background on this error at: https://sqlalche.me/e/20/f405)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('SELECT Transaction_Date, SUM(Transaction_Amount) AS Total_Transaction_Amount \\nFROM transections \\nGROUP BY Transaction_Date \\nORDER BY Transaction_Date;',\n",
       " None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_agent.query(\"show me total transection amount on everyday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b236baaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query Analyzer initialized!\n"
     ]
    }
   ],
   "source": [
    "class QueryAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes and provides insights about generated SQL queries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_connection):\n",
    "        self.db = db_connection\n",
    "    \n",
    "    def analyze_query(self, sql_query):\n",
    "        \"\"\"Analyze a SQL query and provide insights\"\"\"\n",
    "        \n",
    "        analysis = {\n",
    "            'query': sql_query,\n",
    "            'type': self.get_query_type(sql_query),\n",
    "            'complexity': self.assess_complexity(sql_query),\n",
    "            'estimated_performance': None,\n",
    "            'suggestions': []\n",
    "        }\n",
    "        \n",
    "        # Get execution plan\n",
    "        try:\n",
    "            explain_query = f\"EXPLAIN (FORMAT JSON) {sql_query}\"\n",
    "            with self.db.engine.connect() as conn:\n",
    "                result = conn.execute(text(explain_query))\n",
    "                plan = result.fetchone()[0]\n",
    "                analysis['execution_plan'] = plan\n",
    "                analysis['estimated_performance'] = self.extract_performance_metrics(plan)\n",
    "        except Exception as e:\n",
    "            analysis['execution_plan_error'] = str(e)\n",
    "        \n",
    "        # Generate suggestions\n",
    "        analysis['suggestions'] = self.generate_suggestions(sql_query, analysis)\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def get_query_type(self, sql_query):\n",
    "        \"\"\"Determine the type of SQL query\"\"\"\n",
    "        query_upper = sql_query.upper().strip()\n",
    "        \n",
    "        if query_upper.startswith('SELECT'):\n",
    "            return 'SELECT'\n",
    "        elif query_upper.startswith('INSERT'):\n",
    "            return 'INSERT'\n",
    "        elif query_upper.startswith('UPDATE'):\n",
    "            return 'UPDATE'\n",
    "        elif query_upper.startswith('DELETE'):\n",
    "            return 'DELETE'\n",
    "        else:\n",
    "            return 'OTHER'\n",
    "    \n",
    "    def assess_complexity(self, sql_query):\n",
    "        \"\"\"Assess query complexity\"\"\"\n",
    "        query_upper = sql_query.upper()\n",
    "        \n",
    "        complexity_score = 0\n",
    "        \n",
    "        # Count various SQL features\n",
    "        features = {\n",
    "            'JOIN': 2,\n",
    "            'SUBQUERY': 3,\n",
    "            'GROUP BY': 2,\n",
    "            'ORDER BY': 1,\n",
    "            'HAVING': 2,\n",
    "            'UNION': 3,\n",
    "            'CASE': 2,\n",
    "            'EXISTS': 3,\n",
    "            'WINDOW': 4\n",
    "        }\n",
    "        \n",
    "        for feature, score in features.items():\n",
    "            if feature in query_upper:\n",
    "                complexity_score += score\n",
    "        \n",
    "        # Count tables\n",
    "        table_count = query_upper.count('FROM') + query_upper.count('JOIN')\n",
    "        complexity_score += table_count\n",
    "        \n",
    "        if complexity_score <= 3:\n",
    "            return 'LOW'\n",
    "        elif complexity_score <= 8:\n",
    "            return 'MEDIUM'\n",
    "        else:\n",
    "            return 'HIGH'\n",
    "    \n",
    "    def extract_performance_metrics(self, plan):\n",
    "        \"\"\"Extract performance metrics from execution plan\"\"\"\n",
    "        if not plan:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            # Extract basic metrics from the plan\n",
    "            plan_data = plan[0] if isinstance(plan, list) else plan\n",
    "            \n",
    "            metrics = {\n",
    "                'estimated_rows': plan_data.get('Plan', {}).get('Plan Rows', 0),\n",
    "                'estimated_cost': plan_data.get('Plan', {}).get('Total Cost', 0),\n",
    "                'estimated_time': plan_data.get('Plan', {}).get('Startup Cost', 0)\n",
    "            }\n",
    "            \n",
    "            return metrics\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def generate_suggestions(self, sql_query, analysis):\n",
    "        \"\"\"Generate optimization suggestions\"\"\"\n",
    "        suggestions = []\n",
    "        query_upper = sql_query.upper()\n",
    "        \n",
    "        # Check for missing LIMIT\n",
    "        if 'SELECT' in query_upper and 'LIMIT' not in query_upper:\n",
    "            suggestions.append(\"Consider adding LIMIT clause to prevent large result sets\")\n",
    "        \n",
    "        # Check for SELECT *\n",
    "        if 'SELECT *' in query_upper:\n",
    "            suggestions.append(\"Consider selecting specific columns instead of using SELECT *\")\n",
    "        \n",
    "        # Check for complex queries without indexes\n",
    "        if analysis['complexity'] == 'HIGH':\n",
    "            suggestions.append(\"This is a complex query - ensure appropriate indexes exist\")\n",
    "        \n",
    "        # Check for potential performance issues\n",
    "        if 'NOT IN' in query_upper:\n",
    "            suggestions.append(\"Consider using NOT EXISTS instead of NOT IN for better performance\")\n",
    "        \n",
    "        return suggestions\n",
    "    \n",
    "    def display_analysis(self, analysis):\n",
    "        \"\"\"Display query analysis in a readable format\"\"\"\n",
    "        \n",
    "        print(\"üîç SQL Query Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìù Query: {analysis['query'][:100]}...\")\n",
    "        print(f\"üîß Type: {analysis['type']}\")\n",
    "        print(f\"üìä Complexity: {analysis['complexity']}\")\n",
    "        \n",
    "        if analysis.get('estimated_performance'):\n",
    "            perf = analysis['estimated_performance']\n",
    "            print(f\"‚ö° Estimated Rows: {perf.get('estimated_rows', 'N/A')}\")\n",
    "            print(f\"üí∞ Estimated Cost: {perf.get('estimated_cost', 'N/A')}\")\n",
    "        \n",
    "        if analysis['suggestions']:\n",
    "            print(\"\\nüí° Suggestions:\")\n",
    "            for i, suggestion in enumerate(analysis['suggestions'], 1):\n",
    "                print(f\"  {i}. {suggestion}\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Initialize query analyzer\n",
    "if connection_success:\n",
    "    query_analyzer = QueryAnalyzer(db)\n",
    "    print(\"‚úÖ Query Analyzer initialized!\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot initialize Query Analyzer - no database connection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "375d22f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"SELECT film_info FROM actor_info WHERE first_name = 'Nick' LIMIT 10;\",\n",
       " 'type': 'SELECT',\n",
       " 'complexity': 'LOW',\n",
       " 'estimated_performance': {'estimated_rows': 3,\n",
       "  'estimated_cost': 2613.44,\n",
       "  'estimated_time': 0.7},\n",
       " 'suggestions': [],\n",
       " 'execution_plan': [{'Plan': {'Node Type': 'Limit',\n",
       "    'Parallel Aware': False,\n",
       "    'Async Capable': False,\n",
       "    'Startup Cost': 0.7,\n",
       "    'Total Cost': 2613.44,\n",
       "    'Plan Rows': 3,\n",
       "    'Plan Width': 32,\n",
       "    'Plans': [{'Node Type': 'Subquery Scan',\n",
       "      'Parent Relationship': 'Outer',\n",
       "      'Parallel Aware': False,\n",
       "      'Async Capable': False,\n",
       "      'Alias': 'actor_info',\n",
       "      'Startup Cost': 0.7,\n",
       "      'Total Cost': 2613.44,\n",
       "      'Plan Rows': 3,\n",
       "      'Plan Width': 32,\n",
       "      'Plans': [{'Node Type': 'Aggregate',\n",
       "        'Strategy': 'Sorted',\n",
       "        'Partial Mode': 'Simple',\n",
       "        'Parent Relationship': 'Subquery',\n",
       "        'Parallel Aware': False,\n",
       "        'Async Capable': False,\n",
       "        'Startup Cost': 0.7,\n",
       "        'Total Cost': 2613.41,\n",
       "        'Plan Rows': 3,\n",
       "        'Plan Width': 49,\n",
       "        'Group Key': ['a.actor_id'],\n",
       "        'Plans': [{'Node Type': 'Nested Loop',\n",
       "          'Parent Relationship': 'Outer',\n",
       "          'Parallel Aware': False,\n",
       "          'Async Capable': False,\n",
       "          'Join Type': 'Left',\n",
       "          'Startup Cost': 0.7,\n",
       "          'Total Cost': 76.71,\n",
       "          'Plan Rows': 82,\n",
       "          'Plan Width': 89,\n",
       "          'Inner Unique': True,\n",
       "          'Join Filter': '(fc.category_id = c.category_id)',\n",
       "          'Plans': [{'Node Type': 'Nested Loop',\n",
       "            'Parent Relationship': 'Outer',\n",
       "            'Parallel Aware': False,\n",
       "            'Async Capable': False,\n",
       "            'Join Type': 'Left',\n",
       "            'Startup Cost': 0.7,\n",
       "            'Total Cost': 56.89,\n",
       "            'Plan Rows': 82,\n",
       "            'Plan Width': 19,\n",
       "            'Inner Unique': False,\n",
       "            'Plans': [{'Node Type': 'Index Scan',\n",
       "              'Parent Relationship': 'Outer',\n",
       "              'Parallel Aware': False,\n",
       "              'Async Capable': False,\n",
       "              'Scan Direction': 'Forward',\n",
       "              'Index Name': 'actor_pkey',\n",
       "              'Relation Name': 'actor',\n",
       "              'Alias': 'a',\n",
       "              'Startup Cost': 0.14,\n",
       "              'Total Cost': 16.66,\n",
       "              'Plan Rows': 3,\n",
       "              'Plan Width': 17,\n",
       "              'Filter': \"((first_name)::text = 'Nick'::text)\"},\n",
       "             {'Node Type': 'Nested Loop',\n",
       "              'Parent Relationship': 'Inner',\n",
       "              'Parallel Aware': False,\n",
       "              'Async Capable': False,\n",
       "              'Join Type': 'Left',\n",
       "              'Startup Cost': 0.56,\n",
       "              'Total Cost': 13.14,\n",
       "              'Plan Rows': 27,\n",
       "              'Plan Width': 4,\n",
       "              'Inner Unique': False,\n",
       "              'Plans': [{'Node Type': 'Index Only Scan',\n",
       "                'Parent Relationship': 'Outer',\n",
       "                'Parallel Aware': False,\n",
       "                'Async Capable': False,\n",
       "                'Scan Direction': 'Forward',\n",
       "                'Index Name': 'film_actor_pkey',\n",
       "                'Relation Name': 'film_actor',\n",
       "                'Alias': 'fa',\n",
       "                'Startup Cost': 0.28,\n",
       "                'Total Cost': 4.75,\n",
       "                'Plan Rows': 27,\n",
       "                'Plan Width': 4,\n",
       "                'Index Cond': '(actor_id = a.actor_id)'},\n",
       "               {'Node Type': 'Index Only Scan',\n",
       "                'Parent Relationship': 'Inner',\n",
       "                'Parallel Aware': False,\n",
       "                'Async Capable': False,\n",
       "                'Scan Direction': 'Forward',\n",
       "                'Index Name': 'film_category_pkey',\n",
       "                'Relation Name': 'film_category',\n",
       "                'Alias': 'fc',\n",
       "                'Startup Cost': 0.28,\n",
       "                'Total Cost': 0.3,\n",
       "                'Plan Rows': 1,\n",
       "                'Plan Width': 4,\n",
       "                'Index Cond': '(film_id = fa.film_id)'}]}]},\n",
       "           {'Node Type': 'Materialize',\n",
       "            'Parent Relationship': 'Inner',\n",
       "            'Parallel Aware': False,\n",
       "            'Async Capable': False,\n",
       "            'Startup Cost': 0.0,\n",
       "            'Total Cost': 1.24,\n",
       "            'Plan Rows': 16,\n",
       "            'Plan Width': 72,\n",
       "            'Plans': [{'Node Type': 'Seq Scan',\n",
       "              'Parent Relationship': 'Outer',\n",
       "              'Parallel Aware': False,\n",
       "              'Async Capable': False,\n",
       "              'Relation Name': 'category',\n",
       "              'Alias': 'c',\n",
       "              'Startup Cost': 0.0,\n",
       "              'Total Cost': 1.16,\n",
       "              'Plan Rows': 16,\n",
       "              'Plan Width': 72}]}]},\n",
       "         {'Node Type': 'Aggregate',\n",
       "          'Strategy': 'Sorted',\n",
       "          'Partial Mode': 'Simple',\n",
       "          'Parent Relationship': 'SubPlan',\n",
       "          'Subplan Name': 'SubPlan 1',\n",
       "          'Parallel Aware': False,\n",
       "          'Async Capable': False,\n",
       "          'Startup Cost': 5.37,\n",
       "          'Total Cost': 30.68,\n",
       "          'Plan Rows': 2,\n",
       "          'Plan Width': 34,\n",
       "          'Group Key': ['fa_1.actor_id'],\n",
       "          'Plans': [{'Node Type': 'Nested Loop',\n",
       "            'Parent Relationship': 'Outer',\n",
       "            'Parallel Aware': False,\n",
       "            'Async Capable': False,\n",
       "            'Join Type': 'Inner',\n",
       "            'Startup Cost': 5.37,\n",
       "            'Total Cost': 30.15,\n",
       "            'Plan Rows': 2,\n",
       "            'Plan Width': 17,\n",
       "            'Inner Unique': True,\n",
       "            'Plans': [{'Node Type': 'Hash Join',\n",
       "              'Parent Relationship': 'Outer',\n",
       "              'Parallel Aware': False,\n",
       "              'Async Capable': False,\n",
       "              'Join Type': 'Inner',\n",
       "              'Startup Cost': 5.09,\n",
       "              'Total Cost': 23.76,\n",
       "              'Plan Rows': 2,\n",
       "              'Plan Width': 6,\n",
       "              'Inner Unique': True,\n",
       "              'Hash Cond': '(fc_1.film_id = fa_1.film_id)',\n",
       "              'Plans': [{'Node Type': 'Seq Scan',\n",
       "                'Parent Relationship': 'Outer',\n",
       "                'Parallel Aware': False,\n",
       "                'Async Capable': False,\n",
       "                'Relation Name': 'film_category',\n",
       "                'Alias': 'fc_1',\n",
       "                'Startup Cost': 0.0,\n",
       "                'Total Cost': 18.5,\n",
       "                'Plan Rows': 62,\n",
       "                'Plan Width': 2,\n",
       "                'Filter': '(category_id = c.category_id)'},\n",
       "               {'Node Type': 'Hash',\n",
       "                'Parent Relationship': 'Inner',\n",
       "                'Parallel Aware': False,\n",
       "                'Async Capable': False,\n",
       "                'Startup Cost': 4.75,\n",
       "                'Total Cost': 4.75,\n",
       "                'Plan Rows': 27,\n",
       "                'Plan Width': 4,\n",
       "                'Plans': [{'Node Type': 'Index Only Scan',\n",
       "                  'Parent Relationship': 'Outer',\n",
       "                  'Parallel Aware': False,\n",
       "                  'Async Capable': False,\n",
       "                  'Scan Direction': 'Forward',\n",
       "                  'Index Name': 'film_actor_pkey',\n",
       "                  'Relation Name': 'film_actor',\n",
       "                  'Alias': 'fa_1',\n",
       "                  'Startup Cost': 0.28,\n",
       "                  'Total Cost': 4.75,\n",
       "                  'Plan Rows': 27,\n",
       "                  'Plan Width': 4,\n",
       "                  'Index Cond': '(actor_id = a.actor_id)'}]}]},\n",
       "             {'Node Type': 'Index Scan',\n",
       "              'Parent Relationship': 'Inner',\n",
       "              'Parallel Aware': False,\n",
       "              'Async Capable': False,\n",
       "              'Scan Direction': 'Forward',\n",
       "              'Index Name': 'film_pkey',\n",
       "              'Relation Name': 'film',\n",
       "              'Alias': 'f',\n",
       "              'Startup Cost': 0.28,\n",
       "              'Total Cost': 3.2,\n",
       "              'Plan Rows': 1,\n",
       "              'Plan Width': 19,\n",
       "              'Index Cond': '(film_id = fc_1.film_id)'}]}]}]}]}]}}]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyser = QueryAnalyzer(db_connection=db)\n",
    "\n",
    "analyser.analyze_query(\"SELECT film_info FROM actor_info WHERE first_name = 'Nick' LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0922696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
