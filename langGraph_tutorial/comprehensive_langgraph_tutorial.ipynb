{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Complete LangGraph Tutorial: From Basics to Advanced Applications\n",
        "\n",
        "## ğŸ“– Comprehensive Guide for Building Stateful AI Agents\n",
        "\n",
        "Welcome to the most comprehensive LangGraph tutorial! This notebook will take you from complete beginner to building sophisticated AI agents step by step.\n",
        "\n",
        "### ğŸ¯ What You'll Learn\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "- âœ… Understand LangGraph's core concepts (Nodes, Edges, State)\n",
        "- âœ… Build your first simple chatbot\n",
        "- âœ… Add memory and persistence to conversations\n",
        "- âœ… Implement tool calling and external integrations\n",
        "- âœ… Create human-in-the-loop workflows\n",
        "- âœ… Build multi-agent systems\n",
        "- âœ… Handle complex state management\n",
        "- âœ… Deploy production-ready applications\n",
        "\n",
        "### ğŸ“š Based on Official LangGraph Documentation\n",
        "\n",
        "This tutorial follows the [official LangGraph documentation](https://langchain-ai.github.io/langgraph/) and incorporates best practices from the LangGraph team.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Table of Contents\n",
        "\n",
        "1. **[Setup & Installation](#setup)**\n",
        "2. **[Part 1: Understanding LangGraph Fundamentals](#part1)**\n",
        "3. **[Part 2: Building Your First Simple Agent](#part2)**\n",
        "4. **[Part 3: Adding Memory with Checkpointing](#part3)**\n",
        "5. **[Part 4: Tool Integration & External APIs](#part4)**\n",
        "6. **[Part 5: Human-in-the-Loop Workflows](#part5)**\n",
        "7. **[Part 6: Advanced State Management](#part6)**\n",
        "8. **[Part 7: Multi-Agent Systems](#part7)**\n",
        "9. **[Part 8: Real-World Use Cases](#part8)**\n",
        "10. **[Part 9: Production Deployment](#part9)**\n",
        "\n",
        "Let's begin this exciting journey! ğŸŒŸ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Setup & Installation {#setup}\n",
        "\n",
        "Before we dive into LangGraph, let's set up our environment properly.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "1. **Python 3.8+** installed on your system\n",
        "2. **API Keys** for LLM providers (we'll use OpenAI in this tutorial)\n",
        "3. **Basic Python knowledge** - understanding of functions, classes, and dictionaries\n",
        "\n",
        "### What is LangGraph?\n",
        "\n",
        "**LangGraph** is a library for building **stateful, multi-actor applications** with LLMs. It extends LangChain with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a **cyclic** manner.\n",
        "\n",
        "Key features:\n",
        "- ğŸ”„ **Cyclic workflows** (not just linear chains)\n",
        "- ğŸ’¾ **Persistent state** across interactions\n",
        "- ğŸ¯ **Conditional routing** between different paths\n",
        "- ğŸ”§ **Human-in-the-loop** capabilities\n",
        "- ğŸ“Š **Built-in observability** with LangSmith\n",
        "\n",
        "Think of it as a way to build AI agents that can:\n",
        "- Remember previous conversations\n",
        "- Make decisions about what to do next\n",
        "- Use tools and external APIs\n",
        "- Involve humans when needed\n",
        "- Handle complex, multi-step workflows\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q langgraph langsmith langchain-openai python-dotenv\n",
        "\n",
        "# Optional: for visualization\n",
        "%pip install -q matplotlib graphviz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith setup (optional but recommended for debugging):\n",
            "âœ… LangSmith tracing enabled!\n",
            "ğŸš€ Environment setup complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "from typing import Annotated, Dict, List, Any\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Set up environment variables\n",
        "def setup_environment():\n",
        "    \"\"\"Setup API keys for the tutorial\"\"\"\n",
        "    \n",
        "    # OpenAI API Key\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        openai_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "    \n",
        "    # Optional: LangSmith for observability (highly recommended)\n",
        "    if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
        "        print(\"LangSmith setup (optional but recommended for debugging):\")\n",
        "        langsmith_key = getpass.getpass(\"Enter your LangSmith API Key (or press Enter to skip): \")\n",
        "        if langsmith_key:\n",
        "            os.environ[\"LANGSMITH_API_KEY\"] = langsmith_key\n",
        "            os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "            os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph-Tutorial\"\n",
        "            print(\"âœ… LangSmith tracing enabled!\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Skipping LangSmith setup\")\n",
        "    \n",
        "    print(\"ğŸš€ Environment setup complete!\")\n",
        "\n",
        "# Run setup\n",
        "setup_environment()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Part 1: Understanding LangGraph Fundamentals {#part1}\n",
        "\n",
        "Before we build anything, let's understand the core concepts that make LangGraph powerful.\n",
        "\n",
        "### ğŸ”‘ Core Concepts\n",
        "\n",
        "#### 1. **State** \n",
        "- The \"memory\" of your application\n",
        "- Shared data structure that persists across all steps\n",
        "- Can contain messages, variables, flags, or any data you need\n",
        "\n",
        "#### 2. **Nodes**\n",
        "- Individual functions or operations in your workflow\n",
        "- Each node receives the current state and returns updates\n",
        "- Think of them as \"workers\" that do specific tasks\n",
        "\n",
        "#### 3. **Edges** \n",
        "- Connections between nodes that define the flow\n",
        "- Can be simple (A â†’ B) or conditional (A â†’ B or C based on logic)\n",
        "\n",
        "#### 4. **Graph**\n",
        "- The complete workflow combining nodes and edges\n",
        "- Defines how your AI agent behaves and makes decisions\n",
        "\n",
        "### ğŸ¯ Simple Mental Model\n",
        "\n",
        "Think of LangGraph like a **flowchart for AI agents**:\n",
        "\n",
        "```\n",
        "[User Input] â†’ [AI Thinks] â†’ [Uses Tool?] \n",
        "                    â†“              â†“\n",
        "               [Respond]      [Call Tool] â†’ [AI Thinks] â†’ [Respond]\n",
        "```\n",
        "\n",
        "But unlike a simple flowchart, LangGraph can:\n",
        "- Remember everything that happened before\n",
        "- Loop back to previous steps\n",
        "- Involve humans in the decision-making\n",
        "- Handle complex, branching logic\n",
        "\n",
        "Let's see this in action! ğŸ‘‡\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– Part 2: Building Your First Simple Agent {#part2}\n",
        "\n",
        "Let's start with the simplest possible LangGraph application - a basic chatbot that can have a conversation.\n",
        "\n",
        "### Step 1: Define the State\n",
        "\n",
        "The state is like the \"memory\" of our agent. For a chatbot, we need to remember the conversation history.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… State defined!\n",
            "Our state has one field: 'messages' that will store the conversation history\n",
            "The add_messages function ensures new messages are appended, not replaced\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Define our State - this is the \"memory\" of our agent\n",
        "class State(TypedDict):\n",
        "    # messages will store our conversation history\n",
        "    # add_messages is a special function that appends new messages instead of replacing them\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "print(\"âœ… State defined!\")\n",
        "print(\"Our state has one field: 'messages' that will store the conversation history\")\n",
        "print(\"The add_messages function ensures new messages are appended, not replaced\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 2: Create the Language Model\n",
        "\n",
        "Now we need an AI model to power our chatbot. We'll use OpenAI's GPT model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Language model initialized!\n",
            "Model: gpt-3.5-turbo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test response: Hello there! How are you doing today?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",  # You can also use \"gpt-4\" if you have access\n",
        "    temperature=0.7,        # Controls creativity (0 = deterministic, 1 = very creative)\n",
        ")\n",
        "\n",
        "print(\"âœ… Language model initialized!\")\n",
        "print(f\"Model: {llm.model_name}\")\n",
        "\n",
        "# Let's test it quickly\n",
        "test_response = llm.invoke(\"Say hello in a friendly way!\")\n",
        "print(f\"Test response: {test_response.content}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 3: Create the Chatbot Node\n",
        "\n",
        "A **node** is a function that:\n",
        "1. Takes the current state as input\n",
        "2. Does some work (like calling the LLM)\n",
        "3. Returns updates to the state\n",
        "\n",
        "Let's create our chatbot node:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chatbot_node(state: State) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    The main chatbot node that processes messages and generates responses.\n",
        "    \n",
        "    Args:\n",
        "        state: Current state containing conversation history\n",
        "        \n",
        "    Returns:\n",
        "        Dict with new messages to add to state\n",
        "    \"\"\"\n",
        "    # Get the conversation history from state\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # Call the LLM with the conversation history\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # Return the new message to be added to state\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "print(\"âœ… Chatbot node created!\")\n",
        "print(\"This node will:\")\n",
        "print(\"1. Take the conversation history from state\")\n",
        "print(\"2. Send it to the LLM\")\n",
        "print(\"3. Return the LLM's response to be added to state\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 4: Build the Graph\n",
        "\n",
        "Now we'll create the graph by:\n",
        "1. Creating a StateGraph\n",
        "2. Adding our chatbot node\n",
        "3. Defining the flow (edges)\n",
        "4. Compiling it into a runnable application\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create the graph builder\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Step 2: Add our chatbot node\n",
        "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
        "\n",
        "# Step 3: Define the flow\n",
        "# START -> chatbot -> END\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Step 4: Compile the graph\n",
        "simple_chatbot = graph_builder.compile()\n",
        "\n",
        "print(\"âœ… Simple chatbot graph created!\")\n",
        "print(\"Flow: START â†’ chatbot â†’ END\")\n",
        "print(\"Ready to chat! ğŸ‰\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 5: Test Your First LangGraph Agent!\n",
        "\n",
        "Let's test our simple chatbot. We'll send it a message and see how it responds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test our simple chatbot\n",
        "def test_simple_chatbot():\n",
        "    print(\"ğŸ¤– Testing Simple Chatbot\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Create initial state with a user message\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"Hello! My name is Alex. What's your name?\")]\n",
        "    }\n",
        "    \n",
        "    # Run the chatbot\n",
        "    result = simple_chatbot.invoke(initial_state)\n",
        "    \n",
        "    # Print the conversation\n",
        "    for i, message in enumerate(result[\"messages\"]):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    return result\n",
        "\n",
        "# Run the test\n",
        "first_result = test_simple_chatbot()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### ğŸ‰ Congratulations!\n",
        "\n",
        "You just built your first LangGraph agent! But there's a problem...\n",
        "\n",
        "**The Issue**: Our chatbot doesn't remember previous conversations. Each time we call it, it starts fresh.\n",
        "\n",
        "Let's test this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test memory issue - the chatbot won't remember the previous conversation\n",
        "print(\"ğŸ§ª Testing Memory Issue\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Second message - asking about the name mentioned earlier\n",
        "second_state = {\n",
        "    \"messages\": [HumanMessage(content=\"What was my name again?\")]\n",
        "}\n",
        "\n",
        "result2 = simple_chatbot.invoke(second_state)\n",
        "\n",
        "for message in result2[\"messages\"]:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"ğŸ¤– AI: {message.content}\")\n",
        "\n",
        "print(\"\\nâŒ As you can see, the chatbot doesn't remember your name!\")\n",
        "print(\"This is because each call starts with a fresh state.\")\n",
        "print(\"In the next section, we'll fix this with **memory and checkpointing**! ğŸ§ \")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ§  Part 3: Adding Memory with Checkpointing {#part3}\n",
        "\n",
        "The power of LangGraph really shines when we add **persistent memory**. This allows our agent to:\n",
        "\n",
        "- Remember conversations across multiple interactions\n",
        "- Resume from where it left off\n",
        "- Handle long-running workflows\n",
        "- Support human-in-the-loop scenarios\n",
        "\n",
        "### What is Checkpointing?\n",
        "\n",
        "**Checkpointing** automatically saves the state after each step. When you invoke the graph again with the same `thread_id`, it loads the saved state and continues from there.\n",
        "\n",
        "Think of it like a video game save system! ğŸ®\n",
        "\n",
        "### Step 1: Set Up a Checkpointer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "print(\"âœ… Checkpointer created!\")\n",
        "print(\"This will automatically save and restore conversation state\")\n",
        "print(\"In production, you'd use a real database file instead of ':memory:'\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 2: Create a Chatbot with Memory\n",
        "\n",
        "Now let's rebuild our chatbot with memory capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_builder_with_memory = StateGraph(State)\n",
        "\n",
        "graph_builder_with_memory.add_node(\"chatbot\", chatbot_node)\n",
        "graph_builder_with_memory.add_edge(START, \"chatbot\")\n",
        "graph_builder_with_memory.add_edge(\"chatbot\", END)\n",
        "\n",
        "chatbot_with_memory = graph_builder_with_memory.compile(checkpointer=memory)\n",
        "unique_id = uuid.uuid4()\n",
        "print(f\"unique ID:- {unique_id}\")\n",
        "\n",
        "\n",
        "unique_id = \"831173cc-dd33-4542-925d-7958802a975f\"\n",
        "def test_simple(user_input):\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\":str(unique_id)}}\n",
        "    result = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=user_input)]},\n",
        "        config\n",
        "    )\n",
        "    return result\n",
        "    \n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 3: Test the Memory\n",
        "\n",
        "The key to using memory is the **config** parameter with a `thread_id`. All conversations with the same `thread_id` will share memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_memory_chatbot():\n",
        "    print(\"ğŸ§  Testing Chatbot with Memory\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Configuration with thread_id - this is crucial for memory!\n",
        "    config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "    \n",
        "    # First message\n",
        "    print(\"ğŸ“ First interaction:\")\n",
        "    result1 = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"Hi! My name is Alex and I love programming.\")]},\n",
        "        config  # Pass config as second parameter!\n",
        "    )\n",
        "    \n",
        "    for message in result1[\"messages\"]:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\nğŸ“ Second interaction (same thread_id):\")\n",
        "    result2 = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"What's my name and what do I love?\")]},\n",
        "        config  # Same config = same memory!\n",
        "    )\n",
        "    \n",
        "    # Only show the new messages\n",
        "    new_messages = result2[\"messages\"][len(result1[\"messages\"]):]\n",
        "    for message in new_messages:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\nâœ… Success! The chatbot remembered both your name and interest!\")\n",
        "    return result2\n",
        "\n",
        "# Test the memory\n",
        "memory_result = test_memory_chatbot()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### ğŸ” Understanding Thread IDs\n",
        "\n",
        "Different `thread_id`s create separate conversations. Let's test this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a different thread_id\n",
        "print(\"ğŸ”„ Testing Different Thread ID\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Different thread_id = fresh conversation\n",
        "different_config = {\"configurable\": {\"thread_id\": \"conversation_2\"}}\n",
        "\n",
        "result_different = chatbot_with_memory.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What's my name?\")]},\n",
        "    different_config  # Different thread_id!\n",
        ")\n",
        "\n",
        "for message in result_different[\"messages\"]:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"ğŸ¤– AI: {message.content}\")\n",
        "\n",
        "print(\"\\nğŸ¯ Key Insight:\")\n",
        "print(\"- Same thread_id = shared memory\")\n",
        "print(\"- Different thread_id = separate conversations\")\n",
        "print(\"- This allows multiple users or conversation contexts!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### ğŸ“Š Inspecting the State\n",
        "\n",
        "You can inspect the current state of any conversation using `get_state()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the state of our first conversation\n",
        "config1 = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "state_snapshot = chatbot_with_memory.get_state(config1)\n",
        "\n",
        "print(\"ğŸ“Š State Inspection\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Number of messages: {len(state_snapshot.values['messages'])}\")\n",
        "print(f\"Next node to execute: {state_snapshot.next}\")\n",
        "print(f\"Thread ID: {state_snapshot.config['configurable']['thread_id']}\")\n",
        "\n",
        "print(\"\\nğŸ’¬ Full conversation history:\")\n",
        "for i, message in enumerate(state_snapshot.values[\"messages\"], 1):\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"{i}. ğŸ‘¤ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"{i}. ğŸ¤– AI: {message.content[:100]}...\")  # Truncate for readability\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ› ï¸ Part 4: Tool Integration & External APIs {#part4}\n",
        "\n",
        "Real AI agents need to interact with the outside world! Let's add tool calling capabilities to our chatbot.\n",
        "\n",
        "### What are Tools?\n",
        "\n",
        "Tools are functions that your AI agent can call to:\n",
        "- Search the web\n",
        "- Query databases\n",
        "- Send emails\n",
        "- Perform calculations\n",
        "- Access APIs\n",
        "- And much more!\n",
        "\n",
        "### Step 1: Create a Simple Tool\n",
        "\n",
        "Let's start with a simple calculator tool:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "import math\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Evaluate a mathematical expression safely.\n",
        "    \n",
        "    Args:\n",
        "        expression: A mathematical expression to evaluate (e.g., \"2 + 3 * 4\")\n",
        "        \n",
        "    Returns:\n",
        "        The result of the calculation\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Safe evaluation of mathematical expressions\n",
        "        # Only allow basic math operations\n",
        "        allowed_names = {\n",
        "            k: v for k, v in math.__dict__.items() if not k.startswith(\"__\")\n",
        "        }\n",
        "        allowed_names.update({\"abs\": abs, \"round\": round})\n",
        "        \n",
        "        result = eval(expression, {\"__builtins__\": {}}, allowed_names)\n",
        "        return f\"The result of {expression} is {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calculating {expression}: {str(e)}\"\n",
        "\n",
        "@tool  \n",
        "def get_current_time() -> str:\n",
        "    \"\"\"Get the current time.\"\"\"\n",
        "    from datetime import datetime\n",
        "    return f\"Current time is: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "# Test our tools\n",
        "print(\"ğŸ§® Testing Calculator Tool:\")\n",
        "print(calculator.invoke({\"expression\": \"2 + 3 * 4\"}))\n",
        "print(calculator.invoke({\"expression\": \"sqrt(16) + 5\"}))\n",
        "\n",
        "print(\"\\nâ° Testing Time Tool:\")\n",
        "print(get_current_time.invoke({}))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 2: Create a Tool-Enabled Chatbot\n",
        "\n",
        "Now we need to:\n",
        "1. Bind tools to our LLM\n",
        "2. Create a tool-calling node\n",
        "3. Add conditional logic to decide when to use tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import ToolMessage\n",
        "\n",
        "# Step 1: Create LLM with tools\n",
        "tools = [calculator, get_current_time]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "# Step 2: Create nodes\n",
        "def chatbot_with_tools(state: State) -> Dict[str, Any]:\n",
        "    \"\"\"Chatbot that can use tools\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Step 3: Create tool node using LangGraph's prebuilt ToolNode\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "print(\"âœ… Tool-enabled chatbot components created!\")\n",
        "print(\"- LLM knows about our tools\")\n",
        "print(\"- Chatbot node can generate tool calls\") \n",
        "print(\"- Tool node can execute the tools\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 3: Add Conditional Logic\n",
        "\n",
        "We need to route between:\n",
        "- **Tools**: If the LLM wants to use a tool\n",
        "- **End**: If the LLM gives a final response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "def should_continue(state: State) -> Literal[\"tools\", \"__end__\"]:\n",
        "    \"\"\"\n",
        "    Determine if we should use tools or end the conversation.\n",
        "    \n",
        "    Returns:\n",
        "        \"tools\" if the last message has tool calls\n",
        "        \"__end__\" if we should end\n",
        "    \"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    \n",
        "    # If the last message has tool calls, we should use tools\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    else:\n",
        "        return \"__end__\"\n",
        "\n",
        "print(\"âœ… Conditional logic created!\")\n",
        "print(\"This function decides whether to:\")\n",
        "print(\"- Use tools (if LLM made tool calls)\")\n",
        "print(\"- End conversation (if LLM gave final response)\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 4: Build the Tool-Enabled Graph\n",
        "\n",
        "Now let's put it all together:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the graph\n",
        "tool_graph_builder = StateGraph(State)\n",
        "\n",
        "# Add nodes\n",
        "tool_graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
        "tool_graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Add edges\n",
        "tool_graph_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Add conditional edges\n",
        "tool_graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",  # From chatbot node\n",
        "    should_continue,  # Use this function to decide\n",
        "    {\n",
        "        \"tools\": \"tools\",    # If should_continue returns \"tools\", go to tools node\n",
        "        \"__end__\": END       # If should_continue returns \"__end__\", end the graph\n",
        "    }\n",
        ")\n",
        "\n",
        "# After tools, always go back to chatbot\n",
        "tool_graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "# Compile with memory\n",
        "tool_chatbot = tool_graph_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "print(\"âœ… Tool-enabled chatbot with memory created!\")\n",
        "print(\"Flow: START â†’ chatbot â†’ [tools OR end] â†’ (if tools) â†’ chatbot â†’ ...\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 5: Test the Tool-Enabled Chatbot\n",
        "\n",
        "Let's test our enhanced chatbot with some math problems and time queries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_tool_chatbot():\n",
        "    print(\"ğŸ› ï¸ Testing Tool-Enabled Chatbot\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"tool_test_1\"}}\n",
        "    \n",
        "    # Test 1: Math calculation\n",
        "    print(\"ğŸ“ Test 1: Math Calculation\")\n",
        "    result1 = tool_chatbot.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"What's 15 * 7 + sqrt(144)?\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    for message in result1[\"messages\"]:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "                print(f\"ğŸ¤– AI: [Calling tool: {message.tool_calls[0]['name']}]\")\n",
        "            else:\n",
        "                print(f\"ğŸ¤– AI: {message.content}\")\n",
        "        elif isinstance(message, ToolMessage):\n",
        "            print(f\"ğŸ”§ Tool: {message.content}\")\n",
        "    \n",
        "    print(\"\\nğŸ“ Test 2: Current Time\")\n",
        "    result2 = tool_chatbot.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"What time is it right now?\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    # Show only new messages\n",
        "    new_messages = result2[\"messages\"][len(result1[\"messages\"]):]\n",
        "    for message in new_messages:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "                print(f\"ğŸ¤– AI: [Calling tool: {message.tool_calls[0]['name']}]\")\n",
        "            else:\n",
        "                print(f\"ğŸ¤– AI: {message.content}\")\n",
        "        elif isinstance(message, ToolMessage):\n",
        "            print(f\"ğŸ”§ Tool: {message.content}\")\n",
        "    \n",
        "    print(\"\\nâœ… Tool integration working perfectly!\")\n",
        "    return result2\n",
        "\n",
        "# Test the tool-enabled chatbot\n",
        "tool_result = test_tool_chatbot()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ¤ Part 5: Human-in-the-Loop Workflows {#part5}\n",
        "\n",
        "Sometimes AI agents need human oversight or approval before taking actions. LangGraph makes this easy with **interrupts**.\n",
        "\n",
        "### What are Interrupts?\n",
        "\n",
        "Interrupts pause the graph execution at specific nodes, allowing humans to:\n",
        "- Review what the agent plans to do\n",
        "- Modify the state if needed\n",
        "- Approve or reject actions\n",
        "- Provide additional guidance\n",
        "\n",
        "### Step 1: Create an Agent that Asks for Help\n",
        "\n",
        "Let's create an agent that can request human assistance when it's unsure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced State with human assistance flag\n",
        "class HumanLoopState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    ask_human: bool  # Flag to request human help\n",
        "\n",
        "@tool\n",
        "def request_human_help(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Request help from a human supervisor.\n",
        "    \n",
        "    Args:\n",
        "        question: The question or situation where human help is needed\n",
        "        \n",
        "    Returns:\n",
        "        Confirmation that help has been requested\n",
        "    \"\"\"\n",
        "    return f\"Human help requested for: {question}\"\n",
        "\n",
        "# Add the new tool to our toolkit\n",
        "human_tools = [calculator, get_current_time, request_human_help]\n",
        "llm_with_human_tools = llm.bind_tools(human_tools)\n",
        "\n",
        "print(\"âœ… Human-in-the-loop tools created!\")\n",
        "print(\"The agent can now request human assistance when needed\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 2: Create Nodes with Human Assistance Logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chatbot_with_human_help(state: HumanLoopState) -> Dict[str, Any]:\n",
        "    \"\"\"Chatbot that can request human help\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm_with_human_tools.invoke(messages)\n",
        "    \n",
        "    # Check if the agent requested human help\n",
        "    ask_human = False\n",
        "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
        "        for tool_call in response.tool_calls:\n",
        "            if tool_call['name'] == 'request_human_help':\n",
        "                ask_human = True\n",
        "                break\n",
        "    \n",
        "    return {\"messages\": [response], \"ask_human\": ask_human}\n",
        "\n",
        "def human_node(state: HumanLoopState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Human intervention node - this is where humans can provide input\n",
        "    \"\"\"\n",
        "    # In a real application, this would wait for human input\n",
        "    # For this demo, we'll simulate human response\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        tool_call = last_message.tool_calls[0]\n",
        "        if tool_call['name'] == 'request_human_help':\n",
        "            # Simulate human response\n",
        "            human_response = \"I've reviewed your question. Please proceed with the calculation and provide a detailed explanation.\"\n",
        "            \n",
        "            # Create a tool message with the human's response\n",
        "            tool_message = ToolMessage(\n",
        "                content=human_response,\n",
        "                tool_call_id=tool_call['id']\n",
        "            )\n",
        "            return {\"messages\": [tool_message], \"ask_human\": False}\n",
        "    \n",
        "    return {\"ask_human\": False}\n",
        "\n",
        "print(\"âœ… Human-in-the-loop nodes created!\")\n",
        "print(\"- chatbot_with_human_help: Can request human assistance\")\n",
        "print(\"- human_node: Handles human intervention\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 3: Build the Human-in-the-Loop Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def route_human_loop(state: HumanLoopState) -> Literal[\"human\", \"tools\", \"__end__\"]:\n",
        "    \"\"\"Route based on whether human help is needed or tools should be called\"\"\"\n",
        "    \n",
        "    if state.get(\"ask_human\", False):\n",
        "        return \"human\"\n",
        "    \n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    \n",
        "    # Check for tool calls (excluding request_human_help)\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            if tool_call['name'] != 'request_human_help':\n",
        "                return \"tools\"\n",
        "    \n",
        "    return \"__end__\"\n",
        "\n",
        "# Build the graph\n",
        "human_loop_builder = StateGraph(HumanLoopState)\n",
        "\n",
        "# Add nodes\n",
        "human_loop_builder.add_node(\"chatbot\", chatbot_with_human_help)\n",
        "human_loop_builder.add_node(\"tools\", ToolNode(human_tools))\n",
        "human_loop_builder.add_node(\"human\", human_node)\n",
        "\n",
        "# Add edges\n",
        "human_loop_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Add conditional routing\n",
        "human_loop_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_human_loop,\n",
        "    {\n",
        "        \"human\": \"human\",\n",
        "        \"tools\": \"tools\", \n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# After human or tools, go back to chatbot\n",
        "human_loop_builder.add_edge(\"human\", \"chatbot\")\n",
        "human_loop_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "# Compile with interrupt before human node\n",
        "human_loop_chatbot = human_loop_builder.compile(\n",
        "    checkpointer=checkpointer,\n",
        "    interrupt_before=[\"human\"]  # This pauses execution before human node\n",
        ")\n",
        "\n",
        "print(\"âœ… Human-in-the-loop chatbot created!\")\n",
        "print(\"Key feature: interrupt_before=['human'] pauses for human input\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 4: Test Human-in-the-Loop\n",
        "\n",
        "Let's test the interrupt functionality:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_human_loop():\n",
        "    print(\"ğŸ¤ Testing Human-in-the-Loop\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"human_loop_test\"}}\n",
        "    \n",
        "    # Request that requires human help\n",
        "    print(\"ğŸ“ Asking agent to request human help:\")\n",
        "    \n",
        "    initial_result = human_loop_chatbot.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"I need help with a complex decision. Can you request human assistance?\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    for message in initial_result[\"messages\"]:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "                print(f\"ğŸ¤– AI: [Requesting human help]\")\n",
        "            else:\n",
        "                print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    # Check if execution was interrupted\n",
        "    state = human_loop_chatbot.get_state(config)\n",
        "    print(f\"\\nğŸ” Current state:\")\n",
        "    print(f\"Next node to execute: {state.next}\")\n",
        "    print(f\"Ask human flag: {state.values.get('ask_human', False)}\")\n",
        "    \n",
        "    if state.next == (\"human\",):\n",
        "        print(\"\\nâœ… Execution interrupted! Human intervention required.\")\n",
        "        print(\"In a real app, a human would now review and provide input.\")\n",
        "        \n",
        "        # Continue execution (simulating human approval)\n",
        "        print(\"\\nâ–¶ï¸ Continuing execution (simulating human input)...\")\n",
        "        final_result = human_loop_chatbot.invoke(None, config)\n",
        "        \n",
        "        # Show the final messages\n",
        "        new_messages = final_result[\"messages\"][len(initial_result[\"messages\"]):]\n",
        "        for message in new_messages:\n",
        "            if isinstance(message, ToolMessage):\n",
        "                print(f\"ğŸ‘¨â€ğŸ’¼ Human: {message.content}\")\n",
        "            elif isinstance(message, AIMessage):\n",
        "                print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    return final_result\n",
        "\n",
        "# Test the human-in-the-loop functionality\n",
        "human_loop_result = test_human_loop()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ¯ Part 6: Advanced State Management {#part6}\n",
        "\n",
        "So far, we've used simple state with just messages. But LangGraph can handle much more complex state for sophisticated applications.\n",
        "\n",
        "### Custom State Example: Research Assistant\n",
        "\n",
        "Let's build a research assistant that:\n",
        "- Tracks research topics\n",
        "- Maintains a list of findings\n",
        "- Counts the number of searches performed\n",
        "- Has a confidence score for its conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced State for Research Assistant\n",
        "class ResearchState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    research_topic: str\n",
        "    findings: List[str]\n",
        "    search_count: int\n",
        "    confidence_score: float\n",
        "    max_searches: int\n",
        "\n",
        "# Research tools\n",
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulate a web search (in real app, use actual search API).\n",
        "    \n",
        "    Args:\n",
        "        query: Search query\n",
        "        \n",
        "    Returns:\n",
        "        Search results\n",
        "    \"\"\"\n",
        "    # Simulate search results\n",
        "    results = {\n",
        "        \"python\": \"Python is a high-level programming language known for its simplicity and versatility.\",\n",
        "        \"machine learning\": \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
        "        \"langgraph\": \"LangGraph is a library for building stateful, multi-actor applications with LLMs.\",\n",
        "        \"default\": f\"Search results for '{query}': This is simulated search data with relevant information.\"\n",
        "    }\n",
        "    \n",
        "    return results.get(query.lower(), results[\"default\"])\n",
        "\n",
        "@tool\n",
        "def analyze_findings(findings_list: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze research findings and provide insights.\n",
        "    \n",
        "    Args:\n",
        "        findings_list: Comma-separated list of findings\n",
        "        \n",
        "    Returns:\n",
        "        Analysis of the findings\n",
        "    \"\"\"\n",
        "    findings = findings_list.split(',')\n",
        "    analysis = f\"Analysis of {len(findings)} findings: \"\n",
        "    \n",
        "    if len(findings) >= 3:\n",
        "        analysis += \"Comprehensive research completed. Strong evidence base.\"\n",
        "    elif len(findings) >= 2:\n",
        "        analysis += \"Good research foundation. Consider additional sources.\"\n",
        "    else:\n",
        "        analysis += \"Limited research. More investigation needed.\"\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# Research tools list\n",
        "research_tools = [web_search, analyze_findings, calculator, get_current_time]\n",
        "research_llm = llm.bind_tools(research_tools)\n",
        "\n",
        "print(\"âœ… Advanced research assistant tools created!\")\n",
        "print(\"Tools available: web_search, analyze_findings, calculator, get_current_time\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Create Research Assistant Nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def research_chatbot(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Research chatbot with advanced state management\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # Add context about current research state\n",
        "    context_message = f\"\"\"\n",
        "Current research status:\n",
        "- Topic: {state.get('research_topic', 'Not set')}\n",
        "- Findings so far: {len(state.get('findings', []))}\n",
        "- Searches performed: {state.get('search_count', 0)}/{state.get('max_searches', 5)}\n",
        "- Confidence: {state.get('confidence_score', 0.0):.1f}/10.0\n",
        "\"\"\"\n",
        "    \n",
        "    # Add context to messages for the LLM\n",
        "    full_messages = messages + [HumanMessage(content=context_message)]\n",
        "    response = research_llm.invoke(full_messages)\n",
        "    \n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def research_tool_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced tool node that updates research state\"\"\"\n",
        "    # Execute tools\n",
        "    tool_node_result = ToolNode(research_tools)(state)\n",
        "    \n",
        "    # Update research-specific state\n",
        "    updates = {}\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            if tool_call['name'] == 'web_search':\n",
        "                # Increment search count\n",
        "                updates[\"search_count\"] = state.get(\"search_count\", 0) + 1\n",
        "                \n",
        "                # Add to findings\n",
        "                current_findings = state.get(\"findings\", [])\n",
        "                query = tool_call['args']['query']\n",
        "                current_findings.append(f\"Search: {query}\")\n",
        "                updates[\"findings\"] = current_findings\n",
        "                \n",
        "                # Update confidence based on number of searches\n",
        "                search_count = updates[\"search_count\"]\n",
        "                updates[\"confidence_score\"] = min(search_count * 2.0, 10.0)\n",
        "    \n",
        "    # Combine tool results with state updates\n",
        "    result = {\"messages\": tool_node_result[\"messages\"]}\n",
        "    result.update(updates)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def initialize_research(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Initialize research session if needed\"\"\"\n",
        "    updates = {}\n",
        "    \n",
        "    if not state.get(\"research_topic\"):\n",
        "        # Extract topic from the first message\n",
        "        if state[\"messages\"]:\n",
        "            first_msg = state[\"messages\"][0].content\n",
        "            updates[\"research_topic\"] = first_msg[:100]  # First 100 chars as topic\n",
        "    \n",
        "    if \"search_count\" not in state:\n",
        "        updates[\"search_count\"] = 0\n",
        "    \n",
        "    if \"findings\" not in state:\n",
        "        updates[\"findings\"] = []\n",
        "        \n",
        "    if \"confidence_score\" not in state:\n",
        "        updates[\"confidence_score\"] = 0.0\n",
        "        \n",
        "    if \"max_searches\" not in state:\n",
        "        updates[\"max_searches\"] = 5\n",
        "    \n",
        "    return updates\n",
        "\n",
        "print(\"âœ… Research assistant nodes created!\")\n",
        "print(\"- research_chatbot: Handles conversations with research context\")\n",
        "print(\"- research_tool_node: Executes tools and updates research state\") \n",
        "print(\"- initialize_research: Sets up initial research parameters\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Build the Research Assistant Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def research_router(state: ResearchState) -> Literal[\"tools\", \"complete\", \"__end__\"]:\n",
        "    \"\"\"Advanced routing logic for research assistant\"\"\"\n",
        "    \n",
        "    # Check if we've reached max searches\n",
        "    if state.get(\"search_count\", 0) >= state.get(\"max_searches\", 5):\n",
        "        return \"complete\"\n",
        "    \n",
        "    # Check for tool calls\n",
        "    messages = state[\"messages\"]\n",
        "    if messages:\n",
        "        last_message = messages[-1]\n",
        "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "            return \"tools\"\n",
        "    \n",
        "    return \"__end__\"\n",
        "\n",
        "def complete_research(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Finalize research and provide summary\"\"\"\n",
        "    findings = state.get(\"findings\", [])\n",
        "    confidence = state.get(\"confidence_score\", 0.0)\n",
        "    topic = state.get(\"research_topic\", \"Unknown\")\n",
        "    \n",
        "    summary = f\"\"\"\n",
        "Research Complete!\n",
        "\n",
        "Topic: {topic}\n",
        "Total findings: {len(findings)}\n",
        "Confidence score: {confidence:.1f}/10.0\n",
        "\n",
        "Summary of findings:\n",
        "{chr(10).join(f\"- {finding}\" for finding in findings)}\n",
        "\n",
        "Research session concluded.\n",
        "\"\"\"\n",
        "    \n",
        "    return {\"messages\": [AIMessage(content=summary)]}\n",
        "\n",
        "# Build the research assistant graph\n",
        "research_builder = StateGraph(ResearchState)\n",
        "\n",
        "# Add nodes\n",
        "research_builder.add_node(\"initialize\", initialize_research)\n",
        "research_builder.add_node(\"chatbot\", research_chatbot)\n",
        "research_builder.add_node(\"tools\", research_tool_node)\n",
        "research_builder.add_node(\"complete\", complete_research)\n",
        "\n",
        "# Add edges\n",
        "research_builder.add_edge(START, \"initialize\")\n",
        "research_builder.add_edge(\"initialize\", \"chatbot\")\n",
        "\n",
        "# Add conditional routing\n",
        "research_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    research_router,\n",
        "    {\n",
        "        \"tools\": \"tools\",\n",
        "        \"complete\": \"complete\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "research_builder.add_edge(\"tools\", \"chatbot\")\n",
        "research_builder.add_edge(\"complete\", END)\n",
        "\n",
        "# Compile\n",
        "research_assistant = research_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "print(\"âœ… Advanced research assistant created!\")\n",
        "print(\"Flow: START â†’ initialize â†’ chatbot â†’ [tools/complete/end] â†’ ...\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Test the Advanced Research Assistant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_research_assistant():\n",
        "    print(\"ğŸ”¬ Testing Advanced Research Assistant\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"research_session_1\"}}\n",
        "    \n",
        "    # Start research\n",
        "    result = research_assistant.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"I want to research machine learning applications\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    print(\"ğŸ“Š Research Session Started\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Show state after initialization\n",
        "    state = research_assistant.get_state(config)\n",
        "    print(f\"Topic: {state.values.get('research_topic', 'Not set')}\")\n",
        "    print(f\"Search count: {state.values.get('search_count', 0)}\")\n",
        "    print(f\"Confidence: {state.values.get('confidence_score', 0.0):.1f}\")\n",
        "    print(f\"Findings: {len(state.values.get('findings', []))}\")\n",
        "    \n",
        "    # Continue with searches\n",
        "    print(\"\\nğŸ“ Requesting searches...\")\n",
        "    result = research_assistant.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"Please search for information about machine learning\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    # Show updated state\n",
        "    state = research_assistant.get_state(config)\n",
        "    print(f\"\\nAfter search:\")\n",
        "    print(f\"Search count: {state.values.get('search_count', 0)}\")\n",
        "    print(f\"Confidence: {state.values.get('confidence_score', 0.0):.1f}\")\n",
        "    print(f\"Findings: {state.values.get('findings', [])}\")\n",
        "    \n",
        "    # Show final messages\n",
        "    print(\"\\nğŸ’¬ Final Response:\")\n",
        "    for message in result[\"messages\"][-2:]:  # Show last 2 messages\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ğŸ¤– AI: {message.content[:200]}...\")  # Truncate for readability\n",
        "        elif isinstance(message, ToolMessage):\n",
        "            print(f\"ğŸ”§ Tool: {message.content[:100]}...\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Test the research assistant\n",
        "research_result = test_research_assistant()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ‘¥ Part 7: Multi-Agent Systems {#part7}\n",
        "\n",
        "LangGraph really shines when building systems with multiple specialized agents working together. Let's create a simple multi-agent system with:\n",
        "\n",
        "1. **Researcher Agent**: Gathers information\n",
        "2. **Analyst Agent**: Analyzes the information\n",
        "3. **Writer Agent**: Creates final reports\n",
        "\n",
        "### Multi-Agent State\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiAgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    task: str\n",
        "    research_data: List[str]\n",
        "    analysis_results: List[str]\n",
        "    final_report: str\n",
        "    current_agent: str\n",
        "    task_complete: bool\n",
        "\n",
        "# Specialized agents\n",
        "def researcher_agent(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Specialized research agent\"\"\"\n",
        "    \n",
        "    # Create research-focused prompt\n",
        "    research_prompt = f\"\"\"\n",
        "You are a Research Agent. Your job is to gather information about: {state.get('task', 'the given topic')}.\n",
        "\n",
        "Current research data: {state.get('research_data', [])}\n",
        "\n",
        "Please search for relevant information and add your findings to the research data.\n",
        "Focus on factual information and reliable sources.\n",
        "\"\"\"\n",
        "    \n",
        "    messages = state[\"messages\"] + [HumanMessage(content=research_prompt)]\n",
        "    response = research_llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [response],\n",
        "        \"current_agent\": \"researcher\"\n",
        "    }\n",
        "\n",
        "def analyst_agent(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Specialized analysis agent\"\"\"\n",
        "    \n",
        "    research_data = state.get('research_data', [])\n",
        "    analysis_prompt = f\"\"\"\n",
        "You are an Analysis Agent. Your job is to analyze the research data and provide insights.\n",
        "\n",
        "Research data to analyze:\n",
        "{chr(10).join(f\"- {data}\" for data in research_data)}\n",
        "\n",
        "Please provide analysis, identify patterns, and draw conclusions.\n",
        "Focus on insights and implications.\n",
        "\"\"\"\n",
        "    \n",
        "    messages = state[\"messages\"] + [HumanMessage(content=analysis_prompt)]\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # Extract analysis from response (simplified)\n",
        "    analysis_results = state.get('analysis_results', [])\n",
        "    analysis_results.append(f\"Analysis: {response.content[:200]}...\")\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [response],\n",
        "        \"analysis_results\": analysis_results,\n",
        "        \"current_agent\": \"analyst\"\n",
        "    }\n",
        "\n",
        "def writer_agent(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Specialized writing agent\"\"\"\n",
        "    \n",
        "    research_data = state.get('research_data', [])\n",
        "    analysis_results = state.get('analysis_results', [])\n",
        "    \n",
        "    writing_prompt = f\"\"\"\n",
        "You are a Writing Agent. Create a comprehensive final report.\n",
        "\n",
        "Research Data:\n",
        "{chr(10).join(f\"- {data}\" for data in research_data)}\n",
        "\n",
        "Analysis Results:\n",
        "{chr(10).join(f\"- {result}\" for result in analysis_results)}\n",
        "\n",
        "Please create a well-structured final report that combines the research and analysis.\n",
        "\"\"\"\n",
        "    \n",
        "    messages = state[\"messages\"] + [HumanMessage(content=writing_prompt)]\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [response],\n",
        "        \"final_report\": response.content,\n",
        "        \"current_agent\": \"writer\",\n",
        "        \"task_complete\": True\n",
        "    }\n",
        "\n",
        "print(\"âœ… Multi-agent system agents created!\")\n",
        "print(\"- Researcher Agent: Gathers information\")\n",
        "print(\"- Analyst Agent: Analyzes findings\") \n",
        "print(\"- Writer Agent: Creates final reports\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Build Multi-Agent Workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_agent_router(state: MultiAgentState) -> Literal[\"researcher\", \"analyst\", \"writer\", \"__end__\"]:\n",
        "    \"\"\"Route between different agents based on current state\"\"\"\n",
        "    \n",
        "    if state.get(\"task_complete\", False):\n",
        "        return \"__end__\"\n",
        "    \n",
        "    current_agent = state.get(\"current_agent\", \"\")\n",
        "    research_data = state.get(\"research_data\", [])\n",
        "    analysis_results = state.get(\"analysis_results\", [])\n",
        "    \n",
        "    # Start with researcher\n",
        "    if not current_agent:\n",
        "        return \"researcher\"\n",
        "    \n",
        "    # After researcher, go to analyst if we have research data\n",
        "    if current_agent == \"researcher\" and len(research_data) > 0:\n",
        "        return \"analyst\"\n",
        "    \n",
        "    # After analyst, go to writer if we have analysis\n",
        "    if current_agent == \"analyst\" and len(analysis_results) > 0:\n",
        "        return \"writer\"\n",
        "    \n",
        "    # Continue with current agent if more work needed\n",
        "    if current_agent == \"researcher\" and len(research_data) < 2:\n",
        "        return \"researcher\"\n",
        "    \n",
        "    return \"__end__\"\n",
        "\n",
        "def initialize_multi_agent(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Initialize multi-agent state\"\"\"\n",
        "    updates = {}\n",
        "    \n",
        "    if not state.get(\"task\") and state.get(\"messages\"):\n",
        "        # Extract task from first message\n",
        "        first_msg = state[\"messages\"][0].content\n",
        "        updates[\"task\"] = first_msg\n",
        "    \n",
        "    if \"research_data\" not in state:\n",
        "        updates[\"research_data\"] = []\n",
        "    \n",
        "    if \"analysis_results\" not in state:\n",
        "        updates[\"analysis_results\"] = []\n",
        "    \n",
        "    if \"current_agent\" not in state:\n",
        "        updates[\"current_agent\"] = \"\"\n",
        "    \n",
        "    if \"task_complete\" not in state:\n",
        "        updates[\"task_complete\"] = False\n",
        "    \n",
        "    return updates\n",
        "\n",
        "# Enhanced tool node for multi-agent\n",
        "def multi_agent_tool_node(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Tool node that updates research data\"\"\"\n",
        "    tool_result = ToolNode(research_tools)(state)\n",
        "    \n",
        "    # Update research data if web search was performed\n",
        "    updates = {}\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            if tool_call['name'] == 'web_search':\n",
        "                research_data = state.get(\"research_data\", [])\n",
        "                query = tool_call['args']['query']\n",
        "                # Add the search result to research data\n",
        "                research_data.append(f\"Search result for '{query}': {tool_result['messages'][-1].content}\")\n",
        "                updates[\"research_data\"] = research_data\n",
        "    \n",
        "    result = {\"messages\": tool_result[\"messages\"]}\n",
        "    result.update(updates)\n",
        "    return result\n",
        "\n",
        "# Build multi-agent graph\n",
        "multi_agent_builder = StateGraph(MultiAgentState)\n",
        "\n",
        "# Add nodes\n",
        "multi_agent_builder.add_node(\"initialize\", initialize_multi_agent)\n",
        "multi_agent_builder.add_node(\"researcher\", researcher_agent)\n",
        "multi_agent_builder.add_node(\"tools\", multi_agent_tool_node)\n",
        "multi_agent_builder.add_node(\"analyst\", analyst_agent)\n",
        "multi_agent_builder.add_node(\"writer\", writer_agent)\n",
        "\n",
        "# Add edges\n",
        "multi_agent_builder.add_edge(START, \"initialize\")\n",
        "\n",
        "# From initialize, route to appropriate agent\n",
        "multi_agent_builder.add_conditional_edges(\n",
        "    \"initialize\",\n",
        "    multi_agent_router,\n",
        "    {\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"writer\": \"writer\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Researcher can use tools or continue to next agent\n",
        "multi_agent_builder.add_conditional_edges(\n",
        "    \"researcher\",\n",
        "    lambda state: \"tools\" if (\n",
        "        state[\"messages\"] and \n",
        "        hasattr(state[\"messages\"][-1], 'tool_calls') and \n",
        "        state[\"messages\"][-1].tool_calls\n",
        "    ) else multi_agent_router(state),\n",
        "    {\n",
        "        \"tools\": \"tools\",\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"writer\": \"writer\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# After tools, route to next agent\n",
        "multi_agent_builder.add_conditional_edges(\n",
        "    \"tools\",\n",
        "    multi_agent_router,\n",
        "    {\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"writer\": \"writer\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Analyst routes to next agent\n",
        "multi_agent_builder.add_conditional_edges(\n",
        "    \"analyst\",\n",
        "    multi_agent_router,\n",
        "    {\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"writer\": \"writer\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Writer completes the task\n",
        "multi_agent_builder.add_edge(\"writer\", END)\n",
        "\n",
        "# Compile\n",
        "multi_agent_system = multi_agent_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "print(\"âœ… Multi-agent system created!\")\n",
        "print(\"Flow: initialize â†’ researcher â†’ [tools] â†’ analyst â†’ writer â†’ END\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Test Multi-Agent System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_multi_agent_system():\n",
        "    print(\"ğŸ‘¥ Testing Multi-Agent System\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"multi_agent_test\"}}\n",
        "    \n",
        "    # Start the multi-agent workflow\n",
        "    result = multi_agent_system.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"Research and analyze the benefits of renewable energy\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    # Get final state\n",
        "    final_state = multi_agent_system.get_state(config)\n",
        "    \n",
        "    print(\"ğŸ“Š Multi-Agent Workflow Complete!\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Task: {final_state.values.get('task', 'Not set')}\")\n",
        "    print(f\"Current Agent: {final_state.values.get('current_agent', 'None')}\")\n",
        "    print(f\"Task Complete: {final_state.values.get('task_complete', False)}\")\n",
        "    \n",
        "    print(f\"\\nğŸ”¬ Research Data ({len(final_state.values.get('research_data', []))}):\")\n",
        "    for i, data in enumerate(final_state.values.get('research_data', []), 1):\n",
        "        print(f\"{i}. {data[:100]}...\")\n",
        "    \n",
        "    print(f\"\\nğŸ“ˆ Analysis Results ({len(final_state.values.get('analysis_results', []))}):\")\n",
        "    for i, analysis in enumerate(final_state.values.get('analysis_results', []), 1):\n",
        "        print(f\"{i}. {analysis[:100]}...\")\n",
        "    \n",
        "    print(f\"\\nğŸ“ Final Report:\")\n",
        "    final_report = final_state.values.get('final_report', 'No report generated')\n",
        "    print(final_report[:300] + \"...\" if len(final_report) > 300 else final_report)\n",
        "    \n",
        "    print(f\"\\nğŸ’¬ Total Messages: {len(final_state.values.get('messages', []))}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Test the multi-agent system\n",
        "multi_agent_result = test_multi_agent_system()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸŒŸ Part 8: Real-World Use Cases {#part8}\n",
        "\n",
        "Now that you understand LangGraph fundamentals, let's explore some real-world applications:\n",
        "\n",
        "### ğŸ¯ Use Case Examples\n",
        "\n",
        "1. **Customer Support Agent**\n",
        "   - Handles inquiries with memory\n",
        "   - Escalates to humans when needed\n",
        "   - Accesses knowledge bases and APIs\n",
        "\n",
        "2. **Code Review Assistant**  \n",
        "   - Analyzes code for issues\n",
        "   - Suggests improvements\n",
        "   - Runs tests and checks\n",
        "\n",
        "3. **Content Creation Pipeline**\n",
        "   - Research â†’ Outline â†’ Write â†’ Edit â†’ Publish\n",
        "   - Multiple specialized agents\n",
        "   - Human approval at key stages\n",
        "\n",
        "4. **Data Analysis Workflow**\n",
        "   - Data ingestion â†’ Cleaning â†’ Analysis â†’ Visualization â†’ Report\n",
        "\n",
        "### ğŸ’¡ Key Patterns You've Learned\n",
        "\n",
        "1. **State Management**: Custom state for complex workflows\n",
        "2. **Tool Integration**: External APIs and functions\n",
        "3. **Human-in-the-Loop**: Interrupts and approvals\n",
        "4. **Multi-Agent Coordination**: Specialized agents working together\n",
        "5. **Conditional Routing**: Dynamic decision making\n",
        "6. **Memory & Persistence**: Conversation continuity\n",
        "\n",
        "### ğŸš€ Next Steps\n",
        "\n",
        "To build production applications:\n",
        "\n",
        "1. **Error Handling**: Add try-catch blocks and error recovery\n",
        "2. **Authentication**: Secure your APIs and data\n",
        "3. **Monitoring**: Use LangSmith for observability\n",
        "4. **Scaling**: Consider deployment options\n",
        "5. **Testing**: Write comprehensive tests\n",
        "6. **Documentation**: Document your workflows\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## ğŸ“ Conclusion & Summary\n",
        "\n",
        "Congratulations! ğŸ‰ You've completed the comprehensive LangGraph tutorial. Here's what you've accomplished:\n",
        "\n",
        "### âœ… What You've Built\n",
        "\n",
        "1. **Simple Chatbot** - Basic conversation with LLM\n",
        "2. **Memory-Enabled Agent** - Persistent conversations with checkpointing  \n",
        "3. **Tool-Integrated Agent** - External API calls and function execution\n",
        "4. **Human-in-the-Loop System** - Interrupts and human oversight\n",
        "5. **Advanced State Management** - Complex workflows with custom state\n",
        "6. **Multi-Agent System** - Coordinated specialized agents\n",
        "\n",
        "### ğŸ§  Core Concepts Mastered\n",
        "\n",
        "- **State**: The memory and data of your application\n",
        "- **Nodes**: Functions that process and transform state\n",
        "- **Edges**: Connections that define workflow flow\n",
        "- **Conditional Routing**: Dynamic decision making\n",
        "- **Checkpointing**: Persistent memory across sessions\n",
        "- **Interrupts**: Human-in-the-loop capabilities\n",
        "- **Tools**: External function integration\n",
        "\n",
        "### ğŸ“š Additional Resources\n",
        "\n",
        "- **Official Documentation**: https://langchain-ai.github.io/langgraph/\n",
        "- **LangSmith Observability**: https://langsmith.langchain.com/\n",
        "- **LangChain Community**: https://github.com/langchain-ai/langchain\n",
        "- **Examples Repository**: https://github.com/langchain-ai/langgraph/tree/main/examples\n",
        "\n",
        "### ğŸš€ Your Next Journey\n",
        "\n",
        "You now have the foundation to build sophisticated AI agents and workflows. Start with a simple use case and gradually add complexity as you become more comfortable with the patterns.\n",
        "\n",
        "**Happy building with LangGraph!** ğŸŒŸ\n",
        "\n",
        "---\n",
        "\n",
        "*This tutorial was created based on the official LangGraph documentation and best practices. For the latest updates and features, always refer to the official documentation.*\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# ğŸš€ Complete LangGraph Tutorial: From Basics to Advanced Applications\n",
        "\n",
        "## ğŸ“– Comprehensive Guide for Building Stateful AI Agents\n",
        "\n",
        "Welcome to the most comprehensive LangGraph tutorial! This notebook will take you from complete beginner to building sophisticated AI agents step by step.\n",
        "\n",
        "### ğŸ¯ What You'll Learn\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "- âœ… Understand LangGraph's core concepts (Nodes, Edges, State)\n",
        "- âœ… Build your first simple chatbot\n",
        "- âœ… Add memory and persistence to conversations\n",
        "- âœ… Implement tool calling and external integrations\n",
        "- âœ… Create human-in-the-loop workflows\n",
        "- âœ… Build multi-agent systems\n",
        "- âœ… Handle complex state management\n",
        "- âœ… Deploy production-ready applications\n",
        "\n",
        "### ğŸ“š Based on Official LangGraph Documentation\n",
        "\n",
        "This tutorial follows the [official LangGraph documentation](https://langchain-ai.github.io/langgraph/) and incorporates best practices from the LangGraph team.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Table of Contents\n",
        "\n",
        "1. **[Setup & Installation](#setup)**\n",
        "2. **[Part 1: Understanding LangGraph Fundamentals](#part1)**\n",
        "3. **[Part 2: Building Your First Simple Agent](#part2)**\n",
        "4. **[Part 3: Adding Memory with Checkpointing](#part3)**\n",
        "5. **[Part 4: Tool Integration & External APIs](#part4)**\n",
        "6. **[Part 5: Human-in-the-Loop Workflows](#part5)**\n",
        "7. **[Part 6: Advanced State Management](#part6)**\n",
        "8. **[Part 7: Multi-Agent Systems](#part7)**\n",
        "9. **[Part 8: Real-World Use Cases](#part8)**\n",
        "10. **[Part 9: Production Deployment](#part9)**\n",
        "\n",
        "Let's begin this exciting journey! ğŸŒŸ\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ai_experiment",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
